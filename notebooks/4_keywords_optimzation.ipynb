{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "from sentence_transformers.util import cos_sim\n",
    "import sys\n",
    "import key_words\n",
    "\n",
    "from typing import Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = \"data/train_phase1.tsv\"\n",
    "df = pd.read_csv(train_path, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "def save_obj(obj:object,name:str):\n",
    "    ext = '.pickle'\n",
    "    with open(name + ext, 'wb') as handle:\n",
    "        pickle.dump(obj, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def load_obj(name:str)->object:\n",
    "    ext = '.pickle'\n",
    "    with open(name + ext, 'rb') as handle:\n",
    "        return pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformer_model\n",
    "import grading_model\n",
    "\n",
    "BERT = transformer_model.BERTModel()\n",
    "\n",
    "GM = grading_model.GradingModel(BERT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## optimize match_keywords function last output grade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.48 ms ± 28.9 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit reduce(lambda x,y: x+y,map(fn_, similarities))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.45 ms ± 52.9 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%timeit sum(map(fn_, similarities))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.51 ms ± 59.7 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit np.sum(np.array(list(map(fn_, similarities))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# optimize all keywords file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_emb = load_obj(\"data/ess_emb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ess_1_model_answers = load_obj(\"data/essaySet_1_model_answers\")\n",
    "ess_2_model_answers = load_obj(\"data/essaySet_2_model_answers\")\n",
    "ess_3_model_answers = load_obj(\"data/essaySet_3_model_answers\")\n",
    "ess_4_model_answers = load_obj(\"data/essaySet_4_model_answers\")\n",
    "ess_5_model_answers = load_obj(\"data/essaySet_5_model_answers\")\n",
    "ess_6_model_answers = load_obj(\"data/essaySet_6_model_answers\")\n",
    "ess_7_model_answers = load_obj(\"data/essaySet_7_model_answers\")\n",
    "ess_8_model_answers = load_obj(\"data/essaySet_8_model_answers\")\n",
    "ess_9_model_answers = load_obj(\"data/essaySet_9_model_answers\")\n",
    "ess_10_model_answers = load_obj(\"data/essaySet_10_model_answers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ess_1_model_answers_emb = BERT.model.encode(ess_1_model_answers)\n",
    "ess_2_model_answers_emb = BERT.model.encode(ess_2_model_answers)\n",
    "ess_3_model_answers_emb = BERT.model.encode(ess_3_model_answers)\n",
    "ess_4_model_answers_emb = BERT.model.encode(ess_4_model_answers)\n",
    "ess_5_model_answers_emb = BERT.model.encode(ess_5_model_answers)\n",
    "ess_6_model_answers_emb = BERT.model.encode(ess_6_model_answers)\n",
    "ess_7_model_answers_emb = BERT.model.encode(ess_7_model_answers)\n",
    "ess_8_model_answers_emb = BERT.model.encode(ess_8_model_answers)\n",
    "ess_9_model_answers_emb = BERT.model.encode(ess_9_model_answers)\n",
    "ess_10_model_answers_emb = BERT.model.encode(ess_10_model_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_dict = {'ess_1_model_answers_emb': ess_1_model_answers_emb,\n",
    " 'ess_2_model_answers_emb': ess_2_model_answers_emb,\n",
    " 'ess_3_model_answers_emb': ess_3_model_answers_emb,\n",
    " 'ess_4_model_answers_emb': ess_4_model_answers_emb,\n",
    " 'ess_5_model_answers_emb': ess_5_model_answers_emb,\n",
    " 'ess_6_model_answers_emb': ess_6_model_answers_emb,\n",
    " 'ess_7_model_answers_emb': ess_7_model_answers_emb,\n",
    " 'ess_8_model_answers_emb': ess_8_model_answers_emb,\n",
    " 'ess_9_model_answers_emb': ess_9_model_answers_emb,\n",
    " 'ess_10_model_answers_emb': ess_10_model_answers_emb}\n",
    "save_obj(emb_dict, \"data/model_answer_emb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_dict = load_obj(\"data/model_answer_emb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = df.query('EssaySet == 1')[\"EssayText\"].values.tolist()\n",
    "model_candidates = list(map(lambda ans: key_words.candidates_tokens(ans,n_gram_range=(1,2)),ess_1_model_answers))\n",
    "# model_candidate_emb = BERT.model.encode(model_candidates)\n",
    "# emb_dict[ess_1_model_answers_emb]\n",
    "model_candidate_emb = list(map(lambda  cand:BERT.model.encode(cand),model_candidates))\n",
    "\n",
    "\n",
    "l = zip(emb_dict['ess_1_model_answers_emb'],model_candidate_emb,model_candidates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_obj(l, \"data/ess_1_model_answers_emb__candidates\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maximal_marginal_relevance(doc_embedding: np.ndarray,\n",
    "        word_embeddings: np.ndarray,\n",
    "        words,\n",
    "        top_n = 5,\n",
    "        diversity = 0.8):\n",
    "    \"\"\"\n",
    "    Maximal Marginal Relevance algorithm for keyword extraction\n",
    "    * from KeyBERT repository on github\n",
    "\n",
    "    Args:\n",
    "        doc_embedding (numpy.ndarray): embedding of shape (1, 768)\n",
    "        word_embeddings (numpy.ndarray): embedding of shape (N, 768)\n",
    "        words (List[str]): list of words\n",
    "        top_n (Optional[int]): number of top words to extract\n",
    "        diversity (Optional[float]): diversity of top words to extract\n",
    "\n",
    "    Returns:\n",
    "        List[Tuple[str, float]]: list of top_n words with their scores\n",
    "    \"\"\"\n",
    "    # make sure 2d array\n",
    "    if doc_embedding.ndim == 1:\n",
    "        doc_embedding = doc_embedding.reshape(1, -1)\n",
    "\n",
    "    # Extract similarity within words, and between words and the document\n",
    "\n",
    "    word_doc_similarity = np.array(cos_sim(word_embeddings, doc_embedding)).clip(-1, 1).round(6)\n",
    "    word_similarity = np.array(cos_sim(word_embeddings, word_embeddings)).clip(-1, 1).round(6)\n",
    "\n",
    "    # Initialize candidates and already choose best keyword/keyphras\n",
    "    keywords_idx = [np.argmax(word_doc_similarity)]\n",
    "    candidates_idx = [i for i in range(len(words)) if i != keywords_idx[0]]\n",
    "\n",
    "    for _ in range(top_n - 1):\n",
    "        # Extract similarities within candidates and\n",
    "        # between candidates and selected keywords/phrases\n",
    "        candidate_similarities = word_doc_similarity[candidates_idx, :]\n",
    "        target_similarities = np.max(word_similarity[candidates_idx][:, keywords_idx], axis=1)\n",
    "\n",
    "        # Calculate maximal_marginal_relevance\n",
    "        mmr = (1-diversity) * candidate_similarities -\\\n",
    "            diversity * target_similarities.reshape(-1, 1)\n",
    "        # if return mmr is empty\n",
    "        if mmr.size == 0:\n",
    "            continue\n",
    "        mmr = candidates_idx[np.argmax(mmr)]\n",
    "\n",
    "        # Update keywords & candidates\n",
    "        keywords_idx.append(mmr)\n",
    "        candidates_idx.remove(mmr)\n",
    "\n",
    "    return [words[idx] for idx in keywords_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def emb_keywords(keywords):\n",
    "    x = np.array(list(map(lambda k:\n",
    "     np.array(list(map(lambda x: BERT.model.encode(str(x)), k))),keywords)))\n",
    "    if x.ndim == 3:\n",
    "        x = x.reshape(max(x.shape[1],x.shape[0]),x.shape[2])\n",
    "    # print(\"emb\",x.shape)\n",
    "    return x\n",
    "    # return list(map(lambda k: np.array(list(map(lambda x: BERT.model.encode(str(x)), k))),keywords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def emb_keywords(keywords):\n",
    "    # return np.array(list(map(lambda k: np.array(list(map(lambda x: BERT.model.encode(str(x)), k))),keywords)))\n",
    "    return np.array(list(map(lambda k: np.array(list(map(lambda x: get_words_emb(str(x)), k))),keywords)))\n",
    "    # return np.array(list(map(BERT.model.encode, keywords)))\n",
    "def old_emb_keywords(keywords):\n",
    "    return np.array(list(map(BERT.model.encode, keywords)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kw_match(keywords_emb,candidates_emb):\n",
    "    combination = list(zip(keywords_emb,candidates_emb))\n",
    "\n",
    "    similarities = list(map(lambda comb:\n",
    "                    np.array(cos_sim(comb[0],\n",
    "                    comb[1])).clip(-1, 1).round(6),\n",
    "                    combination))\n",
    "\n",
    "    def fn_ (x: np.array):\n",
    "        \"\"\"\n",
    "        return the no. of matched keywords\n",
    "        \"\"\"\n",
    "        if not np.sum(x>= thershold):\n",
    "            return 0\n",
    "        if np.sum(x >= thershold) > 1.0:\n",
    "            return 1.0\n",
    "        return np.sum(x >= thershold)\n",
    "\n",
    "    # res = np.sum(np.array(list(map(fn_, similarities))))\n",
    "    # 5.51 ms ± 59.7 µs per loop (mean ± std. dev. of 7 runs, 100 loops each) for 600 elements\n",
    "\n",
    "    res = sum(map(fn_, similarities))\n",
    "    # 5.45 ms ± 52.9 µs per loop (mean ± std. dev. of 7 runs, 100 loops each) for 600 elements\n",
    "    # return res/float(len(keywords_emb))\n",
    "    return res/len(keywords_emb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimzation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "see what is faster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43.1 ms ± 319 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit list(map(lambda doc:key_words.get_candidates(students_n_grams, doc),docs[:100]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43 ms ± 363 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "docs_100 = docs[:100]\n",
    "batch = 50\n",
    "n_docs = len(docs_100)\n",
    "def dummy():\n",
    "    for i in range(0,n_docs,batch):\n",
    "        students_candidates = list(map(lambda doc:\n",
    "                    key_words.get_candidates(students_n_grams, doc),\n",
    "                    docs_100[i:i+batch]))\n",
    "%timeit dummy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> in batch mode is faster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# output essay 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_candidates = list(map(lambda ans: key_words.candidates_tokens(ans,n_gram_range=(1,2)),ess_2_model_answers))\n",
    "model_candidate_emb = list(map(lambda cand:BERT.model.encode(cand),model_candidates))\n",
    "keywords = list(map(lambda x: maximal_marginal_relevance(\n",
    "    x[0].reshape(1, -1),x[1],x[2]),zip(emb_dict['ess_2_model_answers_emb'],\n",
    "    model_candidate_emb,model_candidates)))\n",
    "students_n_grams = list(map(lambda keyword: sorted(key_words.get_n_grams(keyword)),keywords))\n",
    "docs = df.query('EssaySet == 2')[\"EssayText\"].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['improve',\n",
       " 'used experiment',\n",
       " 'student',\n",
       " 'data names',\n",
       " 'experiment plastic',\n",
       " 'better new',\n",
       " 'conclusion plastic',\n",
       " 'type plastic',\n",
       " 'student experiment',\n",
       " 'types',\n",
       " 'held stronger',\n",
       " 'plastic length',\n",
       " 'names',\n",
       " 'data',\n",
       " 'stronger different',\n",
       " 'length conclusion',\n",
       " 'experiment better',\n",
       " 'grafin',\n",
       " 'type grafin',\n",
       " 'names type',\n",
       " 'new data',\n",
       " 'better',\n",
       " 'experiment',\n",
       " 'plastic type',\n",
       " 'grafin improve',\n",
       " 'new',\n",
       " 'held',\n",
       " 'conclusion',\n",
       " 'changing type',\n",
       " 'type used',\n",
       " 'used',\n",
       " 'length',\n",
       " 'type held',\n",
       " 'different',\n",
       " 'improve student',\n",
       " 'type',\n",
       " 'changing',\n",
       " 'different types',\n",
       " 'plastic',\n",
       " 'stronger']"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key_words.candidates_tokens(str(docs[0]), n_gram_range=(1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['improve',\n",
       "  'used experiment',\n",
       "  'student',\n",
       "  'data names',\n",
       "  'experiment plastic',\n",
       "  'better new',\n",
       "  'conclusion plastic',\n",
       "  'type plastic',\n",
       "  'student experiment',\n",
       "  'types',\n",
       "  'held stronger',\n",
       "  'plastic length',\n",
       "  'names',\n",
       "  'data',\n",
       "  'stronger different',\n",
       "  'length conclusion',\n",
       "  'experiment better',\n",
       "  'grafin',\n",
       "  'type grafin',\n",
       "  'names type',\n",
       "  'new data',\n",
       "  'better',\n",
       "  'experiment',\n",
       "  'plastic type',\n",
       "  'grafin improve',\n",
       "  'new',\n",
       "  'held',\n",
       "  'conclusion',\n",
       "  'changing type',\n",
       "  'type used',\n",
       "  'used',\n",
       "  'length',\n",
       "  'type held',\n",
       "  'different',\n",
       "  'improve student',\n",
       "  'type',\n",
       "  'changing',\n",
       "  'different types',\n",
       "  'plastic',\n",
       "  'stronger']]"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_candidates([(1,2),], docs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['stretchability polymer',\n",
       "  'did',\n",
       "  'samples start',\n",
       "  'measurements length',\n",
       "  'plastic sample'],\n",
       " ['sample stretchability',\n",
       "  'polymer plastics',\n",
       "  'did',\n",
       "  'samples start',\n",
       "  'compared']]"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[ans for ind, ans in enumerate(keywords[:2])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0\n",
      "['stretchability polymer', 'did', 'samples start', 'measurements length', 'plastic sample']\n",
      "iter 1\n",
      "['sample stretchability', 'polymer plastics', 'did', 'samples start', 'compared']\n"
     ]
    }
   ],
   "source": [
    "keywords_embeddings_lis = []\n",
    "for ind, ans in enumerate(keywords[:2]):\n",
    "    print(\"iter\",ind)\n",
    "    keywords_embeddings =  np.array(list(map(BERT.model.encode, ans)))\n",
    "    print(ans)\n",
    "    keywords_embeddings_lis.append(keywords_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_candidates(n_grams, doc):\n",
    "    x = list(map(lambda gram :\n",
    "        key_words.candidates_tokens(str(doc), n_gram_range=gram)\n",
    "        , n_grams))\n",
    "    # print shapes\n",
    "    # print([len(i) for i in x])\n",
    "    return x\n",
    "\n",
    "def keywords_pipeline_output(keywords,docs,n_gram_list,batch = 50):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        keywords: list of list of keywords\n",
    "        docs: list of list of tokens\n",
    "        batch: batch size\n",
    "    Returns:\n",
    "        a dict of each\n",
    "            n_gram_dict : dict of dict of list of list of tokens\n",
    "            n_grams_list : list of list of n_grams\n",
    "            students_candidates_list : list of list of list of tokens\n",
    "            keywords_embeddings_list  : list of list of list of embeddings\n",
    "            students_candidates_emb_list : list of list of list of embeddings\n",
    "            times : list of times taken for each model answer\n",
    "    \"\"\"\n",
    "\n",
    "    def check_n_gram_dict(n_grams, doc, ind):\n",
    "        res = []\n",
    "        for n_gram in n_grams:\n",
    "            x = n_gram_dict[ind].get(n_gram)\n",
    "            if x is None:\n",
    "                # x =  key_words.get_candidates(n_gram, doc)\n",
    "                x =  get_candidates([n_gram], doc)\n",
    "                # print(\"in\",x)\n",
    "                n_gram_dict[ind][n_gram] = x\n",
    "                res.append(x)\n",
    "            else:\n",
    "                res.append(x)\n",
    "        # print(\"out\",res)\n",
    "        return res\n",
    "\n",
    "\n",
    "    def new_check_n_gram_dict(n_grams, doc, ind):\n",
    "        # res = []\n",
    "        # emb = []\n",
    "        # for n_gram in n_grams:\n",
    "        #     x = n_gram_dict[ind].get(n_gram)\n",
    "        #     if x is None:\n",
    "        #         # x =  key_words.get_candidates(n_gram, doc)\n",
    "        #         x =  get_candidates([n_gram], doc)\n",
    "        #         n_gram_dict[ind][n_gram] = x\n",
    "        #         # cand_emb_dict[ind][n_gram] = list(map(BERT.model.encode, x))\n",
    "        #         cand_emb_dict[ind][n_gram] = emb_keywords(x)\n",
    "        #         # emb.append(cand_emb_dict[ind][n_gram])\n",
    "        #         res.append(x)\n",
    "        #     else:\n",
    "        #         res.append(x)\n",
    "        # return res\n",
    "        def fn(n_gram):\n",
    "            x = n_gram_dict[ind].get(n_gram)\n",
    "            if x is None:\n",
    "                # print(\"hi\",ind)\n",
    "                x =  get_candidates([n_gram], doc)\n",
    "                n_gram_dict[ind][n_gram] = x\n",
    "                cand_emb_dict[ind][n_gram] = emb_keywords(x)\n",
    "                return x\n",
    "            else:\n",
    "                # print(\"f\",ind)\n",
    "                return x\n",
    "        return list(map(fn, n_grams))\n",
    "        # do it in parallel\n",
    "        # return list(map(lambda n_gram:\n",
    "        #     n_gram_dict[ind].get(n_gram)\n",
    "        #     if n_gram_dict[ind].get(n_gram) is None\n",
    "        #     else n_gram_dict[ind][n_gram], n_grams))\n",
    "\n",
    "\n",
    "    def check_candidates_dict(n_grams, doc, ind):\n",
    "        # res = []\n",
    "        # for n_gram in n_grams:\n",
    "        #     x = cand_emb_dict[ind].get(n_gram)\n",
    "        #     if x is None:\n",
    "        #         # x =  key_words.get_candidates(n_gram, doc)\n",
    "        #         x =  get_candidates([n_gram], doc)\n",
    "        #         cand_emb_dict[ind][n_gram] = x\n",
    "        #         res.append(x)\n",
    "        #     else:\n",
    "        #         res.append(x)\n",
    "        # return res\n",
    "        # do it in parallel\n",
    "        return list(map(lambda n_gram:\n",
    "            cand_emb_dict[ind].get(n_gram)\n",
    "            if cand_emb_dict[ind].get(n_gram) is None\n",
    "            else cand_emb_dict[ind][n_gram],\n",
    "            n_grams))\n",
    "\n",
    "\n",
    "\n",
    "    n_docs = len(docs)\n",
    "    if n_docs < batch:\n",
    "        batch = n_docs\n",
    "    \n",
    "    n_gram_dict = {ind: {n_gram: None for n_gram in set([item for sublist in n_gram_list for item in sublist])} for ind ,doc in enumerate(docs)}\n",
    "    cand_emb_dict = {ind: {n_gram: None for n_gram in set([item for sublist in n_gram_list for item in sublist])} for ind ,doc in enumerate(docs)}\n",
    "\n",
    "    n_grams_list = []\n",
    "    students_candidates_list = []\n",
    "    keywords_embeddings_list = []\n",
    "    students_candidates_emb_list = []\n",
    "    times = []\n",
    "    hh_list = []\n",
    "\n",
    "    # all model answers\n",
    "    for ind, ans in enumerate(keywords):\n",
    "        print(\"model answer\",ind)\n",
    "        t1 = time.perf_counter()\n",
    "        students_n_grams = key_words.get_n_grams(ans)\n",
    "        keywords_embeddings =  list(map(BERT.model.encode, ans))\n",
    "\n",
    "        students_candidates_list_s = []\n",
    "        students_candidates_emb_list_s = []\n",
    "        # hh_list_s = []\n",
    "\n",
    "        # do in batches\n",
    "        for i in range(0,n_docs,batch):\n",
    "            # students_candidates = [0]\n",
    "            # students_candidates = list(map(lambda doc:\n",
    "            #                 check_n_gram_dict(students_n_grams, doc[1], i+doc[0]),\n",
    "            #                 enumerate(docs[i:i+batch])))\n",
    "            # print(\"students_candidates\",students_candidates[0])\n",
    "            # students_candidates_emb = []\n",
    "            # for j,k in enumerate(students_candidates):\n",
    "            #     print(j,k)\n",
    "            #     students_candidates_emb.extend(emb_keywords(k))\n",
    "            # print(\"students_candidates\",len(students_candidates))\n",
    "            # students_candidates_emb =  []\n",
    "            students_candidates = list(map(lambda doc:\n",
    "                            new_check_n_gram_dict(students_n_grams, doc[1], i+doc[0]),\n",
    "                            enumerate(docs[i:i+batch])))\n",
    "            students_candidates_emb = list(map(lambda doc:\n",
    "                            check_candidates_dict(students_n_grams, doc[1], i+doc[0]),\n",
    "                            enumerate(docs[i:i+batch])))\n",
    "            # students_candidates_emb =  list(map(lambda st: list(map( emb_keywords, st)),students_candidates))\n",
    "            # students_candidates_emb =  list(map(lambda doc:\n",
    "            #                 check_n_gram_dict(students_n_grams, doc[1], i+doc[0]),\n",
    "            #                 enumerate(docs[i:i+batch])))\n",
    "            # hh = [x.shape for x in students_candidates_emb]\n",
    "            # students_candidates_emb =  list(map(fn, students_candidates))\n",
    "            # hh =  list(map(old_fn, students_candidates))\n",
    "            # list(map(lambda doc:\n",
    "            #                 check_candidates_dict(students_n_grams, doc[1], i+doc[0]),\n",
    "            #                 enumerate(students_candidates)))\n",
    "\n",
    "            students_candidates_list_s.extend(students_candidates)\n",
    "            # hh_list_s.extend(hh)\n",
    "            students_candidates_emb_list_s.extend(students_candidates_emb)\n",
    "\n",
    "        if n_docs % batch != 0 and n_docs > batch:\n",
    "            students_candidates = list(map(lambda doc:\n",
    "                            check_n_gram_dict(students_n_grams, doc[1], i+doc[0]),\n",
    "                            enumerate(docs[i+batch:])))\n",
    "            students_candidates_emb =  list(map(emb_keywords, students_candidates))\n",
    "\n",
    "            students_candidates_list_s.extend(students_candidates)\n",
    "            students_candidates_emb_list_s.extend(students_candidates_emb)\n",
    "\n",
    "        n_grams_list.append(students_n_grams)\n",
    "        keywords_embeddings_list.append(keywords_embeddings)\n",
    "\n",
    "        students_candidates_list.append(students_candidates_list_s)\n",
    "        students_candidates_emb_list.append(students_candidates_emb_list_s)\n",
    "        # hh_list.append(hh_list_s)\n",
    "        times.append(time.perf_counter() - t1)\n",
    "    # retun a dict of each n_grams_list, students_candidates_list, keywords_embeddings_list, students_candidates_emb_list\n",
    "    return {\n",
    "        \"n_gram_dict\": n_gram_dict,\n",
    "        \"n_grams_list\": n_grams_list,\n",
    "        # \"cand_emb_dict\": cand_emb_dict,\n",
    "        # \"hh_list\": hh_list,\n",
    "        \"students_candidates_list\": students_candidates_list,\n",
    "        \"keywords_embeddings_list\": keywords_embeddings_list,\n",
    "        \"students_candidates_emb_list\": students_candidates_emb_list,\n",
    "        \"times\": times\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_keywords(\n",
    "    keywords_emb:list[np.ndarray],\n",
    "    candidates_emb : list[np.ndarray],\n",
    "    thershold: float)\\\n",
    "        -> float:\n",
    "    \"\"\"\n",
    "    match keywords with candidates in a document\n",
    "\n",
    "    Args:\n",
    "        keywords_emb (List[np.ndarray]): list of keywords embeddings\n",
    "        candidates_emb (List[np.ndarray]): list of document's candidates embeddings\n",
    "        thershold (float): threshold\n",
    "\n",
    "    Returns:\n",
    "        float: score\n",
    "    \n",
    "    example:\n",
    "        >>> match_keywords(keywords_emb, candidates_emb, thershold=0.5)\n",
    "        >>> 0.8\n",
    "    \"\"\"\n",
    "    # candidates_emb_shapes = list(map(lambda emb: emb.shape, candidates_emb))\n",
    "    # keys_emb_shapes = list(map(lambda emb: emb.reshape(1, -1).shape, keywords_emb))\n",
    "    # check if candidates_emb shape is 1d\n",
    "    # def shape_check(emb):\n",
    "    #     if emb.size == 1:\n",
    "    #         return emb.reshape(-1, 1)\n",
    "    #     else:\n",
    "    #         return emb\n",
    "    \n",
    "    # candidates_emb = list(map(shape_check, candidates_emb))\n",
    "    # keywords_emb = list(map(shape_check, keywords_emb))\n",
    "\n",
    "    # combination = list(zip(keywords_emb,candidates_emb))\n",
    "    # print(\"checked\")\n",
    "\n",
    "    # similarities = list(map(lambda comb:\n",
    "    #                 np.array(cos_sim(comb[0],\n",
    "    #                 comb[1])).clip(-1, 1).round(6),\n",
    "    #                 combination))\n",
    "\n",
    "\n",
    "    # similarities = list(map(lambda comb:\n",
    "    #                 print(comb[0].shape,comb[1].shape),\n",
    "    #                 combination))\n",
    "\n",
    "    # similarities = list(map(lambda cand:\n",
    "    #                 print(np.array(keywords_emb).shape, cand.reshape(cand.shape[1],cand.shape[2]).shape),\n",
    "    #                 candidates_emb))\n",
    "    # .__array__().max()\n",
    "\n",
    "    # similarities = list(map(lambda cand:\n",
    "    #                 print(np.array(keywords_emb).shape, cand.shape),\n",
    "    #                 candidates_emb))\n",
    "\n",
    "    similarities = list(map(lambda cand:\n",
    "                    cos_sim(np.array(keywords_emb), cand.reshape(cand.shape[0],cand.shape[1])).__array__().max(axis=1).round(6).clip(-1, 1),\n",
    "                    candidates_emb))\n",
    "    # print shape of similarities\n",
    "    # print(np.array(similarities).shape)\n",
    "\n",
    "\n",
    "\n",
    "    # similarities = list(map(lambda cand:\n",
    "    #                 np.array(list(map(lambda key:\n",
    "    #                 cos_sim(key, cand.reshape(cand.shape[1],cand.shape[2])).clip(-1, 1).round(6),keywords_emb))),\n",
    "    #                 # print(key.shape, cand.reshape(cand.shape[1],cand.shape[2]).shape),keywords_emb))),\n",
    "    #                 candidates_emb))\n",
    "\n",
    "    # def fn_ (x: np.array):\n",
    "    #     \"\"\"\n",
    "    #     return the no. of matched keywords\n",
    "    #     \"\"\"\n",
    "    #     if not np.sum(x>= thershold):\n",
    "    #         return 0\n",
    "    #     if np.sum(x >= thershold) > 1.0:\n",
    "    #         return 1.0\n",
    "    #     return np.sum(x >= thershold)\n",
    "\n",
    "    # res = sum(map(fn_, similarities))\n",
    "    # return res/len(keywords_emb)\n",
    "    # print(type(combination[0][1]))\n",
    "\n",
    "    # similarities = list(map(lambda comb:\n",
    "    #                 list(map(lambda comb_:\n",
    "    #                 print(type(comb[0]),type(comb_)),comb[1])),\n",
    "    #                 combination))\n",
    "\n",
    "    # similarities = list(map(lambda comb:\n",
    "    #                 print(type(comb[0]),type(comb[1])),\n",
    "    #                 combination))\n",
    "    # list(map(lambda sim: print((sim.__array__().shape)) , similarities))\n",
    "    return similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def para_keywords_pipeline_output(keywords,docs,n_gram_list,batch = 50):\n",
    "\n",
    "    def check_n_gram_dict(n_grams, doc, ind):\n",
    "        res = []\n",
    "        for n_gram in n_grams:\n",
    "            x = n_gram_dict[ind].get(n_gram)\n",
    "            if x is None:\n",
    "                x =  get_candidates([n_gram], doc)\n",
    "                n_gram_dict[ind][n_gram] = x\n",
    "                res.append(x)\n",
    "            else:\n",
    "                res.append(x)\n",
    "        return res\n",
    "\n",
    "\n",
    "    def new_check_n_gram_dict(n_grams, doc, ind):\n",
    "        def fn(n_gram):\n",
    "            x = n_gram_dict[ind].get(n_gram)\n",
    "            if x is None:\n",
    "                # print(\"hi\",ind)\n",
    "                x =  get_candidates([n_gram], doc)\n",
    "                n_gram_dict[ind][n_gram] = x\n",
    "                # cand_emb_dict[ind][n_gram] = emb_keywords(x)\n",
    "                return x\n",
    "            else:\n",
    "                # print(\"f\",ind)\n",
    "                return x\n",
    "        return list(map(fn, n_grams))\n",
    "\n",
    "\n",
    "    def check_candidates_dict(n_grams, doc, ind):\n",
    "        def fn(n_gram):\n",
    "            x = cand_emb_dict[ind].get(n_gram)\n",
    "            if x is None:\n",
    "                cand_emb_dict[ind][n_gram] = emb_keywords(n_gram_dict[ind][n_gram])\n",
    "                return cand_emb_dict[ind][n_gram]\n",
    "            else:\n",
    "                return x\n",
    "        return list(map(fn,n_grams))\n",
    "\n",
    "    n_docs = len(docs)\n",
    "    if n_docs < batch:\n",
    "        batch = n_docs\n",
    "    \n",
    "    n_gram_dict = {ind: {n_gram: None for n_gram in set([item for sublist in n_gram_list for item in sublist])} for ind ,doc in enumerate(docs)}\n",
    "    cand_emb_dict = {ind: {n_gram: None for n_gram in set([item for sublist in n_gram_list for item in sublist])} for ind ,doc in enumerate(docs)}\n",
    "\n",
    "    n_grams_list = []\n",
    "    students_candidates_list = []\n",
    "    keywords_embeddings_list = []\n",
    "    students_candidates_emb_list = []\n",
    "    times = []\n",
    "    hh_list = []\n",
    "\n",
    "    # all model answers\n",
    "    for ind, ans in enumerate(keywords):\n",
    "        print(\"model answer\",ind)\n",
    "        t1 = time.perf_counter()\n",
    "        students_n_grams = key_words.get_n_grams(ans)\n",
    "        keywords_embeddings =  list(map(BERT.model.encode, ans))\n",
    "\n",
    "        students_candidates_list_s = []\n",
    "        students_candidates_emb_list_s = []\n",
    "        # hh_list_s = []\n",
    "\n",
    "        # do in batches\n",
    "        for i in range(0,n_docs,batch):\n",
    "            students_candidates = [0]\n",
    "\n",
    "            students_candidates = list(map(lambda doc:\n",
    "                            new_check_n_gram_dict(students_n_grams, doc[1], i+doc[0]),\n",
    "                            enumerate(docs[i:i+batch])))\n",
    "            students_candidates_emb = list(map(lambda doc:\n",
    "                            check_candidates_dict(students_n_grams, doc[1], i+doc[0]),\n",
    "                            enumerate(docs[i:i+batch])))\n",
    "            students_candidates_list_s.extend(students_candidates)\n",
    "            # hh_list_s.extend(hh)\n",
    "            students_candidates_emb_list_s.extend(students_candidates_emb)\n",
    "\n",
    "        if n_docs % batch != 0 and n_docs > batch:\n",
    "            students_candidates = list(map(lambda doc:\n",
    "                            check_n_gram_dict(students_n_grams, doc[1], i+doc[0]),\n",
    "                            enumerate(docs[i+batch:])))\n",
    "            # students_candidates_emb = list(map(lambda doc:\n",
    "            #                 check_candidates_dict(students_n_grams, doc[1], i+doc[0]),\n",
    "            #                 enumerate(docs[i:i+batch])))\n",
    "            students_candidates_emb =  list(map(emb_keywords, students_candidates))\n",
    "\n",
    "            students_candidates_list_s.extend(students_candidates)\n",
    "            students_candidates_emb_list_s.extend(students_candidates_emb)\n",
    "\n",
    "        n_grams_list.append(students_n_grams)\n",
    "        keywords_embeddings_list.append(keywords_embeddings)\n",
    "\n",
    "        students_candidates_list.append(students_candidates_list_s)\n",
    "        students_candidates_emb_list.append(students_candidates_emb_list_s)\n",
    "        # hh_list.append(hh_list_s)\n",
    "        times.append(time.perf_counter() - t1)\n",
    "    # retun a dict of each n_grams_list, students_candidates_list, keywords_embeddings_list, students_candidates_emb_list\n",
    "    return {\n",
    "        \"n_gram_dict\": n_gram_dict,\n",
    "        \"n_grams_list\": n_grams_list,\n",
    "        \"students_candidates_list\": students_candidates_list,\n",
    "        \"keywords_embeddings_list\": keywords_embeddings_list,\n",
    "        \"students_candidates_emb_list\": students_candidates_emb_list,\n",
    "        \"times\": times\n",
    "    }\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def para_keywords_pipeline_output_dict(keywords,docs,n_gram_list,batch = 50):\n",
    "\n",
    "    def check_n_gram_dict(n_grams, doc, ind):\n",
    "        def fn(n_gram):\n",
    "            x = n_gram_dict[ind].get(n_gram)\n",
    "            if x is None:\n",
    "                x =  get_candidates([n_gram], doc)\n",
    "                n_gram_dict[ind][n_gram] = x\n",
    "                return x\n",
    "            else:\n",
    "                return x\n",
    "        return list(map(fn, n_grams))\n",
    "\n",
    "    n_docs = len(docs)\n",
    "    if n_docs < batch:\n",
    "        batch = n_docs\n",
    "    \n",
    "    n_gram_dict = {ind: {n_gram: None for n_gram in set([item for sublist in n_gram_list for item in sublist])} for ind ,doc in enumerate(docs)}\n",
    "\n",
    "    n_grams_list = []\n",
    "    students_candidates_list = []\n",
    "    keywords_embeddings_list = []\n",
    "    students_candidates_emb_list = []\n",
    "    times = []\n",
    "\n",
    "    # all model answers\n",
    "    for ind, ans in enumerate(keywords):\n",
    "        print(\"model answer\",ind)\n",
    "        t1 = time.perf_counter()\n",
    "        students_n_grams = key_words.get_n_grams(ans)\n",
    "        keywords_embeddings =  list(map(BERT.model.encode, ans))\n",
    "\n",
    "        students_candidates_list_s = []\n",
    "        students_candidates_emb_list_s = []\n",
    "        # do in batches\n",
    "        for i in range(0,n_docs,batch):\n",
    "            students_candidates = list(map(lambda doc:\n",
    "                            check_n_gram_dict(students_n_grams, doc[1], i+doc[0]),\n",
    "                            enumerate(docs[i:i+batch])))\n",
    "            students_candidates_emb =  list(map( lambda st: list(map( emb_keywords ,st)), students_candidates))\n",
    "            # students_candidates_emb =  list(map(emb_d, students_candidates))\n",
    "            students_candidates_list_s.extend(students_candidates)\n",
    "            students_candidates_emb_list_s.extend(students_candidates_emb)\n",
    "\n",
    "        if n_docs % batch != 0 and n_docs > batch:\n",
    "            students_candidates = list(map(lambda doc:\n",
    "                            check_n_gram_dict(students_n_grams, doc[1], i+doc[0]),\n",
    "                            enumerate(docs[i+batch:])))\n",
    "            students_candidates_emb =  list(map( lambda st: list(map( emb_keywords ,st)), students_candidates))\n",
    "\n",
    "            students_candidates_list_s.extend(students_candidates)\n",
    "            students_candidates_emb_list_s.extend(students_candidates_emb)\n",
    "\n",
    "        n_grams_list.append(students_n_grams)\n",
    "        keywords_embeddings_list.append(keywords_embeddings)\n",
    "        students_candidates_list.append(students_candidates_list_s)\n",
    "        students_candidates_emb_list.append(students_candidates_emb_list_s)\n",
    "        times.append(time.perf_counter() - t1)\n",
    "    return {\n",
    "        \"n_gram_dict\": n_gram_dict,\n",
    "        \"n_grams_list\": n_grams_list,\n",
    "        \"students_candidates_list\": students_candidates_list,\n",
    "        \"keywords_embeddings_list\": keywords_embeddings_list,\n",
    "        \"students_candidates_emb_list\": students_candidates_emb_list,\n",
    "        \"times\": times\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def emb_(keywords):\n",
    "    # x = list(map(lambda st: list(map(lambda k:\n",
    "    #  list(map(lambda ans: len(ans),k)) ,st)) ,st_cands))\n",
    "    #! x = list(map(lambda k:list(map(lambda x: BERT.model.encode(str(x)) ,k)) ,keywords))\n",
    "    x = list(map(lambda k:list(map(lambda x: get_words_emb(str(x)) ,k)) ,keywords))\n",
    "    # x = np.array(list(map(lambda k:\n",
    "    #  np.array(list(map(lambda x: BERT.model.encode(str(x)), k))),st_cands)))\n",
    "    # if x.ndim == 3:\n",
    "    #     x = x.reshape(max(x.shape[1],x.shape[0]),x.shape[2])\n",
    "    # print(\"emb\",x.shape)\n",
    "    return x\n",
    "    # return list(map(lambda k: np.array(list(map(lambda x: BERT.model.encode(str(x)), k))),keywords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normal_keywords_pipeline_output(keywords,docs,n_gram_list,batch = 50):\n",
    "    # if not isinstance(docs, list):\n",
    "    #     docs = [docs]\n",
    "\n",
    "    n_docs = len(docs)\n",
    "    if n_docs < batch:\n",
    "        batch = n_docs\n",
    "    keywords_embeddings_list = []\n",
    "    students_candidates_emb_list = []\n",
    "\n",
    "    students_n_grams = key_words.get_n_grams(keywords)\n",
    "    keywords_embeddings =  list(map(BERT.model.encode, keywords))\n",
    "    #! keywords_embeddings =  list(map(BERT.model.encode, [keywords]))\n",
    "    # print(\"keywords_embeddings\",len(keywords_embeddings))\n",
    "    # print(\"keywords_embeddings\",keywords_embeddings[0].shape)\n",
    "\n",
    "    students_candidates_emb_list_s = []\n",
    "    # do in batches\n",
    "    print(\"students n grams\",students_n_grams)\n",
    "    for i in range(0,n_docs,batch):\n",
    "        students_candidates = list(map(lambda doc: get_candidates(students_n_grams, doc),docs[i:i+batch]))\n",
    "        # print  lens of students_candidates\n",
    "        print(\"students_candidates\",[[len(y) for y in st ]for st in students_candidates])\n",
    "        # students_candidates = list(map(lambda doc:\n",
    "        #                 new_check_n_gram_dict(students_n_grams, doc[1], i+doc[0]),\n",
    "        #                 enumerate(docs[i:i+batch])))\n",
    "\n",
    "        # students_candidates_emb = emb_keywords(n_gram_dict[ind][n_gram])\n",
    "        # list(map(lambda n_grams:list(map(lambda doc:\n",
    "        #     key_words.get_candidates(n_grams, doc),docs[i:i+batch])),students_n_grams))\n",
    "\n",
    "        # students_candidates_emb =  list(map(BERT.model.encode, students_candidates))\n",
    "        # students_candidates_emb =  list(map(lambda st: list(map( lambda x: BERT.model.encode(str(x)), st)),students_candidates))\n",
    "        # students_candidates_emb = list(map(lambda st: list(map( lambda x: len(x), st)),students_candidates))\n",
    "        # print(list(map(lambda st: list(map( lambda x: len(x), st)),students_candidates)))\n",
    "        # print(list(map(lambda st: emb_keywords(st).shape,students_candidates)))\n",
    "        students_candidates_emb =  list(map( emb_, students_candidates))\n",
    "        # print(emb_keywords(students_candidates).shape)\n",
    "        # students_candidates_emb = list(map(lambda doc:\n",
    "        #                 check_candidates_dict(students_n_grams, doc[1], i+doc[0]),\n",
    "        #                 enumerate(docs[i:i+batch])))\n",
    "        students_candidates_emb_list_s.extend(students_candidates_emb)\n",
    "\n",
    "    if n_docs % batch != 0 and n_docs > batch:\n",
    "        students_candidates = list(map(lambda doc:\n",
    "            get_candidates(students_n_grams, doc),docs[i:i+batch]))\n",
    "        # students_candidates_emb =  list(map( lambda st: list(map(emb_keywords, st)),students_candidates))\n",
    "        students_candidates_emb =  list(map(lambda st: list(map( lambda x: BERT.model.encode(str(x)), st)),students_candidates))\n",
    "        students_candidates_emb_list_s.extend(students_candidates_emb)\n",
    "\n",
    "    keywords_embeddings_list.append(keywords_embeddings)\n",
    "    students_candidates_emb_list.append(students_candidates_emb_list_s)\n",
    "    return {\n",
    "        \"keywords_embeddings_list\": keywords_embeddings_list,\n",
    "        \"students_candidates_emb_list\": students_candidates_emb_list,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def emb_keywords(keywords):\n",
    "    # x = np.array(list(map(lambda k: np.array(list(map(lambda x: BERT.model.encode(str(x)), k))),keywords)))\n",
    "    x = np.array(list(map(lambda k: np.array(list(map(lambda x: get_words_emb(str(x)), k))),keywords)))\n",
    "    # print(\"emb be\",x.shape)\n",
    "\n",
    "    if x.ndim == 3:\n",
    "        x = x.reshape(max(x.shape[1],x.shape[0]),x.shape[2])\n",
    "    # print(\"emb\",x.shape)\n",
    "    return x\n",
    "    # return list(map(lambda k: np.array(list(map(lambda x: BERT.model.encode(str(x)), k))),keywords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_emb_dict = {}\n",
    "def get_words_emb(word):\n",
    "    if word in words_emb_dict:\n",
    "        return words_emb_dict[word]\n",
    "    else:\n",
    "        words_emb_dict[word] = BERT.model.encode(word)\n",
    "        return words_emb_dict[word]\n",
    "\n",
    "def emb_d(keywords):\n",
    "    return np.array(list(map(lambda k: np.array(list(map(lambda x: get_words_emb(str(x)) ,k))) ,keywords)))\n",
    "    # return list(map(lambda k:list(map(lambda x: get_words_emb(str(x)) ,k)) ,keywords))\n",
    "\n",
    "def normal_keywords_pipeline_output_with_dict(keywords,docs,n_gram_list,batch = 10):\n",
    "    # if not isinstance(docs, list):\n",
    "    #     docs = [docs]\n",
    "    words_emb_dict = {}\n",
    "\n",
    "    n_docs = len(docs)\n",
    "    if n_docs < batch:\n",
    "        batch = n_docs\n",
    "    keywords_embeddings_list = []\n",
    "    students_candidates_emb_list = []\n",
    "\n",
    "    students_n_grams = key_words.get_n_grams(keywords)\n",
    "    keywords_embeddings =  list(map(BERT.model.encode, keywords))\n",
    "    #! keywords_embeddings =  list(map(BERT.model.encode, [keywords]))\n",
    "    students_candidates_emb_list_s = []\n",
    "    # do in batches\n",
    "    # print(\"students n grams\",students_n_grams)\n",
    "    for i in range(0,n_docs,batch):\n",
    "        students_candidates = list(map(lambda doc: get_candidates(students_n_grams, doc),docs[i:i+batch]))\n",
    "        # print(\"students_candidates\",[[len(y) for y in st ]for st in students_candidates])\n",
    "        students_candidates_emb =  list(map( emb_d, students_candidates))\n",
    "        # students_candidates_emb =  list(map( emb_d, students_candidates))\n",
    "        students_candidates_emb_list_s.extend(students_candidates_emb)\n",
    "\n",
    "    if n_docs % batch != 0 and n_docs > batch:\n",
    "        students_candidates = list(map(lambda doc:\n",
    "            get_candidates(students_n_grams, doc),docs[i:i+batch]))\n",
    "        # students_candidates_emb =  list(map( lambda st: list(map(emb_keywords, st)),students_candidates))\n",
    "        # students_candidates_emb =  list(map(lambda st: list(map( lambda x: BERT.model.encode(str(x)), st)),students_candidates))\n",
    "        students_candidates_emb =  list(map( emb_d, students_candidates))\n",
    "        students_candidates_emb_list_s.extend(students_candidates_emb)\n",
    "\n",
    "    keywords_embeddings_list.append(keywords_embeddings)\n",
    "    students_candidates_emb_list.append(students_candidates_emb_list_s)\n",
    "    return {\n",
    "        \"keywords_embeddings_list\": keywords_embeddings_list,\n",
    "        \"students_candidates_emb_list\": students_candidates_emb_list,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grading(keywords_embeddings_list,students_candidates_emb_list,thershold=0.5):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        keywords_embeddings_list: list of list of list of embeddings\n",
    "        students_candidates_emb_list: list of list of list of embeddings\n",
    "        thershold: thershold for the similarity\n",
    "    Returns:\n",
    "        a list of list of list of grades\n",
    "    \"\"\"\n",
    "    grades = []\n",
    "    for i in range(len(keywords_embeddings_list)):\n",
    "        grades.append(np.array(list(map(lambda st_cand:\n",
    "                match_keywords(keywords_embeddings_list[i], st_cand,\n",
    "                thershold=thershold),\n",
    "                students_candidates_emb_list[i]\n",
    "                ))))\n",
    "    grades = np.array(list(map(lambda sim: (sim.__array__().max(axis=1) >thershold).sum(axis=1)/float(sim.shape[-1]) , grades)))\n",
    "    return grades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 5)\n",
      "(5,)\n",
      "(5,)\n",
      "(2, 5)\n",
      "(5,)\n",
      "(5,)\n",
      "(2, 5)\n",
      "(5,)\n",
      "(5,)\n",
      "(2, 5)\n",
      "(5,)\n",
      "(5,)\n",
      "(2, 5)\n",
      "(5,)\n",
      "(5,)\n",
      "(2, 5)\n",
      "(5,)\n",
      "(5,)\n",
      "(2, 5)\n",
      "(5,)\n",
      "(5,)\n",
      "(2, 5)\n",
      "(5,)\n",
      "(5,)\n",
      "(2, 5)\n",
      "(5,)\n",
      "(5,)\n",
      "(2, 5)\n",
      "(5,)\n",
      "(5,)\n",
      "(2, 5)\n",
      "(5,)\n",
      "(5,)\n",
      "(2, 5)\n",
      "(5,)\n",
      "(5,)\n",
      "(2, 5)\n",
      "(5,)\n",
      "(5,)\n",
      "(2, 5)\n",
      "(5,)\n",
      "(5,)\n",
      "(2, 5)\n",
      "(5,)\n",
      "(5,)\n",
      "(2, 5)\n",
      "(5,)\n",
      "(5,)\n",
      "(2, 5)\n",
      "(5,)\n",
      "(5,)\n",
      "(2, 5)\n",
      "(5,)\n",
      "(5,)\n",
      "(2, 5)\n",
      "(5,)\n",
      "(5,)\n",
      "(2, 5)\n",
      "(5,)\n",
      "(5,)\n",
      "(2, 5)\n",
      "(5,)\n",
      "(5,)\n",
      "(2, 5)\n",
      "(5,)\n",
      "(5,)\n",
      "(2, 5)\n",
      "(5,)\n",
      "(5,)\n",
      "(2, 5)\n",
      "(5,)\n",
      "(5,)\n",
      "(2, 5)\n",
      "(5,)\n",
      "(5,)\n",
      "(2, 5)\n",
      "(5,)\n",
      "(5,)\n",
      "(2, 5)\n",
      "(5,)\n",
      "(5,)\n",
      "(2, 5)\n",
      "(5,)\n",
      "(5,)\n",
      "(2, 5)\n",
      "(5,)\n",
      "(5,)\n",
      "(2, 5)\n",
      "(5,)\n",
      "(5,)\n",
      "(2, 5)\n",
      "(5,)\n",
      "(5,)\n",
      "(2, 5)\n",
      "(5,)\n",
      "(5,)\n",
      "(2, 5)\n",
      "(5,)\n",
      "(5,)\n",
      "(2, 5)\n",
      "(5,)\n",
      "(5,)\n",
      "(2, 5)\n",
      "(5,)\n",
      "(5,)\n",
      "(2, 5)\n",
      "(5,)\n",
      "(5,)\n",
      "(2, 5)\n",
      "(5,)\n",
      "(5,)\n",
      "(2, 5)\n",
      "(5,)\n",
      "(5,)\n",
      "(2, 5)\n",
      "(5,)\n",
      "(5,)\n",
      "(2, 5)\n",
      "(5,)\n",
      "(5,)\n"
     ]
    }
   ],
   "source": [
    "g = grading(keywords_embeddings_lis,d['students_candidates_emb_list'],0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 20)"
      ]
     },
     "execution_count": 433,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_candidates = list(map(lambda ans: key_words.candidates_tokens(ans,n_gram_range=(1,2)),ess_2_model_answers))\n",
    "model_candidate_emb = list(map(lambda cand:BERT.model.encode(cand),model_candidates))\n",
    "keywords = list(map(lambda x: maximal_marginal_relevance(\n",
    "    x[0].reshape(1, -1),x[1],x[2]),zip(emb_dict['ess_2_model_answers_emb'],\n",
    "    model_candidate_emb,model_candidates)))\n",
    "students_n_grams = list(map(lambda keyword: sorted(key_words.get_n_grams(keyword)),keywords))\n",
    "docs = df.query('EssaySet == 2')[\"EssayText\"].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model answer 0\n",
      "emb be (1, 40, 768)\n",
      "emb (40, 768)\n",
      "emb be (1, 41, 768)\n",
      "emb (41, 768)\n",
      "emb be (1, 45, 768)\n",
      "emb (45, 768)\n",
      "emb be (1, 45, 768)\n",
      "emb (45, 768)\n",
      "emb be (1, 27, 768)\n",
      "emb (27, 768)\n",
      "emb be (1, 25, 768)\n",
      "emb (25, 768)\n",
      "emb be (1, 12, 768)\n",
      "emb (12, 768)\n",
      "emb be (1, 11, 768)\n",
      "emb (11, 768)\n",
      "model answer 1\n",
      "emb be (1, 40, 768)\n",
      "emb (40, 768)\n",
      "emb be (1, 41, 768)\n",
      "emb (41, 768)\n",
      "emb be (1, 45, 768)\n",
      "emb (45, 768)\n",
      "emb be (1, 45, 768)\n",
      "emb (45, 768)\n",
      "emb be (1, 27, 768)\n",
      "emb (27, 768)\n",
      "emb be (1, 25, 768)\n",
      "emb (25, 768)\n",
      "emb be (1, 12, 768)\n",
      "emb (12, 768)\n",
      "emb be (1, 11, 768)\n",
      "emb (11, 768)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.41940510000131326"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1 = time.perf_counter()\n",
    "dd = para_keywords_pipeline_output_dict(keywords[0:2],docs[:4],students_n_grams,batch=10)\n",
    "time.perf_counter() - t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 1.        , 1.        , 0.57142857],\n",
       "       [1.        , 1.        , 0.85714286, 0.42857143]])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grading(dd['keywords_embeddings_list'],dd['students_candidates_emb_list'],0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 759,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model answer 0\n",
      "[40]\n",
      "emb (40, 768)\n",
      "[44]\n",
      "emb (44, 768)\n",
      "[43]\n",
      "emb (43, 768)\n",
      "[45]\n",
      "emb (45, 768)\n",
      "[22]\n",
      "emb (22, 768)\n",
      "[21]\n",
      "emb (21, 768)\n",
      "[43]\n",
      "emb (43, 768)\n",
      "[44]\n",
      "emb (44, 768)\n"
     ]
    }
   ],
   "source": [
    "d = keywords_pipeline_output([keywords[0]],docs[:4],students_n_grams,batch=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 678,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model answer 0\n",
      "model answer 0\n",
      "model answer 0\n",
      "model answer 0\n",
      "model answer 0\n",
      "model answer 0\n",
      "model answer 0\n",
      "model answer 0\n",
      "10 s ± 233 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit keywords_pipeline_output([keywords[0]],docs[:5],students_n_grams,batch=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 679,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7.98857470000803]"
      ]
     },
     "execution_count": 679,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d['times']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 680,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model answer 0\n",
      "model answer 0\n",
      "model answer 0\n",
      "model answer 0\n",
      "model answer 0\n",
      "model answer 0\n",
      "model answer 0\n",
      "model answer 0\n",
      "9.75 s ± 47.9 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit para_keywords_pipeline_output([keywords[0]],docs[:5],students_n_grams,batch=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "students n grams [(1, 2), (2, 3)]\n",
      "students_candidates [[40, 44], [43, 45], [22, 21]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\oeb\\AppData\\Local\\Temp/ipykernel_15280/2889523460.py:4: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  x = np.array(list(map(lambda k:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5.166980099998909"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1 = time.perf_counter()\n",
    "dn = normal_keywords_pipeline_output(keywords[0],docs[:3],students_n_grams,batch=10)\n",
    "time.perf_counter() - t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.6, 0.8, 0.6]])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grading(dn['keywords_embeddings_list'],dn['students_candidates_emb_list'],0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\oeb\\AppData\\Local\\Temp/ipykernel_15916/2509446158.py:11: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return np.array(list(map(lambda k: np.array(list(map(lambda x: get_words_emb(str(x)) ,k))) ,keywords)))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.14998909999849275"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1 = time.perf_counter()\n",
    "dn = normal_keywords_pipeline_output_with_dict(keywords[0],docs[:3],students_n_grams,batch=10)\n",
    "time.perf_counter() - t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# {\"asndgv\":[1,2,3,3]}*2\n",
    "embdd = BERT.model.encode(\"asndgv\")\n",
    "dic = {}\n",
    "dic = {\"asndgv\"+str(int(k)) : embdd for k in list(np.arange(1,1000*100))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.000083923339844"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.getsizeof(dic) / 1024 / 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model answer 0\n",
      "model answer 1\n",
      "model answer 2\n",
      "model answer 3\n"
     ]
    }
   ],
   "source": [
    "d1 = para_keywords_pipeline_output(keywords[:4],docs[:4],students_n_grams,batch=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.12975770000048215,\n",
       " 0.12392139999974461,\n",
       " 0.12380529999973078,\n",
       " 0.12464680000084627]"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d1['times']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'keywords_embeddings_list'"
      ]
     },
     "execution_count": 540,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"keywords_embeddings_list\"\n",
    "# n models\n",
    "# [0] n of keywords ARRAY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'students_candidates_emb_list'"
      ]
     },
     "execution_count": 539,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"students_candidates_emb_list\"\n",
    "# n models\n",
    "# [0] n of students\n",
    "# [0][0] each student's candidates (40, 768) , (41, 768) ARRAY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arr 0 (2, 768)\n",
      "arr 1 (2, 768)\n",
      "arr 2 (2, 768)\n",
      "arr 3 (2, 768)\n"
     ]
    }
   ],
   "source": [
    "for i, v in enumerate(dd['students_candidates_emb_list'][0]):\n",
    "    try:\n",
    "        print(\"arr\",i, v.shape)\n",
    "    except:\n",
    "        print(i, len(v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# perfect now the full essays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ngram_top_n_diversity(docs):\n",
    "    lens = []\n",
    "    tokens = []\n",
    "    for doc in docs:\n",
    "        tokens_ = [w.translate(str.maketrans('', '', string.punctuation)) for w in [doc]]\n",
    "        tokens_ = [w for w in set(doc.split()) if w not in stop_words]\n",
    "        tokens.append(tokens_)\n",
    "        lens.append(len(tokens))\n",
    "\n",
    "    model_candidates = tokens\n",
    "    model_candidate_emb = np.array(list(map(lambda cand:BERT.model.encode(cand),model_candidates)))\n",
    "    print(model_candidate_emb.shape)\n",
    "    # list(map(lambda st:cos_sim(model_answer_emb,st),model_candidate_emb))\n",
    "    # cos_sim(model_answer_emb, model_candidate_emb)\n",
    "\n",
    "    lens = np.median(np.array(lens))\n",
    "    return lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23.5\n",
      "19.5\n",
      "33.0\n"
     ]
    }
   ],
   "source": [
    "for i,v in ess_dict.items():\n",
    "    l = get_ngram_top_n_diversity(v)\n",
    "    print(l)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "ess_dict = {\n",
    "    \"ess_1_model_answers\": ess_1_model_answers,\n",
    "    \"ess_2_model_answers\": ess_2_model_answers,\n",
    "    \"ess_3_model_answers\": ess_3_model_answers,\n",
    "    \"ess_4_model_answers\": ess_4_model_answers,\n",
    "    \"ess_5_model_answers\": ess_5_model_answers,\n",
    "    \"ess_6_model_answers\": ess_6_model_answers,\n",
    "    \"ess_7_model_answers\": ess_7_model_answers,\n",
    "    \"ess_8_model_answers\": ess_8_model_answers,\n",
    "    \"ess_9_model_answers\": ess_9_model_answers,\n",
    "    \"ess_10_model_answers\": ess_10_model_answers,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19.0\n",
      "19.0\n",
      "30.0\n",
      "34.0\n",
      "37.0\n",
      "29.5\n",
      "20.5\n",
      "32.0\n",
      "35.0\n",
      "25.5\n"
     ]
    }
   ],
   "source": [
    "for i,v in ess_dict.items():\n",
    "    l = get_ngram_top_n_diversity(v)\n",
    "    if l < 10:\n",
    "        # short \n",
    "        # \n",
    "        tokens = [w.translate(str.maketrans('', '', string.punctuation)) for w in [doc]]\n",
    "        tokens = [w for w in set(doc.split()) if w not in stop_words]\n",
    "        model_candidates = tokens\n",
    "        model_candidate_emb = BERT.model.encode(model_candidates)\n",
    "        cos_sim(model_answer_emb, model_candidate_emb)\n",
    "    print(l)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_keys_div(ind,essay,top_n=5,diversities=[0.8]):\n",
    "    for div in diversities:\n",
    "        keywords = list(map(lambda x: maximal_marginal_relevance(x[0].reshape(1, -1),x[1],x[2],top_n=top_n,diversity=div),\n",
    "            zip(emb_dict[f'ess_{essay}_model_answers_emb'],\n",
    "            model_candidate_emb,model_candidates)))\n",
    "        print(div,keywords[ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_n_l = [7]*10\n",
    "diversity_l = [0.6,0.6, 0.7,0.7,0.7,0.7,0.6,0.7,0.7,0.7]\n",
    "ngram_range_l = [(2,2),(2,2),(2,3),(2,3),(2,3),(2,3),(2,2),(2,3),(2,3),(2,3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "essayset 1\n",
      "model answer 0\n",
      "model answer 1\n",
      "model answer 2\n",
      "model answer 3\n",
      "model answer 4\n",
      "model answer 5\n",
      "model answer 6\n",
      "model answer 7\n",
      "model answer 8\n",
      "model answer 9\n",
      "model answer 10\n",
      "model answer 11\n",
      "model answer 12\n",
      "model answer 13\n",
      "model answer 14\n",
      "model answer 15\n",
      "model answer 16\n",
      "model answer 17\n",
      "model answer 18\n",
      "model answer 19\n",
      "model answer 20\n",
      "model answer 21\n",
      "essayset 2\n",
      "model answer 0\n",
      "model answer 1\n",
      "model answer 2\n",
      "model answer 3\n",
      "model answer 4\n",
      "model answer 5\n",
      "model answer 6\n",
      "model answer 7\n",
      "model answer 8\n",
      "model answer 9\n",
      "model answer 10\n",
      "model answer 11\n",
      "model answer 12\n",
      "model answer 13\n",
      "model answer 14\n",
      "model answer 15\n",
      "model answer 16\n",
      "model answer 17\n",
      "model answer 18\n",
      "model answer 19\n",
      "model answer 20\n",
      "model answer 21\n",
      "essayset 3\n",
      "model answer 0\n",
      "model answer 1\n",
      "model answer 2\n",
      "model answer 3\n",
      "model answer 4\n",
      "model answer 5\n",
      "model answer 6\n",
      "model answer 7\n",
      "model answer 8\n",
      "model answer 9\n",
      "model answer 10\n",
      "model answer 11\n",
      "model answer 12\n",
      "model answer 13\n",
      "model answer 14\n",
      "model answer 15\n",
      "model answer 16\n",
      "model answer 17\n",
      "model answer 18\n",
      "model answer 19\n",
      "model answer 20\n",
      "model answer 21\n",
      "essayset 4\n",
      "model answer 0\n",
      "model answer 1\n",
      "model answer 2\n",
      "model answer 3\n",
      "model answer 4\n",
      "model answer 5\n",
      "model answer 6\n",
      "model answer 7\n",
      "model answer 8\n",
      "model answer 9\n",
      "model answer 10\n",
      "model answer 11\n",
      "model answer 12\n",
      "model answer 13\n",
      "model answer 14\n",
      "model answer 15\n",
      "model answer 16\n",
      "model answer 17\n",
      "model answer 18\n",
      "model answer 19\n",
      "model answer 20\n",
      "model answer 21\n",
      "essayset 5\n",
      "model answer 0\n",
      "model answer 1\n",
      "model answer 2\n",
      "model answer 3\n",
      "model answer 4\n",
      "model answer 5\n",
      "model answer 6\n",
      "model answer 7\n",
      "model answer 8\n",
      "model answer 9\n",
      "model answer 10\n",
      "model answer 11\n",
      "model answer 12\n",
      "model answer 13\n",
      "model answer 14\n",
      "model answer 15\n",
      "model answer 16\n",
      "model answer 17\n",
      "model answer 18\n",
      "model answer 19\n",
      "model answer 20\n",
      "model answer 21\n",
      "essayset 6\n",
      "model answer 0\n",
      "model answer 1\n",
      "model answer 2\n",
      "model answer 3\n",
      "model answer 4\n",
      "model answer 5\n",
      "model answer 6\n",
      "model answer 7\n",
      "model answer 8\n",
      "model answer 9\n",
      "model answer 10\n",
      "model answer 11\n",
      "model answer 12\n",
      "model answer 13\n",
      "model answer 14\n",
      "model answer 15\n",
      "model answer 16\n",
      "model answer 17\n",
      "model answer 18\n",
      "model answer 19\n",
      "model answer 20\n",
      "model answer 21\n",
      "essayset 7\n",
      "model answer 0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_13800/3865244871.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0mdocs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mquery\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'EssaySet == {essay+1}'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"EssayText\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m     \u001b[0mdd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpara_keywords_pipeline_output_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkeywords\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdocs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mstudents_n_grams\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[0mout\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34mf'ess_{essay+1}_keywords'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_13800/2122905503.py\u001b[0m in \u001b[0;36mpara_keywords_pipeline_output_dict\u001b[1;34m(keywords, docs, n_gram_list, batch)\u001b[0m\n\u001b[0;32m     38\u001b[0m                             \u001b[0mcheck_n_gram_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstudents_n_grams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdoc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m                             enumerate(docs[i:i+batch])))\n\u001b[1;32m---> 40\u001b[1;33m             \u001b[0mstudents_candidates_emb\u001b[0m \u001b[1;33m=\u001b[0m  \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mst\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0memb_keywords\u001b[0m \u001b[1;33m,\u001b[0m\u001b[0mst\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstudents_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m             \u001b[1;31m# students_candidates_emb =  list(map(emb_d, students_candidates))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m             \u001b[0mstudents_candidates_list_s\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstudents_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_13800/2122905503.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(st)\u001b[0m\n\u001b[0;32m     38\u001b[0m                             \u001b[0mcheck_n_gram_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstudents_n_grams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdoc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m                             enumerate(docs[i:i+batch])))\n\u001b[1;32m---> 40\u001b[1;33m             \u001b[0mstudents_candidates_emb\u001b[0m \u001b[1;33m=\u001b[0m  \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mst\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0memb_keywords\u001b[0m \u001b[1;33m,\u001b[0m\u001b[0mst\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstudents_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m             \u001b[1;31m# students_candidates_emb =  list(map(emb_d, students_candidates))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m             \u001b[0mstudents_candidates_list_s\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstudents_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_13800/3553079213.py\u001b[0m in \u001b[0;36memb_keywords\u001b[1;34m(keywords)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0memb_keywords\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkeywords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[1;31m# x = np.array(list(map(lambda k: np.array(list(map(lambda x: BERT.model.encode(str(x)), k))),keywords)))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mget_words_emb\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mkeywords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[1;31m# print(\"emb be\",x.shape)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_13800/3553079213.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(k)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0memb_keywords\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkeywords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[1;31m# x = np.array(list(map(lambda k: np.array(list(map(lambda x: BERT.model.encode(str(x)), k))),keywords)))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mget_words_emb\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mkeywords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[1;31m# print(\"emb be\",x.shape)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_13800/3553079213.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0memb_keywords\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkeywords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[1;31m# x = np.array(list(map(lambda k: np.array(list(map(lambda x: BERT.model.encode(str(x)), k))),keywords)))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mget_words_emb\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mkeywords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[1;31m# print(\"emb be\",x.shape)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_13800/3560175523.py\u001b[0m in \u001b[0;36mget_words_emb\u001b[1;34m(word)\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwords_emb_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m         \u001b[0mwords_emb_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBERT\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwords_emb_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\oeb\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sentence_transformers\\SentenceTransformer.py\u001b[0m in \u001b[0;36mencode\u001b[1;34m(self, sentences, batch_size, show_progress_bar, output_value, convert_to_numpy, convert_to_tensor, device, normalize_embeddings)\u001b[0m\n\u001b[0;32m    155\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    156\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 157\u001b[1;33m                 \u001b[0mout_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    158\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    159\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0moutput_value\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'token_embeddings'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\oeb\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    139\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 141\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    142\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\oeb\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\oeb\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sentence_transformers\\models\\Transformer.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, features)\u001b[0m\n\u001b[0;32m     49\u001b[0m             \u001b[0mtrans_features\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'token_type_ids'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'token_type_ids'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m         \u001b[0moutput_states\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mtrans_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m         \u001b[0moutput_tokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutput_states\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\oeb\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\oeb\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    848\u001b[0m             \u001b[0mpast_key_values_length\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpast_key_values_length\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    849\u001b[0m         )\n\u001b[1;32m--> 850\u001b[1;33m         encoder_outputs = self.encoder(\n\u001b[0m\u001b[0;32m    851\u001b[0m             \u001b[0membedding_output\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    852\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mextended_attention_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\oeb\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\oeb\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    522\u001b[0m                 )\n\u001b[0;32m    523\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 524\u001b[1;33m                 layer_outputs = layer_module(\n\u001b[0m\u001b[0;32m    525\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    526\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\oeb\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\oeb\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    408\u001b[0m         \u001b[1;31m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    409\u001b[0m         \u001b[0mself_attn_past_key_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpast_key_value\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mpast_key_value\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 410\u001b[1;33m         self_attention_outputs = self.attention(\n\u001b[0m\u001b[0;32m    411\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    412\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\oeb\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\oeb\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    335\u001b[0m         \u001b[0moutput_attentions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    336\u001b[0m     ):\n\u001b[1;32m--> 337\u001b[1;33m         self_outputs = self.self(\n\u001b[0m\u001b[0;32m    338\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    339\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\oeb\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\oeb\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    275\u001b[0m         \u001b[0mcontext_layer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcontext_layer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    276\u001b[0m         \u001b[0mnew_context_layer_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcontext_layer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mall_head_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 277\u001b[1;33m         \u001b[0mcontext_layer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcontext_layer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mnew_context_layer_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    278\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    279\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mcontext_layer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattention_probs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0moutput_attentions\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mcontext_layer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for essay in range(0,10):\n",
    "    # get top_n, diversity, ngram_range\n",
    "    print(\"essayset\",essay+1)\n",
    "    \n",
    "    model_candidates = list(map(lambda ans: key_words.candidates_tokens(ans,n_gram_range=ngram_range_l[essay+1]),ess_dict[f'ess_{essay+1}_model_answers']))\n",
    "    model_candidate_emb = list(map(lambda cand:BERT.model.encode(cand),model_candidates))\n",
    "    keywords = list(map(lambda x: maximal_marginal_relevance(\n",
    "        x[0].reshape(1, -1),x[1],x[2],top_n=top_n_l[essay+1],diversity=diversity_l[essay+1]),\n",
    "        zip(emb_dict[f'ess_{essay+1}_model_answers_emb'],\n",
    "        model_candidate_emb,model_candidates)))\n",
    "    students_n_grams = list(map(lambda keyword: sorted(key_words.get_n_grams(keyword)),keywords))\n",
    "\n",
    "    docs = df.query(f'EssaySet == {essay+1}')[\"EssayText\"].values.tolist()\n",
    "\n",
    "    dd = para_keywords_pipeline_output_dict(keywords,docs,students_n_grams,batch=10)\n",
    "    \n",
    "    save_obj(dd,f'data/results/keywords_res_essay_{essay+1}')\n",
    "    # out[f'ess_{essay+1}_keywords'] = dd\n",
    "    # save_obj(out,f'data/results/keywords_output_dict{essay+1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_obj(words_emb_dict,f'data/results/words_emb_dict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.000091552734375"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.getsizeof(words_emb_dict) / 1024 / 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "741.5214424000001\n",
      "1851.9714044000002\n",
      "3172.2670228999996\n",
      "4569.112715900001\n",
      "4641.669221300001\n",
      "1953.6475882999985\n"
     ]
    }
   ],
   "source": [
    "for i,v in out.items():\n",
    "    print(v['times'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ess_1_keywords',\n",
       " 'ess_2_keywords',\n",
       " 'ess_3_keywords',\n",
       " 'ess_4_keywords',\n",
       " 'ess_5_keywords',\n",
       " 'ess_6_keywords']"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[i for i in out.keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "essayset 7\n",
      "essayset 8\n",
      "essayset 9\n",
      "essayset 10\n"
     ]
    }
   ],
   "source": [
    "for essay in range(6,10):\n",
    "    # get top_n, diversity, ngram_range\n",
    "    print(\"essayset\",essay+1)\n",
    "    \n",
    "    model_candidates = list(map(lambda ans: key_words.candidates_tokens(ans,n_gram_range=ngram_range_l[essay+1]),ess_dict[f'ess_{essay+1}_model_answers']))\n",
    "    model_candidate_emb = list(map(lambda cand:BERT.model.encode(cand),model_candidates))\n",
    "    keywords = list(map(lambda x: maximal_marginal_relevance(\n",
    "        x[0].reshape(1, -1),x[1],x[2],top_n=top_n_l[essay+1],diversity=diversity_l[essay+1]),\n",
    "        zip(emb_dict[f'ess_{essay+1}_model_answers_emb'],\n",
    "        model_candidate_emb,model_candidates)))\n",
    "    students_n_grams = list(map(lambda keyword: sorted(key_words.get_n_grams(keyword)),keywords))\n",
    "\n",
    "    docs = df.query(f'EssaySet == {essay+1}')[\"EssayText\"].values.tolist()\n",
    "\n",
    "    dd = para_keywords_pipeline_output_dict(keywords,docs,students_n_grams,batch=10)\n",
    "    save_obj(dd,f'data/results/keywords_res_essay_{essay+1}')\n",
    "\n",
    "    del dd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### multilingual-MiniLM vs xmlr roberta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer(\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = df.query(f'EssaySet == {1+1}')[\"EssayText\"].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55.90439499999775\n"
     ]
    }
   ],
   "source": [
    "t1 = time.perf_counter()\n",
    "# 749mb\n",
    "x = model.encode(docs) # 30mb memory\n",
    "print(time.perf_counter() - t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "162.5099635000006"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1 = time.perf_counter()\n",
    "# 750mb\n",
    "x = BERT.model.encode(docs) # 100mb memory\n",
    "time.perf_counter() - t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.48043105006217957\n",
      "0.8797786831855774\n",
      "0.3992762267589569\n",
      "0.5011984705924988\n",
      "0.6617798805236816\n",
      "0.8037792444229126\n"
     ]
    }
   ],
   "source": [
    "print(predict('I am a student','I am hazem')) # 0.48043105006217957\n",
    "print(predict('lava is the molten rock expelled by a volcano during an eruption',\n",
    "    \"الحمم البركانية هي الصخور المنصهرة التي طردها بركان أثناء ثوران بركان\")) # 0.8797786831855774\n",
    "print(predict('increases','decreases')) # 0.3992762267589569\n",
    "print(predict('تقل','decrease')) # 0.5011984705924988\n",
    "print(predict('تنقص','decrease')) # 0.6617798805236816\n",
    "print(predict('تخفيض','decrease')) # 0.8037792444229126\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.8798]])"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos_sim(model.encode([\"lava is the molten rock expelled by a volcano during an eruption\"]),\n",
    "model.encode([\"الحمم البركانية هي الصخور المنصهرة التي طردها بركان أثناء ثوران بركان\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.7766]])"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos_sim(BERT.model.encode([\"increases\"]),BERT.model.encode([\"decreases\"]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0f876b07db73824ba94c3da26a300833b9286c0dd0d4e31723ae4574ddd9b9bb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
