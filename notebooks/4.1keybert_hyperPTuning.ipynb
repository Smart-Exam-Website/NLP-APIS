{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import key_words\n",
    "from sentence_transformers.util import cos_sim\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "def save_obj(obj:object,name:str):\n",
    "    ext = '.pickle'\n",
    "    with open(name + ext, 'wb') as handle:\n",
    "        pickle.dump(obj, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def load_obj(name:str)->object:\n",
    "    ext = '.pickle'\n",
    "    with open(name + ext, 'rb') as handle:\n",
    "        return pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "ess_1_model_answers = load_obj(\"data/essaySet_1_model_answers\")\n",
    "ess_2_model_answers = load_obj(\"data/essaySet_2_model_answers\")\n",
    "ess_3_model_answers = load_obj(\"data/essaySet_3_model_answers\")\n",
    "ess_4_model_answers = load_obj(\"data/essaySet_4_model_answers\")\n",
    "ess_5_model_answers = load_obj(\"data/essaySet_5_model_answers\")\n",
    "ess_6_model_answers = load_obj(\"data/essaySet_6_model_answers\")\n",
    "ess_7_model_answers = load_obj(\"data/essaySet_7_model_answers\")\n",
    "ess_8_model_answers = load_obj(\"data/essaySet_8_model_answers\")\n",
    "ess_9_model_answers = load_obj(\"data/essaySet_9_model_answers\")\n",
    "ess_10_model_answers = load_obj(\"data/essaySet_10_model_answers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_dict = {'ess_1_model_answers_emb': ess_1_model_answers_emb,\n",
    " 'ess_2_model_answers_emb': ess_2_model_answers_emb,\n",
    " 'ess_3_model_answers_emb': ess_3_model_answers_emb,\n",
    " 'ess_4_model_answers_emb': ess_4_model_answers_emb,\n",
    " 'ess_5_model_answers_emb': ess_5_model_answers_emb,\n",
    " 'ess_6_model_answers_emb': ess_6_model_answers_emb,\n",
    " 'ess_7_model_answers_emb': ess_7_model_answers_emb,\n",
    " 'ess_8_model_answers_emb': ess_8_model_answers_emb,\n",
    " 'ess_9_model_answers_emb': ess_9_model_answers_emb,\n",
    " 'ess_10_model_answers_emb': ess_10_model_answers_emb}\n",
    "save_obj(emb_dict, \"data/model_answer_emb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_dict = load_obj(\"data/model_answer_emb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformer_model\n",
    "\n",
    "BERT = transformer_model.BERTModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maximal_marginal_relevance(doc_embedding: np.ndarray,\n",
    "        word_embeddings: np.ndarray,\n",
    "        words,\n",
    "        top_n = 5,\n",
    "        diversity = 0.8):\n",
    "    \"\"\"\n",
    "    Maximal Marginal Relevance algorithm for keyword extraction\n",
    "    * from KeyBERT repository on github\n",
    "\n",
    "    Args:\n",
    "        doc_embedding (numpy.ndarray): embedding of shape (1, 768)\n",
    "        word_embeddings (numpy.ndarray): embedding of shape (N, 768)\n",
    "        words (List[str]): list of words\n",
    "        top_n (Optional[int]): number of top words to extract\n",
    "        diversity (Optional[float]): diversity of top words to extract\n",
    "\n",
    "    Returns:\n",
    "        List[Tuple[str, float]]: list of top_n words with their scores\n",
    "    \"\"\"\n",
    "    # make sure 2d array\n",
    "    if doc_embedding.ndim == 1:\n",
    "        doc_embedding = doc_embedding.reshape(1, -1)\n",
    "\n",
    "    # Extract similarity within words, and between words and the document\n",
    "    # word_doc_similarity = cosine_similarity(word_embeddings, doc_embedding)\n",
    "    # word_similarity = cosine_similarity(word_embeddings)\n",
    "\n",
    "    word_doc_similarity = np.array(cos_sim(word_embeddings, doc_embedding)).clip(-1, 1).round(6)\n",
    "    word_similarity = np.array(cos_sim(word_embeddings, word_embeddings)).clip(-1, 1).round(6)\n",
    "\n",
    "    # Initialize candidates and already choose best keyword/keyphras\n",
    "    keywords_idx = [np.argmax(word_doc_similarity)]\n",
    "    candidates_idx = [i for i in range(len(words)) if i != keywords_idx[0]]\n",
    "\n",
    "    for _ in range(top_n - 1):\n",
    "        # Extract similarities within candidates and\n",
    "        # between candidates and selected keywords/phrases\n",
    "        candidate_similarities = word_doc_similarity[candidates_idx, :]\n",
    "        target_similarities = np.max(word_similarity[candidates_idx][:, keywords_idx], axis=1)\n",
    "\n",
    "        # Calculate maximal_marginal_relevance\n",
    "        mmr = (1-diversity) * candidate_similarities -\\\n",
    "            diversity * target_similarities.reshape(-1, 1)\n",
    "        # if return mmr is empty\n",
    "        if mmr.size == 0:\n",
    "            continue\n",
    "        mmr = candidates_idx[np.argmax(mmr)]\n",
    "\n",
    "        # Update keywords & candidates\n",
    "        keywords_idx.append(mmr)\n",
    "        candidates_idx.remove(mmr)\n",
    "    return [words[idx] for idx in keywords_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_keys_div(ind,essay,top_n=5,diversities=[0.8]):\n",
    "    for div in diversities:\n",
    "        keywords = list(map(lambda x: maximal_marginal_relevance(x[0].reshape(1, -1),x[1],x[2],top_n=top_n,diversity=div),\n",
    "            zip(emb_dict[f'ess_{essay}_model_answers_emb'],\n",
    "            model_candidate_emb,model_candidates)))\n",
    "        print(div,keywords[ind])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ESSAY SET 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('In order to replicate this experiment, you would need to know what the samples were. Also, step three in their procedure is a little confusing and to replicate this experiment they may want to give a little bit more directions and be more specific. They might also want to say what size containers to use or how much vinegar to add to the container to allow someone also to replicate.',\n",
       " 'len',\n",
       " 70)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ess_1_model_answers[-1], \"len\", ess_1_model_answers[-1].split().__len__()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> kws extracted manually by me\n",
    "\n",
    "know what the samples\n",
    "\n",
    "little bit more directions and be more specific\n",
    "\n",
    "size containers\n",
    "\n",
    "how much vinegar to add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 ['replicate experiment need', 'replicate experiment want', 'order replicate experiment', 'confusing replicate experiment', 'experiment need know', 'size containers use', 'replicate experiment', 'say size containers', 'experiment want little']\n",
      "0.1 ['replicate experiment need', 'order replicate experiment', 'size containers use', 'replicate experiment want', 'know samples step', 'confusing replicate experiment', 'experiment need know', 'container allow replicate', 'little confusing replicate']\n",
      "0.2 ['replicate experiment need', 'size containers use', 'know samples step', 'little confusing replicate', 'order replicate experiment', 'replicate experiment want', 'container allow replicate', 'experiment need know', 'confusing replicate experiment']\n",
      "0.3 ['replicate experiment need', 'size containers use', 'know samples step', 'little confusing replicate', 'container allow replicate', 'use vinegar add', 'experiment want little', 'order replicate experiment', 'experiment need know']\n",
      "0.4 ['replicate experiment need', 'size containers use', 'know samples step', 'procedure little confusing', 'use vinegar add', 'container allow replicate', 'experiment want little', 'bit directions specific', 'experiment need know']\n",
      "0.5 ['replicate experiment need', 'size containers use', 'samples step procedure', 'little confusing', 'vinegar add', 'directions specific want', 'need know samples', 'container allow replicate', 'experiment want little']\n",
      "0.6 ['replicate experiment need', 'size containers use', 'little bit directions', 'samples step procedure', 'vinegar add', 'want say', 'need know samples', 'little confusing replicate', 'container allow replicate']\n",
      "0.7 ['replicate experiment need', 'little bit', 'containers use vinegar', 'samples step procedure', 'directions specific want', 'need know samples', 'container allow replicate', 'say size containers', 'procedure little confusing']\n",
      "0.8 ['replicate experiment need', 'little bit', 'containers use vinegar', 'samples step procedure', 'directions specific want', 'need know', 'want say size', 'size containers', 'procedure little confusing']\n",
      "0.9 ['replicate experiment need', 'bit directions specific', 'vinegar add', 'size containers use', 'samples step procedure', 'want say', 'little confusing', 'need know samples', 'confusing replicate']\n"
     ]
    }
   ],
   "source": [
    "# (2,3)\n",
    "compare_keys_div(-1,essay=1,top_n=9,diversities=list(np.arange(0,1,0.1).round(1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> (2,3) , n=7, div = 0.5, 0.6 ,0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"In order to replicate this group's procedure, I would need the following information: ^p 1. When they say in step one that they need to determine the mass of four different samples, they should name and list the samples that they plan on using. ^p 2. In step two when they say to pour vinegar in each of the four separate but identical containers, they should make mention or how much vinegar we should pour in. ^p 3. When they say in step  four that we should rinse each sample with distilled water, how much water should we use and how long should we rinse.\",\n",
       " 'len',\n",
       " 105)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ess_1_model_answers[-2], \"len\", ess_1_model_answers[-2].split().__len__()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> kws extracted manually by me\n",
    "\n",
    "need to determine the mass of four different samples\n",
    "\n",
    "name and list the samples that they plan on using\n",
    "\n",
    "say to pour vinegar\n",
    "\n",
    "how much vinegar we should pour\n",
    "\n",
    "rinse each sample with distilled water, how much water should we use and how long should we rinse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 ['identical containers make', 'identical containers', 'mass different samples', 'use long', 'samples plan using', 'different samples', 'replicate group procedure', 'separate identical containers', 'different samples list']\n",
      "0.1 ['identical containers make', 'use long', 'mass different samples', 'replicate group procedure', 'samples plan using', 'identical containers', 'different samples', 'separate identical containers', 'procedure need']\n",
      "0.2 ['identical containers make', 'use long', 'mass different samples', 'replicate group procedure', 'samples plan using', 'procedure need', 'identical containers', 'using step', 'order replicate']\n",
      "0.3 ['identical containers make', 'use long', 'mass different samples', 'replicate group procedure', 'samples plan using', 'procedure need', 'using step', 'order replicate', 'vinegar separate identical']\n",
      "0.4 ['identical containers make', 'use long', 'mass different samples', 'replicate group procedure', 'samples plan using', 'procedure need', 'using step', 'determine mass', 'make mention vinegar']\n",
      "0.5 ['identical containers make', 'use long', 'mass different samples', 'replicate group procedure', 'samples plan using', 'procedure need', 'using step', 'vinegar pour say', 'make mention']\n",
      "0.6 ['identical containers make', 'use long', 'samples list samples', 'replicate group procedure', 'determine mass different', 'plan using step', 'vinegar pour say', 'procedure need', 'following information say']\n",
      "0.7 ['identical containers make', 'water use long', 'procedure need following', 'samples list samples', 'determine mass', 'order replicate group', 'vinegar pour say', 'plan using', 'make mention']\n",
      "0.8 ['identical containers make', 'water use long', 'following information', 'samples list samples', 'group procedure need', 'determine mass different', 'pour say', 'plan using step', 'replicate group']\n",
      "0.9 ['identical containers make', 'water water use', 'procedure need following', 'samples list samples', 'determine mass', 'step say pour', 'replicate group', 'use long', 'following information say']\n"
     ]
    }
   ],
   "source": [
    "compare_keys_div(-2,essay=2,top_n=9,diversities=list(np.arange(0,1,0.1).round(1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 ['identical containers', 'use long', 'different samples', 'samples', 'samples plan']\n",
      "0.1 ['identical containers', 'use long', 'different samples', 'samples plan', 'samples']\n",
      "0.2 ['identical containers', 'use long', 'different samples', 'samples plan', 'procedure']\n",
      "0.3 ['identical containers', 'use long', 'different samples', 'samples plan', 'order replicate']\n",
      "0.4 ['identical containers', 'use long', 'samples plan', 'different samples', 'procedure need']\n",
      "0.5 ['identical containers', 'use long', 'samples plan', 'different samples', 'procedure need']\n",
      "0.6 ['identical containers', 'use long', 'samples plan', 'different samples', 'determine mass']\n",
      "0.7 ['identical containers', 'use long', 'samples plan', 'determine mass', 'sample distilled']\n",
      "0.8 ['identical containers', 'pour say', 'use long', 'samples plan', 'following information']\n",
      "0.9 ['identical containers', 'pour say', 'need following', 'samples plan', 'long rinse']\n"
     ]
    }
   ],
   "source": [
    "# (1,2)\n",
    "compare_keys_div(-2,essay=2,top_n=5,diversities=list(np.arange(0,1,0.1).round(1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> (2,3) , n=7, div = 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"After reading the group's procedure, additional information that a would need in order to replicate the experiment are that a need to know the size of the containers that the samples are put in, for they need to be identical. U also need to know tha amount of samples u need to put in each container in order to obtain tha same starting mass the group of students recieved from the samples. Another factor a should know is the amount no vinegar that u pour into the containers concerning the oud example of the experiment. Another good peice of information would be the location where the samples are drying and amount of sunlight.\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ess_1_model_answers[-3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> kws extracted manually by me\n",
    "\n",
    " know the size\n",
    "\n",
    "samples are put in\n",
    "\n",
    "know tha amount of samples\n",
    "\n",
    "to put in each container\n",
    "\n",
    "obtain tha same starting mass\n",
    "\n",
    "Another factor amount no vinegar\n",
    "\n",
    "location where the samples are drying and amount of sunlight.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 ['samples need identical', 'samples factor', 'recieved samples factor', 'size containers samples', 'containers samples need']\n",
      "0.1 ['samples need identical', 'samples factor', 'recieved samples factor', 'group procedure additional', 'size containers samples']\n",
      "0.2 ['samples need identical', 'recieved samples factor', 'group procedure additional', 'replicate experiment', 'size containers samples']\n",
      "0.3 ['samples need identical', 'group procedure additional', 'recieved samples factor', 'size containers', 'replicate experiment']\n",
      "0.4 ['samples need identical', 'additional information', 'size containers', 'recieved samples factor', 'replicate experiment']\n",
      "0.5 ['samples need identical', 'tha starting mass', 'additional information', 'size containers', 'oud example experiment']\n",
      "0.6 ['samples need identical', 'tha starting mass', 'additional information', 'pour containers concerning', 'oud example experiment']\n",
      "0.7 ['samples need identical', 'tha starting mass', 'additional information', 'experiment good', 'container order obtain']\n",
      "0.8 ['samples need identical', 'tha starting mass', 'additional information', 'experiment good', 'order obtain tha']\n",
      "0.9 ['samples need identical', 'tha starting mass', 'concerning oud example', 'group students recieved', 'know vinegar pour']\n"
     ]
    }
   ],
   "source": [
    "compare_keys_div(-3,essay=2,top_n=5,diversities=list(np.arange(0,1,0.1).round(1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 ['samples need identical', 'samples factor', 'recieved samples factor', 'size containers samples', 'containers samples need', 'samples need', 'recieved samples', 'tha samples need', 'group procedure additional']\n",
      "0.1 ['samples need identical', 'samples factor', 'recieved samples factor', 'group procedure additional', 'size containers samples', 'replicate experiment', 'tha samples need', 'containers samples need', 'samples need']\n",
      "0.2 ['samples need identical', 'recieved samples factor', 'group procedure additional', 'replicate experiment', 'size containers samples', 'samples factor', 'tha samples need', 'additional information need', 'samples drying']\n",
      "0.3 ['samples need identical', 'group procedure additional', 'recieved samples factor', 'size containers', 'replicate experiment', 'containers samples need', 'peice information', 'tha starting mass', 'samples factor']\n",
      "0.4 ['samples need identical', 'additional information', 'size containers', 'recieved samples factor', 'replicate experiment', 'tha starting mass', 'reading group procedure', 'concerning oud example', 'samples drying']\n",
      "0.5 ['samples need identical', 'tha starting mass', 'additional information', 'size containers', 'oud example experiment', 'recieved samples factor', 'reading group procedure', 'order obtain tha', 'samples drying sunlight']\n",
      "0.6 ['samples need identical', 'tha starting mass', 'additional information', 'pour containers concerning', 'oud example experiment', 'group students recieved', 'order obtain tha', 'reading group procedure', 'good peice']\n",
      "0.7 ['samples need identical', 'tha starting mass', 'additional information', 'experiment good', 'container order obtain', 'group students recieved', 'reading group procedure', 'factor know', 'samples drying sunlight']\n",
      "0.8 ['samples need identical', 'tha starting mass', 'additional information', 'experiment good', 'order obtain tha', 'group students recieved', 'pour containers concerning', 'reading group procedure', 'factor know']\n",
      "0.9 ['samples need identical', 'tha starting mass', 'concerning oud example', 'group students recieved', 'know vinegar pour', 'peice information location', 'size containers', 'procedure additional', 'drying sunlight']\n"
     ]
    }
   ],
   "source": [
    "# (2,3)\n",
    "compare_keys_div(-3,essay=2,top_n=9,diversities=list(np.arange(0,1,0.1).round(1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 ['samples need identical', 'samples factor', 'recieved samples factor', 'size containers samples', 'containers samples need']\n",
      "0.1 ['samples need identical', 'samples factor', 'recieved samples factor', 'group procedure additional', 'size containers samples']\n",
      "0.2 ['samples need identical', 'recieved samples factor', 'group procedure additional', 'replicate experiment', 'size containers samples']\n",
      "0.3 ['samples need identical', 'group procedure additional', 'recieved samples factor', 'size containers', 'replicate experiment']\n",
      "0.4 ['samples need identical', 'additional information', 'size containers', 'recieved samples factor', 'replicate experiment']\n",
      "0.5 ['samples need identical', 'tha starting mass', 'additional information', 'size containers', 'oud example experiment']\n",
      "0.6 ['samples need identical', 'tha starting mass', 'additional information', 'pour containers concerning', 'oud example experiment']\n",
      "0.7 ['samples need identical', 'tha starting mass', 'additional information', 'experiment good', 'container order obtain']\n",
      "0.8 ['samples need identical', 'tha starting mass', 'additional information', 'experiment good', 'order obtain tha']\n",
      "0.9 ['samples need identical', 'tha starting mass', 'concerning oud example', 'group students recieved', 'know vinegar pour']\n"
     ]
    }
   ],
   "source": [
    "# (2,3)\n",
    "compare_keys_div(-3,essay=2,top_n=5,diversities=list(np.arange(0,1,0.1).round(1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 ['samples need identical', 'recieved samples factor', 'size containers samples', 'containers samples need', 'tha samples need', 'group procedure additional', 'replicate experiment need', 'samples need container', 'samples factor know', 'additional information need']\n",
      "0.1 ['samples need identical', 'recieved samples factor', 'group procedure additional', 'size containers samples', 'tha samples need', 'replicate experiment need', 'containers samples need', 'additional information need', 'samples factor know', 'location samples drying']\n",
      "0.2 ['samples need identical', 'recieved samples factor', 'group procedure additional', 'size containers samples', 'replicate experiment need', 'tha samples need', 'additional information need', 'containers samples need', 'information location samples', 'samples factor know']\n",
      "0.3 ['samples need identical', 'group procedure additional', 'recieved samples factor', 'know size containers', 'oud example experiment', 'containers samples need', 'tha starting mass', 'additional information need', 'replicate experiment need', 'information location samples']\n",
      "0.4 ['samples need identical', 'tha starting mass', 'group procedure additional', 'know size containers', 'recieved samples factor', 'oud example experiment', 'peice information location', 'additional information need', 'containers samples need', 'samples drying sunlight']\n",
      "0.5 ['samples need identical', 'tha starting mass', 'procedure additional information', 'know size containers', 'group students recieved', 'oud example experiment', 'recieved samples factor', 'reading group procedure', 'samples drying sunlight', 'order obtain tha']\n",
      "0.6 ['samples need identical', 'tha starting mass', 'concerning oud example', 'group students recieved', 'procedure additional information', 'know size containers', 'reading group procedure', 'information location samples', 'experiment good peice', 'samples drying sunlight']\n",
      "0.7 ['samples need identical', 'tha starting mass', 'concerning oud example', 'group students recieved', 'procedure additional information', 'know vinegar pour', 'reading group procedure', 'container order obtain', 'samples drying sunlight', 'experiment good peice']\n",
      "0.8 ['samples need identical', 'tha starting mass', 'concerning oud example', 'group students recieved', 'know vinegar pour', 'peice information location', 'group procedure additional', 'container order obtain', 'samples drying sunlight', 'experiment good peice']\n",
      "0.9 ['samples need identical', 'tha starting mass', 'concerning oud example', 'group students recieved', 'know vinegar pour', 'peice information location', 'group procedure additional', 'container order obtain', 'samples drying sunlight', 'experiment good peice']\n"
     ]
    }
   ],
   "source": [
    "model_candidates = list(map(lambda ans: key_words.candidates_tokens(ans,n_gram_range=(3,3)),ess_1_model_answers))\n",
    "model_candidate_emb = list(map(lambda cand:BERT.model.encode(cand),model_candidates))\n",
    "compare_keys_div(-3,essay=2,top_n=10,diversities=list(np.arange(0,1,0.1).round(1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> (3,3) , n=5 , div =0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 968/968 [00:00<00:00, 484kB/s]\n",
      "Downloading: 100%|██████████| 190/190 [00:00<00:00, 94.9kB/s]\n",
      "Downloading: 100%|██████████| 3.79k/3.79k [00:00<00:00, 3.79MB/s]\n",
      "Downloading: 100%|██████████| 645/645 [00:00<00:00, 646kB/s]\n",
      "Downloading: 100%|██████████| 122/122 [00:00<00:00, 61.0kB/s]\n",
      "Downloading: 100%|██████████| 229/229 [00:00<00:00, 229kB/s]\n",
      "Downloading: 100%|██████████| 471M/471M [06:09<00:00, 1.28MB/s] \n",
      "Downloading: 100%|██████████| 53.0/53.0 [00:00<00:00, 53.0kB/s]\n",
      "Downloading: 100%|██████████| 5.07M/5.07M [00:02<00:00, 2.03MB/s]\n",
      "Downloading: 100%|██████████| 239/239 [00:00<00:00, 239kB/s]\n",
      "Downloading: 100%|██████████| 9.08M/9.08M [00:04<00:00, 1.94MB/s]\n",
      "Downloading: 100%|██████████| 480/480 [00:00<00:00, 240kB/s]\n",
      "Downloading: 100%|██████████| 14.8M/14.8M [00:07<00:00, 1.91MB/s]\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer(\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_candidates = list(map(lambda ans: key_words.candidates_tokens(ans,n_gram_range=(2,3)),ess_1_model_answers))\n",
    "model_candidate_emb = list(map(lambda cand:model.encode(cand),model_candidates))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 ['replicate experiment need', 'replicate experiment want', 'confusing replicate experiment', 'replicate experiment', 'experiment need know', 'experiment need', 'samples step procedure', 'order replicate experiment', 'experiment want']\n",
      "0.1 ['replicate experiment need', 'experiment need know', 'confusing replicate experiment', 'replicate experiment want', 'samples step procedure', 'replicate experiment', 'experiment need', 'order replicate experiment', 'containers use vinegar']\n",
      "0.2 ['replicate experiment need', 'samples step procedure', 'experiment need know', 'containers use vinegar', 'confusing replicate experiment', 'replicate experiment want', 'replicate experiment', 'experiment need', 'order replicate experiment']\n",
      "0.3 ['replicate experiment need', 'containers use vinegar', 'samples step procedure', 'experiment need know', 'confusing replicate experiment', 'order replicate experiment', 'container allow replicate', 'replicate experiment want', 'replicate experiment']\n",
      "0.4 ['replicate experiment need', 'containers use vinegar', 'samples step procedure', 'experiment need know', 'container allow replicate', 'procedure little confusing', 'order replicate experiment', 'confusing replicate experiment', 'experiment want little']\n",
      "0.5 ['replicate experiment need', 'containers use vinegar', 'procedure little confusing', 'samples step procedure', 'want say size', 'experiment need know', 'container allow replicate', 'directions specific', 'little confusing replicate']\n",
      "0.6 ['replicate experiment need', 'containers use vinegar', 'bit directions specific', 'step procedure little', 'know samples step', 'say size', 'little confusing', 'experiment want little', 'container allow replicate']\n",
      "0.7 ['replicate experiment need', 'bit directions specific', 'containers use vinegar', 'procedure little', 'say size', 'know samples step', 'specific want', 'add container allow', 'experiment want little']\n",
      "0.8 ['replicate experiment need', 'want say size', 'containers use vinegar', 'procedure little', 'bit directions specific', 'need know', 'know samples step', 'add container allow', 'want little bit']\n",
      "0.9 ['replicate experiment need', 'want say size', 'containers use vinegar', 'procedure little', 'bit directions specific', 'need know', 'add container allow', 'know samples', 'want little bit']\n"
     ]
    }
   ],
   "source": [
    "ind = -1\n",
    "for div in list(np.arange(0,1,0.1).round(1)):\n",
    "    keywords = list(map(lambda x: maximal_marginal_relevance(x[0].reshape(1, -1),x[1],x[2],top_n=9,diversity=div),\n",
    "        zip(model.encode(ess_1_model_answers),\n",
    "        model_candidate_emb,model_candidates)))\n",
    "    print(div,keywords[ind])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> long (3,3) top_n = 7, div = 0.7\n",
    "\n",
    "> mid (2,2) top_n = 5, div = 0.65\n",
    "\n",
    "> short (1,1) top_n = 5, div = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "ess_dict = {\n",
    "    \"ess_1_model_answers\": ess_1_model_answers,\n",
    "    \"ess_2_model_answers\": ess_2_model_answers,\n",
    "    \"ess_3_model_answers\": ess_3_model_answers,\n",
    "    \"ess_4_model_answers\": ess_4_model_answers,\n",
    "    \"ess_5_model_answers\": ess_5_model_answers,\n",
    "    \"ess_6_model_answers\": ess_6_model_answers,\n",
    "    \"ess_7_model_answers\": ess_7_model_answers,\n",
    "    \"ess_8_model_answers\": ess_8_model_answers,\n",
    "    \"ess_9_model_answers\": ess_9_model_answers,\n",
    "    \"ess_10_model_answers\": ess_10_model_answers,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_candidates = list(map(lambda ans: key_words.candidates_tokens(ans,n_gram_range=(2,3)),ess_dict[\"ess_9_model_answers\"]))\n",
    "model_candidate_emb = list(map(lambda cand:BERT.model.encode(cand),model_candidates))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The author has a very effective way of organizing the the article. He first introduces the beginning with some startling statements to get your attention, statements such as, 'Grab your telescope! Look up in the sky! It's a comet! It's a meteor!' (1).       Once grabbing the readers attention, he breaks the article into several segments labeled, 'What Is Space Junk?', 'Crash Course,' and 'Little Bits, But a Big Deal.'  All of the following are to condense the article into areas that focus on those specific ideas.     By doing so, it seems that the article was very effective in keeping the reader reading, instead of organizing the article into just one big clump.  Having the organization that it did made the article easy to read and very interesting at the same time, without leaving out any details.\""
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ess_9_model_answers[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " has a very effective way of organizing the the article\n",
    "\n",
    " Once grabbing the readers attention\n",
    "\n",
    "Space Junk?\n",
    "\n",
    "Crash Course"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 ['organizing article introduces', 'organizing article just', 'breaks article segments', 'condense article areas', 'article introduces beginning', 'article segments labeled', 'way organizing article', 'article just big', 'instead organizing article', 'attention breaks article']\n",
      "0.1 ['organizing article introduces', 'breaks article segments', 'organizing article just', 'condense article areas', 'article introduces beginning', 'article segments labeled', 'article just big', 'way organizing article', 'article areas focus', 'following condense article']\n",
      "0.2 ['organizing article introduces', 'breaks article segments', 'article just big', 'condense article areas', 'organizing article just', 'article segments labeled', 'meteor grabbing readers', 'article introduces beginning', 'space junk crash', 'statements attention statements']\n",
      "0.3 ['organizing article introduces', 'space junk crash', 'article just big', 'meteor grabbing readers', 'breaks article segments', 'article areas focus', 'condense article areas', 'organizing article just', 'statements grab telescope', 'article segments labeled']\n",
      "0.4 ['organizing article introduces', 'space junk crash', 'meteor grabbing readers', 'focus specific ideas', 'article just big', 'breaks article segments', 'startling statements attention', 'grab telescope look', 'condense article areas', 'author effective way']\n",
      "0.5 ['organizing article introduces', 'look sky comet', 'easy read interesting', 'junk crash course', 'statements attention statements', 'meteor grabbing readers', 'specific ideas doing', 'article segments labeled', 'big deal following', 'author effective way']\n",
      "0.6 ['organizing article introduces', 'look sky comet', 'junk crash course', 'easy read interesting', 'statements attention statements', 'specific ideas doing', 'meteor grabbing readers', 'big deal following', 'author effective way', 'segments labeled space']\n",
      "0.7 ['organizing article introduces', 'look sky comet', 'junk crash course', 'easy read interesting', 'time leaving details', 'attention statements grab', 'focus specific ideas', 'segments labeled space', 'author effective way', 'big clump having']\n",
      "0.8 ['organizing article introduces', 'look sky comet', 'junk crash course', 'time leaving details', 'attention statements grab', 'reader reading instead', 'focus specific ideas', 'big clump having', 'segments labeled space', 'having organization did']\n",
      "0.9 ['organizing article introduces', 'look sky comet', 'junk crash course', 'time leaving details', 'reader reading instead', 'attention statements grab', 'specific ideas doing', 'big clump having', 'segments labeled space', 'having organization did']\n"
     ]
    }
   ],
   "source": [
    "compare_keys_div(-1,essay=9,top_n=10,diversities=list(np.arange(0,1,0.1).round(1)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 ['organizing article introduces', 'article introduces', 'organizing article', 'organizing article just', 'breaks article segments', 'condense article areas', 'article introduces beginning', 'article segments labeled', 'condense article', 'way organizing article']\n",
      "0.1 ['organizing article introduces', 'breaks article segments', 'organizing article just', 'condense article areas', 'article introduces', 'organizing article', 'article segments labeled', 'article introduces beginning', 'article just big', 'condense article']\n",
      "0.2 ['organizing article introduces', 'breaks article segments', 'article just big', 'condense article areas', 'organizing article just', 'article introduces', 'article segments labeled', 'meteor grabbing readers', 'organizing article', 'space junk crash']\n",
      "0.3 ['organizing article introduces', 'space junk crash', 'article just big', 'meteor grabbing readers', 'breaks article segments', 'article areas focus', 'condense article', 'organizing article just', 'statements grab telescope', 'article segments labeled']\n",
      "0.4 ['organizing article introduces', 'space junk crash', 'meteor grabbing readers', 'focus specific ideas', 'article just big', 'breaks article segments', 'startling statements attention', 'condense article', 'grab telescope', 'segments labeled space']\n",
      "0.5 ['organizing article introduces', 'look sky comet', 'easy read interesting', 'junk crash course', 'statements attention statements', 'meteor grabbing readers', 'specific ideas doing', 'article segments labeled', 'big deal following', 'author effective way']\n",
      "0.6 ['organizing article introduces', 'look sky comet', 'junk crash course', 'easy read interesting', 'statements attention statements', 'specific ideas doing', 'meteor grabbing readers', 'big deal following', 'author effective way', 'labeled space']\n",
      "0.7 ['organizing article introduces', 'look sky comet', 'junk crash course', 'statements grab', 'specific ideas', 'reader reading instead', 'bits big deal', 'attention breaks', 'segments labeled space', 'following condense']\n",
      "0.8 ['organizing article introduces', 'look sky comet', 'junk crash course', 'effective way', 'statements grab', 'time leaving details', 'reader reading instead', 'focus specific ideas', 'big clump having', 'segments labeled space']\n",
      "0.9 ['organizing article introduces', 'look sky comet', 'crash course', 'time leaving details', 'reader reading instead', 'statements grab', 'big clump having', 'ideas doing', 'areas focus specific', 'segments labeled space']\n"
     ]
    }
   ],
   "source": [
    "# 2,3\n",
    "compare_keys_div(-1,essay=9,top_n=10,diversities=list(np.arange(0,1,0.1).round(1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## one limiation \n",
    "\n",
    "https://link.springer.com/chapter/10.1007/978-3-030-79150-6_50#Sec10\n",
    "\n",
    "is that keybert perform bad on small perform bad on docs of length of less than 2* top_n unique terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['You',\n",
       " 'need',\n",
       " 'to',\n",
       " 'know',\n",
       " 'how',\n",
       " 'much',\n",
       " 'vinegar',\n",
       " 'was',\n",
       " 'used',\n",
       " 'in',\n",
       " 'each',\n",
       " 'container.']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ess_1_model_answers[0].split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['arabic',\n",
       " 'english',\n",
       " 'french',\n",
       " 'german',\n",
       " 'italian',\n",
       " 'japanese',\n",
       " 'korean',\n",
       " 'portuguese',\n",
       " 'russian',\n",
       " 'spanish']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "['arabic', 'english', 'french', 'german', 'italian', 'japanese', 'korean', 'portuguese', 'russian', 'spanish']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "len(set(stop_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "doc = 'You need to know how much vinegar was used in each container.'\n",
    "# remove stopwords from doc\n",
    "tokens = [w for w in set(doc.split()) if w not in stop_words]\n",
    "# remove punctuation from tokens\n",
    "tokens = [w.translate(str.maketrans('', '', string.punctuation)) for w in [doc]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['You need to know how much vinegar was used in each container'],\n",
       " 'You need to know how much vinegar was used in each container.')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokens,doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ngram_top_n_diversity(docs,threshold=0.7):\n",
    "    lens = []\n",
    "    tokens = []\n",
    "    for doc in docs:\n",
    "        tokens_ = [w.translate(str.maketrans('', '', string.punctuation)) for w in [doc]]\n",
    "        tokens_ = [w for w in set(doc.split()) if w not in stop_words]\n",
    "        tokens.append(tokens_)\n",
    "        lens.append(len(tokens))\n",
    "\n",
    "    lens = np.median(np.array(lens))\n",
    "    if lens < 10:\n",
    "        print(\"Too few tokens in the documents. Please check the input.\")\n",
    "        model_candidates = tokens\n",
    "        model_candidate_emb = list(map(lambda cand:BERT.model.encode(cand),model_candidates))\n",
    "        cands = list(map(lambda cand: np.array(cos_sim(model_answer_emb,cand)),model_candidate_emb))\n",
    "        list(map(lambda cand: np.where(cand,cand>threshold),cands))\n",
    "        lens = 100\n",
    "    if lens <20 and lens>10 :\n",
    "        print(\"mid\")\n",
    "        # MMR\n",
    "        lens = 100\n",
    "    else:\n",
    "        print(\"long\")\n",
    "        # MMR\n",
    "        lens = 200\n",
    "    model_candidates = tokens\n",
    "    model_candidate_emb = list(map(lambda cand:BERT.model.encode(cand),model_candidates))\n",
    "    list(map(lambda st: np.array(cos_sim(model_answer_emb,st)),model_candidate_emb))\n",
    "    print(model_candidate_emb[0].shape)\n",
    "    # cos_sim(model_answer_emb, model_candidate_emb)\n",
    "    del tokens\n",
    "\n",
    "    return lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans_emb = BERT.model.encode(\"i am so dump as new\")\n",
    "cand = set(\"i am so dump as new\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'am', 'as', 'dump', 'i', 'new', 'so'}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 6], dtype=int64)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cands = np.array(list(map(lambda x: np.array(cos_sim(ans_emb,BERT.model.encode(x))),cand)))\n",
    "np.where(cands>0.4)[0]+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = [w.translate(str.maketrans('', '', string.punctuation)) for w in [ess_1_model_answers[-1]]]\n",
    "# t = [*set([w for w in set(doc.split()) if w not in stop_words])]\n",
    "t = [*set([w for w in set(t[0].split()) if w not in stop_words])]\n",
    "# cands = np.array(list(map(lambda x: np.array(cos_sim(ans_emb,BERT.model.encode(x))),cand)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In order to replicate this experiment, you would need to know what the samples were. Also, step three in their procedure is a little confusing and to replicate this experiment they may want to give a little bit more directions and be more specific. They might also want to say what size containers to use or how much vinegar to add to the container to allow someone also to replicate.'"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ess_1_model_answers[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['used', 'You', 'much', 'need', 'container.', 'vinegar', 'know']"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['much', 'confusing', 'replicate', 'container', 'experiment']"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maximal_marginal_relevance(\n",
    "        ans_emb.reshape(1, -1),BERT.model.encode(t),t,top_n=5,\n",
    "        diversity=0.4)\n",
    "# maximal_marginal_relevance(\n",
    "#         ans_emb.reshape(1, -1),BERT.model.encode([*t]),[*t],top_n=5,\n",
    "#         diversity=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cands[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0], dtype=int64), array([0], dtype=int64))"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(cands[0]>0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "short -> set stopwords (1,1) max sim top sorted 5"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0f876b07db73824ba94c3da26a300833b9286c0dd0d4e31723ae4574ddd9b9bb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
