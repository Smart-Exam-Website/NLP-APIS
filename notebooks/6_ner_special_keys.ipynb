{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, List, Dict, Tuple, Set\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import key_words\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "def save_obj(obj:object,name:str):\n",
    "    ext = '.pickle'\n",
    "    with open(name + ext, 'wb') as handle:\n",
    "        pickle.dump(obj, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def load_obj(name:str)->object:\n",
    "    ext = '.pickle'\n",
    "    with open(name + ext, 'rb') as handle:\n",
    "        return pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXCEPTION_ENTITES = set([\"DATE\",\"TIME\",\"PERCENT\",\"MONEY\",\"QUANTITY\",\"ORDINAL\", \"CARDINAL\",\"WORK_OF_ART\"])\n",
    "NER = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = \"data/train_phase1.tsv\"\n",
    "df = pd.read_csv(train_path, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ner(\n",
    "    paragraph: str)\\\n",
    "        -> Dict[str, str]:\n",
    "    doc = NER(paragraph)\n",
    "    res = {\n",
    "        entity.text : entity.label_\n",
    "        for entity in doc.ents\n",
    "    }\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ess_dict = {\n",
    "    \"ess_1_model_answers\":  load_obj(\"data/essaySet_1_model_answers\"),\n",
    "    \"ess_2_model_answers\": load_obj(\"data/essaySet_2_model_answers\"),\n",
    "    \"ess_3_model_answers\": load_obj(\"data/essaySet_3_model_answers\"),\n",
    "    \"ess_4_model_answers\": load_obj(\"data/essaySet_4_model_answers\"),\n",
    "    \"ess_5_model_answers\": load_obj(\"data/essaySet_5_model_answers\"),\n",
    "    \"ess_6_model_answers\": load_obj(\"data/essaySet_6_model_answers\"),\n",
    "    \"ess_7_model_answers\": load_obj(\"data/essaySet_7_model_answers\"),\n",
    "    \"ess_8_model_answers\": load_obj(\"data/essaySet_8_model_answers\"),\n",
    "    \"ess_9_model_answers\": load_obj(\"data/essaySet_9_model_answers\"),\n",
    "    \"ess_10_model_answers\": load_obj(\"data/essaySet_10_model_answers\"),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "# doc = 'You need to know how much vinegar was used in each container.'\n",
    "# remove stopwords from doc\n",
    "# tokens = [w for w in set(doc.split()) if w not in stop_words]\n",
    "# remove punctuation from tokens\n",
    "# tokens = [w.translate(str.maketrans('', '', string.punctuation)) for w in [doc]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for essay in range(0,10):\n",
    "    key = f'ess_{essay+1}_model_answers'\n",
    "    ess_dict[key] = [w.translate(str.maketrans('', '', string.punctuation)) for w in ess_dict[key] ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_res = {}\n",
    "for essay in range(0,10):\n",
    "    key = f'ess_{essay+1}_model_answers'\n",
    "    ners = list(map(ner,ess_dict[key]))\n",
    "    ner_res[key] = ners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'45DEGC': 'CARDINAL', '53DEGC': 'CARDINAL'},\n",
       " {'Light Gray': 'WORK_OF_ART', 'the winter': 'DATE'},\n",
       " {'Light Gray Because': 'WORK_OF_ART', 'the summer': 'DATE'},\n",
       " {'1': 'CARDINAL', '54DEG': 'CARDINAL'},\n",
       " {'54 degrees': 'QUANTITY', '41 degrees': 'QUANTITY'},\n",
       " {'48 degrees': 'QUANTITY',\n",
       "  '45': 'CARDINAL',\n",
       "  'Li': 'PERSON',\n",
       "  '53DEGC': 'CARDINAL'},\n",
       " {'48DEG': 'CARDINAL'},\n",
       " {'Dark': 'ORG', '54DEG': 'CARDINAL', '41DEG': 'CARDINAL'},\n",
       " {},\n",
       " {},\n",
       " {'53DEGc': 'CARDINAL', '42DEGc': 'CARDINAL'},\n",
       " {'just 10 minutes': 'TIME',\n",
       "  '53': 'CARDINAL',\n",
       "  '10 minutes': 'TIME',\n",
       "  '48DEG': 'CARDINAL',\n",
       "  '10 minut': 'QUANTITY',\n",
       "  '45DEG': 'DATE'},\n",
       " {'Brandi': 'ORG', 'Jerry': 'PERSON'},\n",
       " {'One': 'CARDINAL', '53 degrees': 'QUANTITY', '42 degrees': 'QUANTITY'},\n",
       " {'54 degrees': 'QUANTITY'},\n",
       " {},\n",
       " {'Brandi': 'ORG',\n",
       "  'Jerrys': 'PERSON',\n",
       "  '6DEGC': 'CARDINAL',\n",
       "  '10DEGC': 'CARDINAL',\n",
       "  '12': 'CARDINAL',\n",
       "  'DEGwarmer': 'PRODUCT'},\n",
       " {'54DEG 52DEG 53DEG': 'CARDINAL', '53DEGc': 'CARDINAL'},\n",
       " {},\n",
       " {'6ยบ': 'CARDINAL'},\n",
       " {'8ยบ': 'CARDINAL'},\n",
       " {'42ยบ': 'CARDINAL'}]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_res[\"ess_10_model_answers\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "named_entites_dict = {}\n",
    "for essay in range(0,10):\n",
    "    key = f'ess_{essay+1}_model_answers'\n",
    "    named_entites = list(filter(lambda x: x if x else None, ner_res[key]))\n",
    "    named_entites = list(map(lambda named_entity: list(filter(lambda x: x not in EXCEPTION_ENTITES ,named_entity)), named_entites))\n",
    "    # flatten the list\n",
    "    named_entites = [*set([item for sublist in named_entites for item in sublist])]\n",
    "    named_entites = [*set([str(item).lower() for item in named_entites])]\n",
    "    named_entites_dict[key] = named_entites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[13, 8, 27, 21, 23, 9, 17, 16, 25, 36]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[len(v) for i,v in named_entites_dict.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_grading(\n",
    "    entities: List[str],\n",
    "    doc: str)\\\n",
    "        -> float:\n",
    "    # type check\n",
    "    if not isinstance(entities, list):\n",
    "        entities = [entities]\n",
    "    #  entities contain stop words\n",
    "    grade = [True\n",
    "            for entity in entities\n",
    "            if entity in doc]\n",
    "    try:\n",
    "        return len(grade)/len(entities)\n",
    "    except ZeroDivisionError:\n",
    "        # return -1.0\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [ df.query(f'EssaySet == {i}')[\"EssayText\"].values.tolist() for i in range(1,11)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\oeb\\AppData\\Local\\Temp/ipykernel_2932/542676692.py:1: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  ner_grades = np.array(list(map(lambda essay: np.array(list(map(lambda student_answer:\n"
     ]
    }
   ],
   "source": [
    "ner_grades = np.array(list(map(lambda essay: np.array(list(map(lambda student_answer:\n",
    "        match_grading(named_entites,student_answer),\n",
    "        essay))),docs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1672, 1278, 1891, 1738, 1795, 1797, 1799, 1799, 1798, 1640]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[len(i)for i in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1672, 1278, 1891, 1738, 1795, 1797, 1799, 1799, 1798, 1640]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[i.shape[0] for i in ner_grades.tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a directory to save the results and save\n",
    "os.makedirs('data/results', exist_ok=True)\n",
    "save_obj(ner_grades,\"data/results/ner_res\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# specail keys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## we can get around lack of this data by taking the intersection sets of each model answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "enclosure = \"\\\"\\\"\"\n",
    "ess_set_keys = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "ess1_set = [set(model_ans.split()) for model_ans in ess_dict[\"ess_1_model_answers\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "for essay in range(0,10):\n",
    "    key = f'ess_{essay+1}_model_answers'\n",
    "    ess_set = [set(model_ans.split()) for model_ans in ess_dict[key]]\n",
    "    o = ess_set[0]\n",
    "    for i in range(1,11):\n",
    "        o = o.intersection(ess1_set[i])\n",
    "    ess_set_keys[key] = [*o]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ess_1_model_answers': ['need', 'to'],\n",
       " 'ess_2_model_answers': [],\n",
       " 'ess_3_model_answers': ['to'],\n",
       " 'ess_4_model_answers': ['to'],\n",
       " 'ess_5_model_answers': ['to'],\n",
       " 'ess_6_model_answers': ['to'],\n",
       " 'ess_7_model_answers': ['to'],\n",
       " 'ess_8_model_answers': ['to'],\n",
       " 'ess_9_model_answers': ['to'],\n",
       " 'ess_10_model_answers': ['to']}"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ess_set_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ess_1_model_answers': ['need', 'to'],\n",
       " 'ess_2_model_answers': [' '],\n",
       " 'ess_3_model_answers': ['to'],\n",
       " 'ess_4_model_answers': ['to'],\n",
       " 'ess_5_model_answers': ['to'],\n",
       " 'ess_6_model_answers': ['to'],\n",
       " 'ess_7_model_answers': ['to'],\n",
       " 'ess_8_model_answers': ['to'],\n",
       " 'ess_9_model_answers': ['to'],\n",
       " 'ess_10_model_answers': ['to']}"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ess_set_keys[\"ess_2_model_answers\"]=[\" \"]\n",
    "ess_set_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "special_keywords_res = {}\n",
    "for essay in range(0,10):\n",
    "    key = f'ess_{essay+1}_model_answers'\n",
    "    # hard_keywords = list(map(lambda doc: key_words.get_str_between(doc, enclosure),ess_dict[key]))\n",
    "    hard_keywords = ess_set_keys[key]\n",
    "    # special_keywords = list(map(special_keywords,ess_dict[key]))\n",
    "    docs = df.query(f'EssaySet == {essay+1}')[\"EssayText\"].values.tolist()\n",
    "    # hard_keywords = list(map(lambda keys: match_grading(keys,doc),hard_keywords))\n",
    "    hard_keywords = key_words.hard_keywords_grading(hard_keywords,docs)\n",
    "    special_keywords_res[key] = hard_keywords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1672,)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "special_keywords_res[\"ess_1_model_answers\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a directory to save the results and save\n",
    "os.makedirs('data/results', exist_ok=True)\n",
    "\n",
    "save_obj(special_keywords_res,\"data/results/special_keywords_res\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0f876b07db73824ba94c3da26a300833b9286c0dd0d4e31723ae4574ddd9b9bb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
