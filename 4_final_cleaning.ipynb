{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "from sentence_transformers.util import cos_sim\n",
    "import sys\n",
    "import key_words\n",
    "\n",
    "# from sklearn.feature_extraction.text import CountVectorizer\n",
    "# from sklearn.model_selection import train_test_split\n",
    "from typing import Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_names_t:dict={\"students_emb\": Tuple[str or None, bool],\n",
    "                    \"students_answers\":Tuple[str or None, bool],\n",
    "                    \"model_answers_emb\":Tuple[str or None, bool],\n",
    "                    \"model_answers\":Tuple[str or None, bool],\n",
    "                    \"siamese_sores\":Tuple[str or None, bool],\n",
    "                    \"students_keywords\":Tuple[str or None, bool],\n",
    "                    \"students_keywords_emb\":Tuple[str or None, bool],\n",
    "                    \"model_keywords\":Tuple[str or None, bool],\n",
    "                    \"model_keywords_emb\":Tuple[str or None, bool],\n",
    "                    \"keywords_scores\":Tuple[str or None, bool],\n",
    "                    \"ner_keywords\":Tuple[str or None, bool],\n",
    "                    \"ner_keywords_scores\":Tuple[str or None, bool],\n",
    "                    \"labels\":Tuple[str or None, bool],}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_names:cols_names_t = {\"students_emb\": (\"students_emb\", True),\n",
    "                    \"students_answers\":(\"EssayText\", True),\n",
    "                    \"model_answers_emb\":(\"model_answers_emb\", True),\n",
    "                    \"model_answers\":(\"model_answers\", True),\n",
    "                    \"siamese_sores\":(None, True),\n",
    "                    \"students_keywords\":(\"students_keywords\", True),\n",
    "                    \"students_keywords_emb\":(\"students_keywords_emb\", True),\n",
    "                    \"model_keywords\":(\"model_keywords\", True),\n",
    "                    \"model_keywords_emb\":(\"model_keywords_emb\", True),\n",
    "                    \"keywords_scores\":(\"keywords_scores\", True),\n",
    "                    \"ner_keywords\":(\"ner_keywords\", True),\n",
    "                    \"ner_keywords_scores\":(\"ner_keywords_scores\", True),\n",
    "                    \"labels\":(\"labels\", True),}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = \"data/train_phase1.tsv\"\n",
    "df = pd.read_csv(train_path, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "def save_obj(obj:object,name:str):\n",
    "    ext = '.pickle'\n",
    "    with open(name + ext, 'wb') as handle:\n",
    "        pickle.dump(obj, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def load_obj(name:str)->object:\n",
    "    ext = '.pickle'\n",
    "    with open(name + ext, 'rb') as handle:\n",
    "        return pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformer_model\n",
    "import grading_model\n",
    "\n",
    "BERT = transformer_model.BERTModel()\n",
    "\n",
    "GM = grading_model.GradingModel(BERT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FineTuining(grading_model.GradingModel):\n",
    "    def __init__(self, BERT):\n",
    "        super().__init__(BERT)\n",
    "        self.BERT = BERT\n",
    "        self.model = self.BERT.model\n",
    "        self.tokenizer = self.BERT.tokenizer\n",
    "        self.device = self.BERT.device\n",
    "        self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "        self.tokenizer.to(self.device)\n",
    "\n",
    "    # def predict(self, text):\n",
    "    #     tokens = self.tokenizer.encode(text)\n",
    "    #     tokens = tokens.unsqueeze(0)\n",
    "    #     with torch.no_grad():\n",
    "    #         tokens = tokens.to(self.device)\n",
    "    #         outputs = self.model(tokens)\n",
    "    #         outputs = torch.nn.functional.softmax(outputs, dim=2)\n",
    "    #         outputs = outputs.squeeze(0)\n",
    "    #         outputs = outputs.cpu().numpy()\n",
    "    #         outputs = np.argmax(outputs, axis=1)\n",
    "    #         outputs = self.tokenizer.decode(outputs)\n",
    "    #     return outputs\n",
    "    def pre_fit(self, df,students_emb,model_answers_emb,\n",
    "            top_n=6,diversity=0.5,n_gram_range=(1,2),\n",
    "            threshold=0.5,exception_entites=None,\n",
    "                cols_names:dict={\n",
    "                    \"students_emb\": Tuple[str or None, bool],\n",
    "                    \"students_answers\":Tuple[str or None, bool],\n",
    "                    \"model_answers_emb\":Tuple[str or None, bool],\n",
    "                    \"model_answers\":Tuple[str or None, bool],\n",
    "                    \"siamese_scores\":Tuple[str or None, bool],\n",
    "                    \"students_keywords\":Tuple[str or None, bool],\n",
    "                    \"students_keywords_emb\":Tuple[str or None, bool],\n",
    "                    \"model_keywords\":Tuple[str or None, bool],\n",
    "                    \"model_keywords_emb\":Tuple[str or None, bool],\n",
    "                    \"keywords_scores\":Tuple[str or None, bool],\n",
    "                    \"ner_keywords\":Tuple[str or None, bool],\n",
    "                    \"ner_keywords_scores\":Tuple[str or None, bool],\n",
    "                    \"labels\":Tuple[str or None, bool],}):\n",
    "        \n",
    "        \"\"\"\n",
    "        pre-fit the model with dataframe\n",
    "\n",
    "        Args:\n",
    "            df (pd.DataFrame): dataframe\n",
    "            cols_names (dict): dictionary of columns names\n",
    "                key is column name\n",
    "                value is tuple of (string, bool)\n",
    "                    (column name or None, is_column_name_needed)\n",
    "\n",
    "        \"\"\"\n",
    "        for col,col_name,is_needed in cols_names.items():\n",
    "            if is_needed:\n",
    "                if col_name is None:\n",
    "                    col_name = col\n",
    "                df[col_name] = df[col]\n",
    "\n",
    "        self\n",
    "\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = \"haszem a kutya\"\n",
    "util = cos_sim(BERT.model.encode(s), np.eye(768,dtype=np.float32))\n",
    "util.shape\n",
    "np.eye(768,dtype=np.float32).shape\n",
    "eye = np.eye(768,dtype=np.float32)\n",
    "emb = BERT.model.encode(s).reshape(1, -1)\n",
    "sk = cosine_similarity(emb,eye)\n",
    "n = cosine_similarity(emb)\n",
    "cos = cos_sim(emb, emb)\n",
    "cos\n",
    "# get only 5 digits after decimal point\n",
    "np.array(cos).clip(-1, 1).round(6)\n",
    "n\n",
    "s = \".4\"\n",
    "s2 = \"4\"\n",
    "cos_sim(BERT.model.encode(s), BERT.model.encode(s2))\n",
    "cosine_similarity(BERT.model.encode(s).reshape(1, -1), BERT.model.encode(s2).reshape(1, -1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## optimize match_keywords function last output grade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.48 ms ± 28.9 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit reduce(lambda x,y: x+y,map(fn_, similarities))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.45 ms ± 52.9 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%timeit sum(map(fn_, similarities))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.51 ms ± 59.7 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit np.sum(np.array(list(map(fn_, similarities))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# optimize all keywords file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_emb = load_obj(\"data/ess_emb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ess_1_model_answers = load_obj(\"data/essaySet_1_model_answers\")\n",
    "ess_2_model_answers = load_obj(\"data/essaySet_2_model_answers\")\n",
    "ess_3_model_answers = load_obj(\"data/essaySet_3_model_answers\")\n",
    "ess_4_model_answers = load_obj(\"data/essaySet_4_model_answers\")\n",
    "ess_5_model_answers = load_obj(\"data/essaySet_5_model_answers\")\n",
    "ess_6_model_answers = load_obj(\"data/essaySet_6_model_answers\")\n",
    "ess_7_model_answers = load_obj(\"data/essaySet_7_model_answers\")\n",
    "ess_8_model_answers = load_obj(\"data/essaySet_8_model_answers\")\n",
    "ess_9_model_answers = load_obj(\"data/essaySet_9_model_answers\")\n",
    "ess_10_model_answers = load_obj(\"data/essaySet_10_model_answers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ess_1_model_answers_emb = BERT.model.encode(ess_1_model_answers)\n",
    "ess_2_model_answers_emb = BERT.model.encode(ess_2_model_answers)\n",
    "ess_3_model_answers_emb = BERT.model.encode(ess_3_model_answers)\n",
    "ess_4_model_answers_emb = BERT.model.encode(ess_4_model_answers)\n",
    "ess_5_model_answers_emb = BERT.model.encode(ess_5_model_answers)\n",
    "ess_6_model_answers_emb = BERT.model.encode(ess_6_model_answers)\n",
    "ess_7_model_answers_emb = BERT.model.encode(ess_7_model_answers)\n",
    "ess_8_model_answers_emb = BERT.model.encode(ess_8_model_answers)\n",
    "ess_9_model_answers_emb = BERT.model.encode(ess_9_model_answers)\n",
    "ess_10_model_answers_emb = BERT.model.encode(ess_10_model_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_dict = {'ess_1_model_answers_emb': ess_1_model_answers_emb,\n",
    " 'ess_2_model_answers_emb': ess_2_model_answers_emb,\n",
    " 'ess_3_model_answers_emb': ess_3_model_answers_emb,\n",
    " 'ess_4_model_answers_emb': ess_4_model_answers_emb,\n",
    " 'ess_5_model_answers_emb': ess_5_model_answers_emb,\n",
    " 'ess_6_model_answers_emb': ess_6_model_answers_emb,\n",
    " 'ess_7_model_answers_emb': ess_7_model_answers_emb,\n",
    " 'ess_8_model_answers_emb': ess_8_model_answers_emb,\n",
    " 'ess_9_model_answers_emb': ess_9_model_answers_emb,\n",
    " 'ess_10_model_answers_emb': ess_10_model_answers_emb}\n",
    "save_obj(emb_dict, \"data/model_answer_emb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_dict = load_obj(\"data/model_answer_emb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = df.query('EssaySet == 1')[\"EssayText\"].values.tolist()\n",
    "model_candidates = list(map(lambda ans: key_words.candidates_tokens(ans,n_gram_range=(1,2)),ess_1_model_answers))\n",
    "# model_candidate_emb = BERT.model.encode(model_candidates)\n",
    "# emb_dict[ess_1_model_answers_emb]\n",
    "model_candidate_emb = list(map(lambda  cand:BERT.model.encode(cand),model_candidates))\n",
    "\n",
    "\n",
    "l = zip(emb_dict['ess_1_model_answers_emb'],model_candidate_emb,model_candidates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_obj(l, \"data/ess_1_model_answers_emb__candidates\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maximal_marginal_relevance(doc_embedding: np.ndarray,\n",
    "        word_embeddings: np.ndarray,\n",
    "        words,\n",
    "        top_n = 5,\n",
    "        diversity = 0.8):\n",
    "    \"\"\"\n",
    "    Maximal Marginal Relevance algorithm for keyword extraction\n",
    "    * from KeyBERT repository on github\n",
    "\n",
    "    Args:\n",
    "        doc_embedding (numpy.ndarray): embedding of shape (1, 768)\n",
    "        word_embeddings (numpy.ndarray): embedding of shape (N, 768)\n",
    "        words (List[str]): list of words\n",
    "        top_n (Optional[int]): number of top words to extract\n",
    "        diversity (Optional[float]): diversity of top words to extract\n",
    "\n",
    "    Returns:\n",
    "        List[Tuple[str, float]]: list of top_n words with their scores\n",
    "    \"\"\"\n",
    "    # make sure 2d array\n",
    "    if doc_embedding.ndim == 1:\n",
    "        doc_embedding = doc_embedding.reshape(1, -1)\n",
    "\n",
    "    # Extract similarity within words, and between words and the document\n",
    "\n",
    "    word_doc_similarity = np.array(cos_sim(word_embeddings, doc_embedding)).clip(-1, 1).round(6)\n",
    "    word_similarity = np.array(cos_sim(word_embeddings, word_embeddings)).clip(-1, 1).round(6)\n",
    "\n",
    "    # Initialize candidates and already choose best keyword/keyphras\n",
    "    keywords_idx = [np.argmax(word_doc_similarity)]\n",
    "    candidates_idx = [i for i in range(len(words)) if i != keywords_idx[0]]\n",
    "\n",
    "    for _ in range(top_n - 1):\n",
    "        # Extract similarities within candidates and\n",
    "        # between candidates and selected keywords/phrases\n",
    "        candidate_similarities = word_doc_similarity[candidates_idx, :]\n",
    "        target_similarities = np.max(word_similarity[candidates_idx][:, keywords_idx], axis=1)\n",
    "\n",
    "        # Calculate maximal_marginal_relevance\n",
    "        mmr = (1-diversity) * candidate_similarities -\\\n",
    "            diversity * target_similarities.reshape(-1, 1)\n",
    "        # if return mmr is empty\n",
    "        if mmr.size == 0:\n",
    "            continue\n",
    "        mmr = candidates_idx[np.argmax(mmr)]\n",
    "\n",
    "        # Update keywords & candidates\n",
    "        keywords_idx.append(mmr)\n",
    "        candidates_idx.remove(mmr)\n",
    "\n",
    "    return [words[idx] for idx in keywords_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def emb_keywords(keywords):\n",
    "    x = np.array(list(map(lambda k:\n",
    "     np.array(list(map(lambda x: BERT.model.encode(str(x)), k))),keywords)))\n",
    "    if x.ndim == 3:\n",
    "        x = x.reshape(max(x.shape[1],x.shape[0]),x.shape[2])\n",
    "    # print(\"emb\",x.shape)\n",
    "    return x\n",
    "    # return list(map(lambda k: np.array(list(map(lambda x: BERT.model.encode(str(x)), k))),keywords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22\n",
      "[['know vinegar', 'container', 'need', 'vinegar used', 'used'], ['type vinegar', 'need know', 'container', 'used', 'vinegar used'], ['materials test', 'need know', 'know materials', 'test', 'know'], ['materials used', 'need know', 'size', 'surface area', 'know'], ['sample rinsed', 'need know', 'distilled water', 'long', 'sample'], ['know drying', 'method use', 'need know', 'drying method', 'know'], ['type container', 'need know', 'know size', 'use', 'container use'], ['replicate experiment', 'information need', 'vinegar poured', 'type containers', 'completely different'], ['know vinegar', 'additional information', 'putt sample', 'needed need', 'long'], ['samples vinegar', 'information need', 'replicate experiment', 'materials different', 'completely submerged'], ['students didn', 'vinegar add', 'referring secondly', 'specify samples', 'step need'], ['major experiment', 'know vinegar', 'containers use', 'poured lastly', 'material thats'], ['different containers', 'experiment need', 'vinegar added', 'left 24', 'affect data'], ['know vinegar', 'containers using', 'big', 'materials used', 'need know'], ['know vinegar', 'containers need', 'repeat experiment', 'temperature affects', 'just little'], ['kind vinegar', 'containers use', 'cups kind', 'use', 'vinegar use'], ['vinegar need', 'samples talking', 'know', 'long rinse', 'container'], ['size containers', 'additional information', 'experiment lastly', 'does matter', 'reaches different'], ['sample need', 'containers given', 'know mass', 'doesn tell', 'replicate experiment'], ['samples need', 'starting mass', 'students recieved', 'container order', 'replicate experiment'], ['mention vinegar', 'different samples', 'determine mass', 'procedure need', 'replicate group'], ['replicate experiment', 'specific want', 'vinegar add', 'containers use', 'step procedure']]\n"
     ]
    }
   ],
   "source": [
    "keywords = list(map(lambda x: maximal_marginal_relevance(x[0].reshape(1, -1),x[1],x[2]),zip(emb_dict['ess_1_model_answers_emb'],model_candidate_emb,model_candidates)))\n",
    "print(len(keywords))\n",
    "print(keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# keywords_emb = model.encode(keywords)\n",
    "\n",
    "students_candidates = list(map(lambda n_grams:list(map(lambda doc:key_words.get_candidates(n_grams, doc),docs)),students_n_grams))\n",
    "\n",
    "# students_candidates is len 22         models\n",
    "# students_candidates[0] is len 1672    essays\n",
    "# students_candidates[0][0] is len 2    ngrams\n",
    "# students_candidates[0][0][0] len 40   candidates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def emb_keywords(keywords):\n",
    "    # return np.array(list(map(lambda k: np.array(list(map(lambda x: BERT.model.encode(str(x)), k))),keywords)))\n",
    "    return np.array(list(map(lambda k: np.array(list(map(lambda x: get_words_emb(str(x)), k))),keywords)))\n",
    "    # return np.array(list(map(BERT.model.encode, keywords)))\n",
    "def old_emb_keywords(keywords):\n",
    "    return np.array(list(map(BERT.model.encode, keywords)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = list(map(lambda st: list(map(lambda x: len(x), st)),students_candidates))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22, 1672, 2)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "len(y),len(y[0]),y[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = list(map(lambda x: maximal_marginal_relevance(x[0].reshape(1, -1),x[1],x[2]),zip(emb_dict['ess_1_model_answers_emb'][:2],model_candidate_emb[:2],model_candidates[:2])))\n",
    "students_n_grams = list(map(lambda keyword: sorted(key_words.get_n_grams(keyword)),keywords))\n",
    "students_candidates = list(map(lambda n_grams:list(map(lambda doc:key_words.get_candidates(n_grams, doc),docs[:20])),students_n_grams))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "u = list(map(lambda st: list(map(len, st)),students_candidates))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 20, 20)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(u), len(u[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(students_candidates[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65.8284235000001\n"
     ]
    }
   ],
   "source": [
    "t1 = time.perf_counter()\n",
    "# students_candidates_emb = list(map(lambda cand:list(map(lambda x: np.array(\n",
    "#     list(map(BERT.model.encode, x)))), cand)),students_candidates[0])\n",
    "# students_candidates_emb = list(map(lambda st: list(map(emb_keywords, st)),students_candidates))\n",
    "students_candidates_emb = list(map(lambda st: list(map(lambda kwrds: list(map(emb_keywords, kwrds)), st)) ,students_candidates))\n",
    "\n",
    "print(time.perf_counter() - t1)\n",
    "# keywords_embeddings =  list(map(lambda x: np.array(list(map(BERT.model.encode, x))), keywords))\n",
    "# print(students_candidates_emb)\n",
    "# print(keywords_embeddings[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kw_match(keywords_emb,candidates_emb):\n",
    "    combination = list(zip(keywords_emb,candidates_emb))\n",
    "\n",
    "    similarities = list(map(lambda comb:\n",
    "                    np.array(cos_sim(comb[0],\n",
    "                    comb[1])).clip(-1, 1).round(6),\n",
    "                    combination))\n",
    "\n",
    "    def fn_ (x: np.array):\n",
    "        \"\"\"\n",
    "        return the no. of matched keywords\n",
    "        \"\"\"\n",
    "        if not np.sum(x>= thershold):\n",
    "            return 0\n",
    "        if np.sum(x >= thershold) > 1.0:\n",
    "            return 1.0\n",
    "        return np.sum(x >= thershold)\n",
    "\n",
    "    # res = np.sum(np.array(list(map(fn_, similarities))))\n",
    "    # 5.51 ms ± 59.7 µs per loop (mean ± std. dev. of 7 runs, 100 loops each) for 600 elements\n",
    "\n",
    "    res = sum(map(fn_, similarities))\n",
    "    # 5.45 ms ± 52.9 µs per loop (mean ± std. dev. of 7 runs, 100 loops each) for 600 elements\n",
    "    # return res/float(len(keywords_emb))\n",
    "    return res/len(keywords_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40, 768)"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "students_candidates_emb[0][0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_candidates = list(map(lambda ans: key_words.candidates_tokens(ans,n_gram_range=(1,2)),ess_1_model_answers))\n",
    "# model_candidate_emb = BERT.model.encode(model_candidates)\n",
    "# emb_dict[ess_1_model_answers_emb]\n",
    "model_candidate_emb = list(map(lambda cand:BERT.model.encode(cand),model_candidates))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keywords = list(map(lambda x: maximal_marginal_relevance(\n",
    "#     x[0].reshape(1, -1),x[1],x[2]),\n",
    "#     zip(emb_dict['ess_1_model_answers_emb'][:2],\n",
    "#     model_candidate_emb[:2],model_candidates[:2])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = list(map(lambda x: maximal_marginal_relevance(\n",
    "    x[0].reshape(1, -1),x[1],x[2]),zip(emb_dict['ess_1_model_answers_emb'],\n",
    "    model_candidate_emb,model_candidates)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22, 1672)"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(keywords), len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\oeb\\AppData\\Local\\Temp/ipykernel_15544/1829135151.py:2: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return np.array(list(map(BERT.model.encode, keywords)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "1\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "2\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "3\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "4\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "5\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "6\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "7\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "8\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "9\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "10\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "11\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "12\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "13\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "14\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "15\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "16\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "17\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "18\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "19\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "20\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "21\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Error: empty vocabulary; perhaps the documents only contain stop words\n"
     ]
    }
   ],
   "source": [
    "n_grams_list = []\n",
    "students_candidates_list = []\n",
    "keywords_embeddings_list = []\n",
    "students_candidates_emb_list = []\n",
    "\n",
    "batch = 50\n",
    "n_docs = len(docs)\n",
    "# all model answers\n",
    "for ind, ans in enumerate(keywords):\n",
    "    print(ind)\n",
    "    students_n_grams = key_words.get_n_grams(ans)\n",
    "    keywords_embeddings =  list(map(emb_keywords, ans))\n",
    "    \n",
    "    students_candidates_list_s = []\n",
    "    students_candidates_emb_list_s = []\n",
    "    # do in batches\n",
    "    for i in range(0,n_docs,batch):\n",
    "        students_candidates = list(map(lambda doc:\n",
    "                        key_words.get_candidates(students_n_grams, doc),\n",
    "                        docs[i:i+batch]))\n",
    "        students_candidates_emb =  list(map(emb_keywords, students_candidates))\n",
    "        students_candidates_list_s.extend(students_candidates)\n",
    "        students_candidates_emb_list_s.extend(students_candidates_emb)\n",
    "\n",
    "    if n_docs % batch != 0:\n",
    "        students_candidates = list(map(lambda doc:\n",
    "                        key_words.get_candidates(students_n_grams, doc),\n",
    "                        docs[i+batch:]))\n",
    "        students_candidates_emb =  list(map(emb_keywords, students_candidates))\n",
    "\n",
    "        students_candidates_list_s.extend(students_candidates)\n",
    "        students_candidates_emb_list_s.extend(students_candidates_emb)\n",
    "\n",
    "    n_grams_list.append(students_n_grams)\n",
    "    keywords_embeddings_list.append(keywords_embeddings)\n",
    "    \n",
    "    students_candidates_list.append(students_candidates_list_s)\n",
    "    students_candidates_emb_list.append(students_candidates_emb_list_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1694, 2, 40)"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(students_candidates_list[0]) , len(students_candidates_list[0][0]), len(students_candidates_list[0][0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*the length is abit missed up do we will ignore the last 50 as it is very expensive to re-compute*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the data\n",
    "save_obj(keywords, 'data/essay1_keywords')\n",
    "save_obj(n_grams_list, 'data/essay1_n_grams_list')\n",
    "save_obj(keywords_embeddings_list, 'data/essay1_keywords_embeddings_list')\n",
    "save_obj(students_candidates_list, 'data/essay1_students_candidates_list')\n",
    "# this one is huge 5.5 GB\n",
    "save_obj(students_candidates_emb_list, 'data/essay1_students_candidates_emb_list')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n"
     ]
    }
   ],
   "source": [
    "keywords_grades_list = []\n",
    "for i in range(len(n_grams_list)):\n",
    "    print(i)\n",
    "    keywords_grades = np.array(list(map(lambda st_cand:\n",
    "                key_words.match_keywords(keywords_embeddings_list[i], st_cand,\n",
    "                thershold=0.5),\n",
    "                students_candidates_emb_list[i]\n",
    "                )))\n",
    "    keywords_grades_list.append(keywords_grades)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimzation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "see what is faster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43.1 ms ± 319 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit list(map(lambda doc:key_words.get_candidates(students_n_grams, doc),docs[:100]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43 ms ± 363 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "docs_100 = docs[:100]\n",
    "batch = 50\n",
    "n_docs = len(docs_100)\n",
    "def dummy():\n",
    "    for i in range(0,n_docs,batch):\n",
    "        students_candidates = list(map(lambda doc:\n",
    "                    key_words.get_candidates(students_n_grams, doc),\n",
    "                    docs_100[i:i+batch]))\n",
    "%timeit dummy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> in batch mode is faster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(1, 2), (2, 3)],\n",
       " [(1, 2), (2, 3)],\n",
       " [(1, 2), (2, 3)],\n",
       " [(1, 2), (2, 3)],\n",
       " [(1, 2), (2, 3)],\n",
       " [(1, 2), (2, 3)],\n",
       " [(1, 2), (2, 3)],\n",
       " [(2, 3)],\n",
       " [(1, 2), (2, 3)],\n",
       " [(2, 3)],\n",
       " [(2, 3)],\n",
       " [(2, 3)],\n",
       " [(2, 3)],\n",
       " [(1, 2), (2, 3)],\n",
       " [(2, 3)],\n",
       " [(1, 2), (2, 3)],\n",
       " [(1, 2), (2, 3)],\n",
       " [(2, 3)],\n",
       " [(2, 3)],\n",
       " [(2, 3)],\n",
       " [(2, 3)],\n",
       " [(2, 3)]]"
      ]
     },
     "execution_count": 313,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#before \n",
    "n_grams_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "already in dict 0\n",
      "already in dict 0\n",
      "already in dict 1\n",
      "already in dict 1\n",
      "already in dict 2\n",
      "already in dict 2\n",
      "already in dict 3\n",
      "already in dict 3\n",
      "already in dict 4\n",
      "already in dict 4\n",
      "already in dict 5\n",
      "already in dict 5\n",
      "already in dict 6\n",
      "already in dict 6\n",
      "already in dict 7\n",
      "already in dict 7\n",
      "already in dict 8\n",
      "already in dict 8\n",
      "already in dict 9\n",
      "already in dict 9\n",
      "already in dict 10\n",
      "already in dict 10\n",
      "already in dict 11\n",
      "already in dict 11\n",
      "already in dict 12\n",
      "already in dict 12\n",
      "already in dict 13\n",
      "already in dict 13\n",
      "already in dict 14\n",
      "already in dict 14\n",
      "already in dict 15\n",
      "already in dict 15\n",
      "already in dict 16\n",
      "already in dict 16\n",
      "already in dict 17\n",
      "already in dict 17\n",
      "already in dict 18\n",
      "already in dict 18\n",
      "already in dict 19\n",
      "already in dict 19\n",
      "already in dict 20\n",
      "already in dict 20\n",
      "already in dict 21\n",
      "already in dict 21\n",
      "already in dict 22\n",
      "already in dict 22\n",
      "already in dict 23\n",
      "already in dict 23\n",
      "already in dict 24\n",
      "already in dict 24\n",
      "already in dict 25\n",
      "already in dict 25\n",
      "already in dict 26\n",
      "already in dict 26\n",
      "already in dict 27\n",
      "already in dict 27\n",
      "already in dict 28\n",
      "already in dict 28\n",
      "already in dict 29\n",
      "already in dict 29\n",
      "already in dict 30\n",
      "already in dict 30\n",
      "already in dict 31\n",
      "already in dict 31\n",
      "already in dict 32\n",
      "already in dict 32\n",
      "already in dict 33\n",
      "already in dict 33\n",
      "already in dict 34\n",
      "already in dict 34\n",
      "already in dict 35\n",
      "already in dict 35\n",
      "already in dict 36\n",
      "already in dict 36\n",
      "already in dict 37\n",
      "already in dict 37\n",
      "already in dict 38\n",
      "already in dict 38\n",
      "already in dict 39\n",
      "already in dict 39\n",
      "already in dict 40\n",
      "already in dict 40\n",
      "already in dict 41\n",
      "already in dict 41\n",
      "already in dict 42\n",
      "already in dict 42\n",
      "already in dict 43\n",
      "already in dict 43\n",
      "already in dict 44\n",
      "already in dict 44\n",
      "already in dict 45\n",
      "already in dict 45\n",
      "already in dict 46\n",
      "already in dict 46\n",
      "already in dict 47\n",
      "already in dict 47\n",
      "already in dict 48\n",
      "already in dict 48\n",
      "already in dict 49\n",
      "already in dict 49\n"
     ]
    }
   ],
   "source": [
    "batch = 50\n",
    "# ! \n",
    "# n_docs = len(docs)\n",
    "n_docs = 3\n",
    "\n",
    "# we will try to eliminate redundant n_grams in further keyword extraction\n",
    "# to omptimize the process\n",
    "# dict comprehension\n",
    "# ! becarful to double the len of the dict as mapping is can re \n",
    "n_gram_dict = {ind: {n_gram: None for n_gram in set([item for sublist in n_grams_list for item in sublist])} for ind ,doc in enumerate(docs[:n_docs+batch])}\n",
    "# print(n_gram_dict)\n",
    "# do the same process of keywords but using n_gram_dict\n",
    "n_grams_list_x = []\n",
    "students_candidates_list_x = []\n",
    "keywords_embeddings_list_x = []\n",
    "students_candidates_emb_list_x = []\n",
    "def check_n_gram_dict(n_grams, doc, ind):\n",
    "    res = []\n",
    "    for n_gram in n_grams:\n",
    "        if n_gram_dict[ind].get(n_gram) is None:\n",
    "            # print(\"still not in dict\",ind)\n",
    "            x =  key_words.get_candidates([n_gram], doc)\n",
    "            n_gram_dict[ind][n_gram] = x\n",
    "            res.append(x)\n",
    "        else:\n",
    "            print(\"already in dict\", ind)\n",
    "            res.append(n_gram_dict[ind].get(n_gram))\n",
    "    return res\n",
    "\n",
    "if n_docs < batch:\n",
    "    batch = n_docs\n",
    "\n",
    "# all model answers\n",
    "for ind, ans in enumerate(keywords[:2]):\n",
    "    print(ind)\n",
    "    students_n_grams = key_words.get_n_grams(ans)\n",
    "    keywords_embeddings =  list(map(emb_keywords, ans))\n",
    "\n",
    "    students_candidates_list_s_x = []\n",
    "    students_candidates_emb_list_s_x = []\n",
    "\n",
    "    # do in batches\n",
    "    for i in range(0,n_docs,batch):\n",
    "        # students_candidates = []\n",
    "        # for ind, doc in enumerate(docs[i:i+batch]):\n",
    "        #     students_candidates.extend(check_n_gram_dict(students_n_grams, doc, ind))\n",
    "        students_candidates = list(map(lambda doc:\n",
    "                        check_n_gram_dict(students_n_grams, doc[1], i+doc[0]),\n",
    "                        enumerate(docs[i:i+batch])))\n",
    "        students_candidates_emb =  list(map(emb_keywords, students_candidates))\n",
    "\n",
    "        students_candidates_list_s_x.extend(students_candidates)\n",
    "        students_candidates_emb_list_s_x.extend(students_candidates_emb)\n",
    "\n",
    "    if n_docs % batch != 0 and n_docs > batch:\n",
    "        students_candidates = list(map(lambda doc:\n",
    "                        key_words.get_candidates(students_n_grams, doc),\n",
    "                        docs[i+batch:]))\n",
    "        students_candidates_emb =  list(map(emb_keywords, students_candidates))\n",
    "\n",
    "        students_candidates_list_s_x.extend(students_candidates)\n",
    "        students_candidates_emb_list_s_x.extend(students_candidates_emb)\n",
    "\n",
    "    n_grams_list_x.append(students_n_grams)\n",
    "    keywords_embeddings_list_x.append(keywords_embeddings)\n",
    "\n",
    "    students_candidates_list_x.append(students_candidates_list_s_x)\n",
    "    students_candidates_emb_list_x.append(students_candidates_emb_list_s_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 2, 1, 40)"
      ]
     },
     "execution_count": 371,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(students_candidates_list_s_x),len(students_candidates_list_s_x[0]),len(students_candidates_list_s_x[0][0]),len(students_candidates_list_s_x[0][0][0]),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "keywords_grades_list_x = []\n",
    "for i in range(len(keywords[:2])):\n",
    "    print(i)\n",
    "    keywords_grades = np.array(list(map(lambda st_cand:\n",
    "                key_words.match_keywords(keywords_embeddings_list_x[i], st_cand,\n",
    "                thershold=0.5),\n",
    "                students_candidates_emb_list_x[i]\n",
    "                )))\n",
    "    keywords_grades_list_x.append(keywords_grades)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, True)"
      ]
     },
     "execution_count": 382,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keywords_grades_list_x[0].shape[0] ==batch , len(keywords_grades_list_x) ==2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> same as expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((40, 768), (2,))"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "students_candidates_emb[0][0].shape ,students_candidates_emb[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([array([[ 2.1295432e-02,  1.7679781e-04,  2.1370557e-01, ...,\n",
       "                -4.7906876e-02,  1.6631469e-01, -1.3477266e-01],\n",
       "               [ 3.4837324e-02,  4.1363299e-01,  3.4743777e-01, ...,\n",
       "                -2.3453319e-02, -1.5393102e-01, -1.4185250e-01],\n",
       "               [ 5.0341442e-02,  1.5775688e-01,  3.1891915e-01, ...,\n",
       "                -6.2302817e-02, -9.7999850e-04, -1.2756318e-01],\n",
       "               ...,\n",
       "               [ 7.9539068e-02, -1.4421634e-01,  2.3853749e-01, ...,\n",
       "                 8.1818052e-02, -8.8140994e-02,  2.5620740e-02],\n",
       "               [ 4.3749958e-02, -1.5811451e-01,  1.3444963e-02, ...,\n",
       "                -2.0325319e-03,  6.9406003e-02, -6.1969459e-02],\n",
       "               [ 1.2147342e-01,  2.7818733e-01,  3.0864418e-01, ...,\n",
       "                 5.2360082e-01, -1.6034922e-01, -1.0349195e-01]], dtype=float32),\n",
       "        array([[-1.52700648e-01, -2.33939305e-01,  3.17007661e-01, ...,\n",
       "                 2.50961781e-01,  2.79449254e-01, -1.25926390e-01],\n",
       "               [ 1.39775440e-01, -8.97890478e-02,  3.72249894e-02, ...,\n",
       "                 4.33539689e-01,  3.21120322e-01,  7.76598528e-02],\n",
       "               [ 2.12955177e-02,  1.76684553e-04,  2.13705540e-01, ...,\n",
       "                -4.79070246e-02,  1.66314691e-01, -1.34772748e-01],\n",
       "               ...,\n",
       "               [-6.65579885e-02,  2.67975312e-02,  1.96355194e-01, ...,\n",
       "                -2.76262388e-02,  1.06270745e-01, -8.27484950e-02],\n",
       "               [ 4.37500253e-02, -1.58114389e-01,  1.34449871e-02, ...,\n",
       "                -2.03266437e-03,  6.94059506e-02, -6.19695708e-02],\n",
       "               [ 1.21473432e-01,  2.78187215e-01,  3.08644146e-01, ...,\n",
       "                 5.23600757e-01, -1.60349175e-01, -1.03491940e-01]], dtype=float32)],\n",
       "       dtype=object),\n",
       " array([[[ 0.20318332, -0.12742275,  0.37175897, ...,  0.03886026,\n",
       "           0.34079033,  0.1558689 ],\n",
       "         [-0.0849773 ,  0.06122598,  0.13555624, ..., -0.172347  ,\n",
       "           0.03536982, -0.06337784],\n",
       "         [ 0.22295813, -0.04083218,  0.15055144, ..., -0.11840399,\n",
       "          -0.03320811, -0.03451685],\n",
       "         ...,\n",
       "         [ 0.10231219, -0.10860956,  0.10154093, ..., -0.04175395,\n",
       "          -0.02731597, -0.20023768],\n",
       "         [-0.01256124, -0.0185374 ,  0.20755044, ...,  0.22070342,\n",
       "          -0.03902075, -0.2508593 ],\n",
       "         [-0.08506317,  0.15908335,  0.05656061, ...,  0.40662947,\n",
       "           0.2523834 ,  0.22817008]],\n",
       " \n",
       "        [[ 0.10783594, -0.04388985,  0.30833617, ...,  0.44493628,\n",
       "           0.10673162,  0.1540657 ],\n",
       "         [ 0.38795075,  0.01213554,  0.10626116, ...,  0.31251836,\n",
       "           0.17384703, -0.12416197],\n",
       "         [ 0.20318332, -0.12742275,  0.37175897, ...,  0.03886026,\n",
       "           0.34079033,  0.1558689 ],\n",
       "         ...,\n",
       "         [-0.00399316, -0.30306748,  0.2877283 , ...,  0.3963727 ,\n",
       "           0.49359682,  0.272653  ],\n",
       "         [ 0.11792974,  0.20316859,  0.11574569, ...,  0.07172348,\n",
       "           0.12866692,  0.24937186],\n",
       "         [-0.08506317,  0.15908335,  0.05656061, ...,  0.40662947,\n",
       "           0.2523834 ,  0.22817008]]], dtype=float32),\n",
       " array([array([[ 0.20136234, -0.09084971,  0.03165424, ...,  0.17820014,\n",
       "                 0.20895837, -0.04353629],\n",
       "               [ 0.07309551,  0.06205757,  0.12308053, ...,  0.02941067,\n",
       "                -0.04629743, -0.1243099 ],\n",
       "               [ 0.05034152,  0.15775691,  0.31891927, ..., -0.06230289,\n",
       "                -0.00098002, -0.12756307],\n",
       "               ...,\n",
       "               [ 0.29340416, -0.00627239,  0.34772858, ...,  0.17106156,\n",
       "                -0.18306915, -0.06063427],\n",
       "               [ 0.05304671, -0.16881657,  0.23259123, ...,  0.10346675,\n",
       "                -0.0994814 , -0.10078573],\n",
       "               [ 0.10949878, -0.22762237,  0.1255171 , ..., -0.06013125,\n",
       "                 0.20544243, -0.13593842]], dtype=float32)              ,\n",
       "        array([[ 0.20136227, -0.09084977,  0.0316542 , ...,  0.17820024,\n",
       "                 0.20895839, -0.04353626],\n",
       "               [ 0.2249236 ,  0.1444485 , -0.02062207, ..., -0.01682441,\n",
       "                 0.09672745,  0.13993014],\n",
       "               [-0.02442224, -0.24629787,  0.1688489 , ...,  0.5537003 ,\n",
       "                 0.08628397, -0.09016966],\n",
       "               ...,\n",
       "               [-0.069433  ,  0.10125383,  0.10627663, ...,  0.0137388 ,\n",
       "                 0.00393307, -0.2063522 ],\n",
       "               [-0.08931378,  0.06668338,  0.11729177, ...,  0.15273105,\n",
       "                 0.03337568, -0.20298748],\n",
       "               [ 0.10949881, -0.22762236,  0.12551713, ..., -0.06013117,\n",
       "                 0.20544243, -0.13593839]], dtype=float32)              ],\n",
       "       dtype=object),\n",
       " array([array([[ 1.8198555e-02,  2.6945472e-01,  5.0378907e-01, ...,\n",
       "                -1.8462250e-01,  1.9398680e-01,  1.8124788e-01],\n",
       "               [ 1.1407077e-01, -1.4338528e-01,  3.1238735e-01, ...,\n",
       "                 1.4148277e-02, -5.2782774e-02,  5.4504175e-02],\n",
       "               [ 2.5982568e-01,  4.4579830e-02,  3.2490948e-01, ...,\n",
       "                 2.9488322e-01, -8.9753149e-03,  4.9914728e-04],\n",
       "               ...,\n",
       "               [-1.3299550e-01,  3.5376716e-01,  4.8092514e-01, ...,\n",
       "                -8.4934987e-02,  4.7461659e-02,  7.3972091e-02],\n",
       "               [-1.0072405e-01,  1.7140007e-01,  2.7445927e-01, ...,\n",
       "                -9.1682486e-02, -1.5025345e-02, -3.2398973e-03],\n",
       "               [-1.7462629e-01,  2.1332285e-01,  4.0721893e-01, ...,\n",
       "                -1.4702465e-01, -1.6401216e-02,  1.6626732e-01]], dtype=float32),\n",
       "        array([[ 0.01819847,  0.26945475,  0.50378895, ..., -0.1846225 ,\n",
       "                 0.19398686,  0.18124785],\n",
       "               [-0.15705207,  0.35687107,  0.49203473, ..., -0.08896583,\n",
       "                 0.12191176,  0.10378779],\n",
       "               [ 0.05425411,  0.3345719 ,  0.35200894, ..., -0.08146792,\n",
       "                -0.06395227,  0.27874702],\n",
       "               ...,\n",
       "               [-0.13625966,  0.13065939,  0.3218531 , ..., -0.12826678,\n",
       "                 0.17543462,  0.3674841 ],\n",
       "               [-0.13299549,  0.35376713,  0.48092505, ..., -0.08493492,\n",
       "                 0.04746171,  0.07397206],\n",
       "               [-0.17462632,  0.21332282,  0.40721896, ..., -0.1470247 ,\n",
       "                -0.01640103,  0.16626732]], dtype=float32)              ],\n",
       "       dtype=object),\n",
       " array([array([[-0.12716809, -0.05832044,  0.11385093, ...,  0.59584635,\n",
       "                 0.23627529, -0.3878401 ],\n",
       "               [ 0.05034152,  0.15775691,  0.31891927, ..., -0.06230289,\n",
       "                -0.00098002, -0.12756307],\n",
       "               [ 0.05577002,  0.05838428,  0.09642594, ...,  0.1682564 ,\n",
       "                -0.12393086, -0.22568372],\n",
       "               ...,\n",
       "               [-0.01256125, -0.01853751,  0.2075504 , ...,  0.22070354,\n",
       "                -0.03902073, -0.2508592 ],\n",
       "               [ 0.08254585, -0.01927309,  0.05488254, ...,  0.3098483 ,\n",
       "                 0.04715551, -0.11651389],\n",
       "               [ 0.01571394,  0.16567254,  0.4083006 , ...,  0.23945904,\n",
       "                 0.22825551,  0.25829032]], dtype=float32)              ,\n",
       "        array([[ 0.07276514,  0.20900147,  0.26515368, ..., -0.19245243,\n",
       "                 0.03425738,  0.05924842],\n",
       "               [-0.07343608,  0.02582074,  0.03562665, ...,  0.55963343,\n",
       "                 0.0953749 , -0.2500117 ],\n",
       "               [-0.12716815, -0.05832052,  0.11385097, ...,  0.59584606,\n",
       "                 0.23627539, -0.38784003],\n",
       "               ...,\n",
       "               [ 0.08254585, -0.01927316,  0.05488248, ...,  0.3098483 ,\n",
       "                 0.04715548, -0.11651376],\n",
       "               [ 0.01571394,  0.16567256,  0.4083006 , ...,  0.23945902,\n",
       "                 0.2282554 ,  0.25829026],\n",
       "               [ 0.07609117,  0.32968602,  0.305649  , ...,  0.44753143,\n",
       "                 0.32640925,  0.05469409]], dtype=float32)              ],\n",
       "       dtype=object),\n",
       " array([array([[-0.05130355,  0.21441948,  0.2016879 , ..., -0.1560773 ,\n",
       "                 0.0061666 , -0.13822573],\n",
       "               [-0.09120373,  0.21800303,  0.22832513, ..., -0.09039883,\n",
       "                 0.07536638, -0.18537839],\n",
       "               [ 0.19134486,  0.45450076,  0.3924228 , ...,  0.04275036,\n",
       "                -0.22636504, -0.23125762],\n",
       "               ...,\n",
       "               [ 0.26837456, -0.10752249,  0.42271313, ..., -0.0259877 ,\n",
       "                 0.02510378,  0.14584982],\n",
       "               [ 0.06685591,  0.12812127,  0.32655463, ..., -0.4061596 ,\n",
       "                -0.00174509, -0.48065248],\n",
       "               [ 0.12147339,  0.27818733,  0.30864415, ...,  0.5236009 ,\n",
       "                -0.1603492 , -0.10349201]], dtype=float32)              ,\n",
       "        array([[ 0.19740663,  0.15627442,  0.25131407, ...,  0.3882774 ,\n",
       "                -0.11765745, -0.10897193],\n",
       "               [-0.09120367,  0.21800312,  0.2283252 , ..., -0.0903988 ,\n",
       "                 0.07536642, -0.18537836],\n",
       "               [-0.05130339,  0.21441953,  0.201688  , ..., -0.1560772 ,\n",
       "                 0.00616652, -0.13822572],\n",
       "               ...,\n",
       "               [ 0.426844  , -0.35691032,  0.30567217, ..., -0.31536955,\n",
       "                 0.25926104,  0.24221513],\n",
       "               [ 0.39999288,  0.08185582,  0.36625618, ..., -0.29425192,\n",
       "                 0.13880149,  0.21632397],\n",
       "               [ 0.12147342,  0.27818733,  0.30864418, ...,  0.5236008 ,\n",
       "                -0.16034922, -0.10349195]], dtype=float32)              ],\n",
       "       dtype=object),\n",
       " array([array([[ 0.12494671, -0.06487076,  0.34459588, ...,  0.41379693,\n",
       "                -0.10994519, -0.1879673 ],\n",
       "               [ 0.07612221,  0.45254126,  0.39980993, ...,  0.23894279,\n",
       "                 0.07879975, -0.07907623],\n",
       "               [ 0.10866868, -0.09114345,  0.44809708, ...,  0.5537851 ,\n",
       "                -0.02942719, -0.22894745],\n",
       "               ...,\n",
       "               [ 0.10986631,  0.31929776,  0.42931262, ..., -0.00595196,\n",
       "                -0.15697676, -0.19084607],\n",
       "               [ 0.06685594,  0.12812127,  0.32655463, ..., -0.40615964,\n",
       "                -0.001745  , -0.48065248],\n",
       "               [ 0.12147342,  0.27818733,  0.30864418, ...,  0.5236008 ,\n",
       "                -0.16034922, -0.10349195]], dtype=float32)              ,\n",
       "        array([[-0.28716004, -0.11833537,  0.31042185, ...,  0.36227906,\n",
       "                 0.10097177, -0.09000069],\n",
       "               [ 0.07612221,  0.45254126,  0.39980993, ...,  0.23894279,\n",
       "                 0.07879975, -0.07907623],\n",
       "               [-0.14709595, -0.01855229,  0.3550574 , ...,  0.2301569 ,\n",
       "                 0.17304973, -0.27786916],\n",
       "               ...,\n",
       "               [-0.03182193,  0.07379465,  0.21737106, ..., -0.22684734,\n",
       "                -0.05577925, -0.05888262],\n",
       "               [-0.06937981, -0.14937165,  0.39499167, ...,  0.49770203,\n",
       "                 0.13464841, -0.02820549],\n",
       "               [ 0.12147342,  0.27818733,  0.30864418, ...,  0.5236008 ,\n",
       "                -0.16034922, -0.10349195]], dtype=float32)              ],\n",
       "       dtype=object),\n",
       " array([array([[-0.12716816, -0.05832049,  0.11385097, ...,  0.59584635,\n",
       "                 0.23627536, -0.38784012],\n",
       "               [-0.05086453,  0.09999641,  0.07417344, ...,  0.24045512,\n",
       "                 0.21988389, -0.12471031],\n",
       "               [ 0.26072046,  0.21060373,  0.31554034, ...,  0.5287292 ,\n",
       "                 0.03870532,  0.0640774 ],\n",
       "               ...,\n",
       "               [ 0.2623825 ,  0.13301426,  0.30738932, ...,  0.10537567,\n",
       "                 0.09900061,  0.17575295],\n",
       "               [ 0.24700315,  0.2554094 ,  0.37530336, ...,  0.2877715 ,\n",
       "                -0.18152285, -0.26265207],\n",
       "               [ 0.16159941, -0.4425171 ,  0.09837422, ..., -0.13373102,\n",
       "                 0.33071303, -0.152769  ]], dtype=float32)              ,\n",
       "        array([[ 0.19105041,  0.28563395,  0.35298502, ..., -0.00244475,\n",
       "                 0.0457423 , -0.06205629],\n",
       "               [-0.12716816, -0.05832049,  0.11385097, ...,  0.59584635,\n",
       "                 0.23627536, -0.38784012],\n",
       "               [-0.05086453,  0.09999637,  0.07417347, ...,  0.24045514,\n",
       "                 0.21988386, -0.12471031],\n",
       "               ...,\n",
       "               [ 0.2623825 ,  0.13301414,  0.30738938, ...,  0.10537574,\n",
       "                 0.0990007 ,  0.175753  ],\n",
       "               [ 0.10243106,  0.21962811,  0.17237025, ...,  0.706081  ,\n",
       "                -0.02642878, -0.19011183],\n",
       "               [ 0.16159941, -0.4425171 ,  0.09837422, ..., -0.13373102,\n",
       "                 0.33071303, -0.152769  ]], dtype=float32)              ],\n",
       "       dtype=object),\n",
       " array([array([[ 0.06415579, -0.2573146 ,  0.18113877, ..., -0.3652817 ,\n",
       "                 0.26535663, -0.16793317],\n",
       "               [ 0.12494677, -0.06487065,  0.34459582, ...,  0.4137969 ,\n",
       "                -0.10994513, -0.1879673 ],\n",
       "               [ 0.05034144,  0.15775688,  0.31891915, ..., -0.06230282,\n",
       "                -0.00098   , -0.12756318],\n",
       "               ...,\n",
       "               [ 0.04544844,  0.04519967,  0.24011922, ..., -0.05476   ,\n",
       "                -0.1201771 , -0.21362662],\n",
       "               [-0.00891492, -0.01965695,  0.15203978, ...,  0.24431007,\n",
       "                -0.0488713 , -0.12876011],\n",
       "               [-0.078363  , -0.06613354,  0.22031802, ..., -0.20230222,\n",
       "                 0.08945976, -0.02884746]], dtype=float32)              ,\n",
       "        array([[ 0.06415576, -0.2573146 ,  0.1811388 , ..., -0.3652817 ,\n",
       "                 0.26535666, -0.16793317],\n",
       "               [-0.02538118, -0.26607516,  0.3200292 , ...,  0.34949875,\n",
       "                 0.04360614, -0.05749195],\n",
       "               [ 0.11883219, -0.26073846,  0.11396077, ...,  0.15194853,\n",
       "                 0.24708073, -0.1863297 ],\n",
       "               ...,\n",
       "               [ 0.11792971,  0.20316868,  0.11574562, ...,  0.07172345,\n",
       "                 0.12866679,  0.24937192],\n",
       "               [ 0.10314979, -0.22599038,  0.13483523, ..., -0.04366499,\n",
       "                 0.28422037, -0.03585908],\n",
       "               [ 0.00823447,  0.2700317 ,  0.13773683, ...,  0.5943719 ,\n",
       "                 0.11304058, -0.01543278]], dtype=float32)              ],\n",
       "       dtype=object),\n",
       " array([[[ 0.03685615, -0.01048323,  0.42367998, ...,  0.307587  ,\n",
       "          -0.02111254, -0.19835298],\n",
       "         [ 0.03573563,  0.51350325,  0.1856182 , ...,  0.22622438,\n",
       "           0.1906367 ,  0.25827408],\n",
       "         [ 0.10593285,  0.09865517,  0.2769586 , ...,  0.43358564,\n",
       "          -0.09992244, -0.02455452],\n",
       "         ...,\n",
       "         [ 0.15605736, -0.24530423,  0.4284419 , ...,  0.28918165,\n",
       "           0.1605491 ,  0.04052865],\n",
       "         [-0.11446347, -0.1366173 ,  0.2732301 , ...,  0.33355948,\n",
       "           0.02379034,  0.01609845],\n",
       "         [ 0.24700312,  0.25540942,  0.3753033 , ...,  0.2877715 ,\n",
       "          -0.18152273, -0.262652  ]],\n",
       " \n",
       "        [[ 0.03573561,  0.5135032 ,  0.18561827, ...,  0.22622435,\n",
       "           0.19063671,  0.25827405],\n",
       "         [-0.14161915,  0.33449018,  0.09808462, ...,  0.20420483,\n",
       "           0.24488336,  0.23458353],\n",
       "         [-0.09022904,  0.11351514,  0.325839  , ...,  0.02561699,\n",
       "           0.13945478, -0.32681617],\n",
       "         ...,\n",
       "         [ 0.05199749,  0.1776782 ,  0.13640626, ..., -0.22560228,\n",
       "           0.13454053,  0.25329852],\n",
       "         [-0.00693179,  0.33255085,  0.2105436 , ...,  0.22558   ,\n",
       "           0.00770202,  0.26526642],\n",
       "         [-0.11446343, -0.13661732,  0.27323014, ...,  0.33355936,\n",
       "           0.02379025,  0.0160984 ]]], dtype=float32),\n",
       " array([array([[ 0.13796555,  0.0998468 ,  0.15995568, ...,  0.15101638,\n",
       "                 0.07098493,  0.29399493],\n",
       "               [-0.07198088, -0.09136944,  0.33276588, ...,  0.21778691,\n",
       "                -0.2390475 , -0.40762958],\n",
       "               [-0.09246806, -0.30208993,  0.32999524, ...,  0.04007116,\n",
       "                 0.03630536, -0.08148259],\n",
       "               ...,\n",
       "               [ 0.26837453, -0.10752241,  0.42271325, ..., -0.02598756,\n",
       "                 0.02510368,  0.14584972],\n",
       "               [-0.1792024 , -0.11232376,  0.39520735, ...,  0.25182468,\n",
       "                 0.04828367, -0.07096764],\n",
       "               [ 0.09010034,  0.21388458,  0.1638412 , ...,  0.3131306 ,\n",
       "                 0.02630351,  0.02474187]], dtype=float32)              ,\n",
       "        array([[ 0.1379655 ,  0.09984669,  0.15995577, ...,  0.15101641,\n",
       "                 0.07098482,  0.29399475],\n",
       "               [-0.18038674,  0.03666875,  0.34719002, ...,  0.09359834,\n",
       "                -0.22169311, -0.3739212 ],\n",
       "               [-0.2357357 , -0.24744666,  0.28160283, ...,  0.341962  ,\n",
       "                -0.12990569, -0.2770609 ],\n",
       "               ...,\n",
       "               [ 0.17075613,  0.06232484,  0.24673578, ..., -0.15172555,\n",
       "                 0.17154871,  0.33721888],\n",
       "               [-0.17920242, -0.11232388,  0.39520726, ...,  0.25182468,\n",
       "                 0.04828378, -0.07096769],\n",
       "               [ 0.11792977,  0.20316859,  0.11574554, ...,  0.07172354,\n",
       "                 0.12866688,  0.24937193]], dtype=float32)              ],\n",
       "       dtype=object),\n",
       " array([[[-0.0221353 ,  0.12740178,  0.26309174, ..., -0.09951479,\n",
       "          -0.0495046 , -0.25147238],\n",
       "         [ 0.23420224,  0.469784  ,  0.10411955, ...,  0.50390756,\n",
       "           0.41138774, -0.05320072],\n",
       "         [ 0.05034152,  0.15775691,  0.31891927, ..., -0.06230289,\n",
       "          -0.00098002, -0.12756307],\n",
       "         ...,\n",
       "         [ 0.05357457,  0.07947253,  0.16963974, ...,  0.21701941,\n",
       "          -0.07451922,  0.07497074],\n",
       "         [-0.01256125, -0.01853751,  0.2075504 , ...,  0.22070354,\n",
       "          -0.03902073, -0.2508592 ],\n",
       "         [ 0.06771494,  0.2150778 ,  0.18083791, ..., -0.15640615,\n",
       "          -0.21995322, -0.15917993]],\n",
       " \n",
       "        [[-0.02213538,  0.12740164,  0.2630918 , ..., -0.09951469,\n",
       "          -0.04950472, -0.25147244],\n",
       "         [-0.07069625, -0.21728778,  0.12859972, ...,  0.25635567,\n",
       "           0.31243223, -0.05357131],\n",
       "         [ 0.2342022 ,  0.46978402,  0.10411953, ...,  0.50390744,\n",
       "           0.41138774, -0.05320069],\n",
       "         ...,\n",
       "         [-0.31252787, -0.01409451,  0.27418193, ...,  0.3686541 ,\n",
       "           0.08361137, -0.01675699],\n",
       "         [-0.00149026, -0.01465124,  0.10279534, ...,  0.11265788,\n",
       "           0.12307741,  0.02550787],\n",
       "         [ 0.00962506,  0.26512107,  0.19700266, ...,  0.6855219 ,\n",
       "           0.03573313, -0.11934558]]], dtype=float32),\n",
       " array([array([[ 0.26961723,  0.2503119 ,  0.22249119, ...,  0.0129036 ,\n",
       "                 0.27022636,  0.31778753],\n",
       "               [ 0.2261222 ,  0.52393794,  0.28829926, ...,  0.41405952,\n",
       "                -0.03305536, -0.02880851],\n",
       "               [ 0.22073798, -0.02004804,  0.2045939 , ...,  0.42682412,\n",
       "                 0.32889932, -0.21870464],\n",
       "               ...,\n",
       "               [-0.0303326 , -0.09667878,  0.08336076, ...,  0.13965964,\n",
       "                 0.10318159, -0.07674681],\n",
       "               [ 0.24700312,  0.25540945,  0.37530342, ...,  0.2877715 ,\n",
       "                -0.18152277, -0.26265213],\n",
       "               [ 0.10661441, -0.12953348,  0.3163254 , ...,  0.462785  ,\n",
       "                 0.34347233, -0.00218774]], dtype=float32)              ,\n",
       "        array([[ 0.26961723,  0.25031203,  0.22249122, ...,  0.01290386,\n",
       "                 0.2702263 ,  0.31778744],\n",
       "               [-0.03870277,  0.27750975,  0.00279674, ...,  0.86659175,\n",
       "                 0.26348263, -0.13191156],\n",
       "               [ 0.22612216,  0.5239378 ,  0.28829926, ...,  0.41405958,\n",
       "                -0.03305528, -0.02880844],\n",
       "               ...,\n",
       "               [ 0.10472929,  0.14884712,  0.2538152 , ...,  0.80886227,\n",
       "                 0.21959406, -0.18937448],\n",
       "               [ 0.10661437, -0.12953357,  0.31632543, ...,  0.4627851 ,\n",
       "                 0.3434723 , -0.0021878 ],\n",
       "               [ 0.216345  ,  0.03150674,  0.1278224 , ...,  0.23355623,\n",
       "                -0.0703711 ,  0.11449828]], dtype=float32)              ],\n",
       "       dtype=object),\n",
       " array([array([[-0.07198088, -0.09136944,  0.33276588, ...,  0.21778691,\n",
       "                -0.2390475 , -0.40762958],\n",
       "               [-0.09246806, -0.30208993,  0.32999524, ...,  0.04007116,\n",
       "                 0.03630536, -0.08148259],\n",
       "               [ 0.05034137,  0.15775688,  0.3189192 , ..., -0.06230291,\n",
       "                -0.00098012, -0.12756303],\n",
       "               ...,\n",
       "               [ 0.12985659, -0.13455579,  0.3361442 , ...,  0.06455529,\n",
       "                -0.12954272, -0.2016948 ],\n",
       "               [-0.1792024 , -0.11232376,  0.39520735, ...,  0.25182468,\n",
       "                 0.04828367, -0.07096764],\n",
       "               [-0.04244938, -0.0277177 ,  0.28952077, ..., -0.07141194,\n",
       "                 0.14863957, -0.09589993]], dtype=float32)              ,\n",
       "        array([[-0.2357357 , -0.24744666,  0.28160283, ...,  0.341962  ,\n",
       "                -0.12990569, -0.2770609 ],\n",
       "               [-0.07198088, -0.09136955,  0.332766  , ...,  0.21778688,\n",
       "                -0.2390475 , -0.40762964],\n",
       "               [-0.23947273,  0.02258212,  0.30666193, ...,  0.21819365,\n",
       "                 0.05857215, -0.1140277 ],\n",
       "               [-0.17920242, -0.11232388,  0.39520726, ...,  0.25182468,\n",
       "                 0.04828378, -0.07096769],\n",
       "               [-0.04244932, -0.02771778,  0.2895208 , ..., -0.07141191,\n",
       "                 0.14863952, -0.0959    ]], dtype=float32)              ],\n",
       "       dtype=object),\n",
       " array([[[ 0.12494671, -0.06487069,  0.34459582, ...,  0.4137969 ,\n",
       "          -0.10994512, -0.18796736],\n",
       "         [-0.01697139, -0.0057612 ,  0.23351578, ..., -0.13365588,\n",
       "           0.00475948, -0.28672192],\n",
       "         [ 0.16140974,  0.21091838,  0.32777244, ..., -0.13641106,\n",
       "          -0.35101867, -0.18041946],\n",
       "         ...,\n",
       "         [ 0.18982036, -0.2843972 ,  0.4780849 , ..., -0.14748716,\n",
       "           0.3498436 , -0.1420597 ],\n",
       "         [ 0.06685594,  0.12812127,  0.32655463, ..., -0.40615964,\n",
       "          -0.001745  , -0.48065248],\n",
       "         [ 0.1282411 ,  0.33360633,  0.21377997, ...,  0.59562826,\n",
       "          -0.0970233 , -0.09836715]],\n",
       " \n",
       "        [[-0.01697139, -0.0057612 ,  0.23351578, ..., -0.13365588,\n",
       "           0.00475948, -0.28672192],\n",
       "         [ 0.09929253,  0.26625708,  0.2778843 , ..., -0.2662868 ,\n",
       "           0.16724426, -0.10802638],\n",
       "         [-0.10270125,  0.14497149,  0.08677633, ...,  0.3336013 ,\n",
       "           0.28590003,  0.24025221],\n",
       "         ...,\n",
       "         [ 0.03153757,  0.13842566,  0.19958234, ...,  0.0732626 ,\n",
       "           0.20227937,  0.0519353 ],\n",
       "         [ 0.01767708,  0.02769999,  0.32671326, ...,  0.00205193,\n",
       "           0.0465951 , -0.22114302],\n",
       "         [ 0.12824112,  0.33360636,  0.21377985, ...,  0.5956284 ,\n",
       "          -0.09702329, -0.09836718]]], dtype=float32),\n",
       " array([array([[ 0.12294543, -0.01093462,  0.2691335 , ..., -0.24243052,\n",
       "                -0.18074882, -0.24367659],\n",
       "               [ 0.18480067, -0.42306337,  0.2817315 , ..., -0.38979006,\n",
       "                 0.22106414, -0.19495618],\n",
       "               [ 0.0758671 , -0.48594767,  0.13294627, ..., -0.25456995,\n",
       "                 0.05345338, -0.25126296],\n",
       "               [ 0.120671  , -0.19301395,  0.20382321, ..., -0.03607839,\n",
       "                -0.06521732, -0.13637726],\n",
       "               [-0.03817671, -0.47242033,  0.07437764, ..., -0.33331823,\n",
       "                 0.00300538, -0.13311641]], dtype=float32)              ,\n",
       "        array([[-0.07301079, -0.48699513,  0.00882494, ..., -0.12862065,\n",
       "                -0.0565163 , -0.2783144 ],\n",
       "               [ 0.07586721, -0.4859476 ,  0.13294628, ..., -0.25456995,\n",
       "                 0.05345336, -0.251263  ],\n",
       "               [-0.03817668, -0.47242033,  0.07437769, ..., -0.3333183 ,\n",
       "                 0.00300533, -0.13311635]], dtype=float32)              ],\n",
       "       dtype=object),\n",
       " array([array([[ 0.06289794,  0.16810301,  0.24927178, ..., -0.00999364,\n",
       "                 0.13155247, -0.04708519],\n",
       "               [ 0.26072046,  0.21060373,  0.31554034, ...,  0.5287292 ,\n",
       "                 0.03870532,  0.0640774 ],\n",
       "               [ 0.05034144,  0.15775688,  0.31891915, ..., -0.06230282,\n",
       "                -0.00098   , -0.12756318],\n",
       "               ...,\n",
       "               [ 0.3108028 ,  0.3539401 ,  0.39000013, ...,  0.0597372 ,\n",
       "                 0.06335331, -0.11646673],\n",
       "               [-0.01256117, -0.01853747,  0.2075505 , ...,  0.22070354,\n",
       "                -0.03902072, -0.25085926],\n",
       "               [ 0.10657807, -0.24377295,  0.12579648, ..., -0.00235655,\n",
       "                 0.0726902 , -0.29154205]], dtype=float32)              ,\n",
       "        array([[ 0.15501052,  0.22199044,  0.0824762 , ...,  0.32071462,\n",
       "                 0.1586727 ,  0.148074  ],\n",
       "               [ 0.06289794,  0.16810301,  0.24927178, ..., -0.00999364,\n",
       "                 0.13155247, -0.04708519],\n",
       "               [ 0.15324111, -0.04315345,  0.25885698, ...,  0.21971071,\n",
       "                 0.19975139, -0.06391267],\n",
       "               ...,\n",
       "               [ 0.01063747, -0.34907222,  0.1898881 , ..., -0.22987828,\n",
       "                -0.07457357, -0.09161704],\n",
       "               [-0.14130774,  0.00797169,  0.0788248 , ...,  0.49475792,\n",
       "                 0.06124166, -0.06688146],\n",
       "               [ 0.10657807, -0.24377295,  0.12579648, ..., -0.00235655,\n",
       "                 0.0726902 , -0.29154205]], dtype=float32)              ],\n",
       "       dtype=object),\n",
       " array([array([[ 0.12494677, -0.06487065,  0.34459582, ...,  0.4137969 ,\n",
       "                -0.10994513, -0.1879673 ],\n",
       "               [ 0.22762667,  0.35453495,  0.07807913, ...,  0.45609927,\n",
       "                 0.24221691, -0.03594416],\n",
       "               [ 0.2607205 ,  0.21060367,  0.31554034, ...,  0.52872914,\n",
       "                 0.03870526,  0.06407741],\n",
       "               ...,\n",
       "               [ 0.2294501 ,  0.27590644,  0.05957201, ...,  0.37793612,\n",
       "                 0.19666296,  0.0531553 ],\n",
       "               [ 0.02968463,  0.36295626,  0.24918218, ...,  0.2807647 ,\n",
       "                 0.13998675, -0.4290515 ],\n",
       "               [ 0.09457495,  0.1743297 ,  0.29215902, ...,  0.11118124,\n",
       "                -0.10513575, -0.09703773]], dtype=float32)              ,\n",
       "        array([[ 0.11019174, -0.06926674,  0.07123921, ..., -0.37569824,\n",
       "                -0.11274797, -0.07641514],\n",
       "               [ 0.1871774 , -0.21586697,  0.11445872, ...,  0.39171562,\n",
       "                 0.25419825, -0.02211325],\n",
       "               [ 0.22762667,  0.35453495,  0.07807913, ...,  0.45609927,\n",
       "                 0.24221691, -0.03594416],\n",
       "               ...,\n",
       "               [-0.08237434, -0.0574908 ,  0.28749397, ...,  0.1571706 ,\n",
       "                -0.08788764,  0.18701327],\n",
       "               [ 0.10932764, -0.1387388 ,  0.18556342, ..., -0.20875493,\n",
       "                 0.2252535 ,  0.12643564],\n",
       "               [ 0.09457495,  0.1743297 ,  0.29215902, ...,  0.11118124,\n",
       "                -0.10513575, -0.09703773]], dtype=float32)              ],\n",
       "       dtype=object),\n",
       " array([array([[ 0.17691734, -0.13404377,  0.32201582, ..., -0.16696876,\n",
       "                -0.07588392, -0.20781311],\n",
       "               [ 0.04703483,  0.03474853,  0.13236482, ..., -0.08233303,\n",
       "                 0.2382588 ,  0.18813154],\n",
       "               [-0.20802684,  0.07191694,  0.24226753, ...,  0.1694262 ,\n",
       "                 0.05435101, -0.15002997],\n",
       "               ...,\n",
       "               [-0.05186923, -0.07526501,  0.10628995, ...,  0.03192596,\n",
       "                 0.22426935,  0.2006712 ],\n",
       "               [-0.19252974,  0.28231004,  0.1886513 , ...,  0.14412811,\n",
       "                 0.3806041 ,  0.06995308],\n",
       "               [-0.1145609 ,  0.1711986 ,  0.1614401 , ...,  0.04878109,\n",
       "                 0.04657641, -0.00170743]], dtype=float32)              ,\n",
       "        array([[ 0.0470349 ,  0.03474847,  0.13236482, ..., -0.08233288,\n",
       "                 0.2382588 ,  0.18813154],\n",
       "               [-0.20802686,  0.07191689,  0.24226746, ...,  0.16942623,\n",
       "                 0.05435104, -0.15002999],\n",
       "               [-0.10869329,  0.04386715,  0.13460326, ..., -0.13170898,\n",
       "                -0.01499809, -0.00200692],\n",
       "               ...,\n",
       "               [-0.13499568,  0.22475545,  0.20146771, ...,  0.2507157 ,\n",
       "                 0.30048442,  0.02347718],\n",
       "               [-0.252866  , -0.12641303,  0.21948951, ...,  0.26235393,\n",
       "                -0.00737822, -0.03923238],\n",
       "               [-0.11456104,  0.1711985 ,  0.16144001, ...,  0.04878111,\n",
       "                 0.04657651, -0.00170737]], dtype=float32)              ],\n",
       "       dtype=object),\n",
       " array([array([[ 0.14880577,  0.16444726,  0.29514298, ...,  0.22653063,\n",
       "                 0.06194551,  0.102757  ],\n",
       "               [ 0.06184252,  0.09289128,  0.6046517 , ...,  0.33876508,\n",
       "                -0.2572823 , -0.07468893],\n",
       "               [ 0.10866868, -0.09114345,  0.44809708, ...,  0.5537851 ,\n",
       "                -0.02942719, -0.22894745],\n",
       "               ...,\n",
       "               [-0.01256117, -0.01853747,  0.2075505 , ...,  0.22070354,\n",
       "                -0.03902072, -0.25085926],\n",
       "               [ 0.15276754,  0.14968207, -0.0778823 , ...,  0.6583154 ,\n",
       "                 0.35465336, -0.18843909],\n",
       "               [ 0.1574257 , -0.1215321 ,  0.13140586, ...,  0.17751323,\n",
       "                -0.05004001,  0.02316755]], dtype=float32)              ,\n",
       "        array([[ 0.03697864, -0.09158345,  0.3394034 , ...,  0.26527   ,\n",
       "                -0.2126797 , -0.19306152],\n",
       "               [ 0.14880577,  0.16444723,  0.29514295, ...,  0.22653055,\n",
       "                 0.06194555,  0.1027569 ],\n",
       "               [ 0.06184252,  0.09289128,  0.6046517 , ...,  0.33876508,\n",
       "                -0.2572823 , -0.07468893],\n",
       "               ...,\n",
       "               [-0.06845828,  0.01649418,  0.21722412, ...,  0.38289833,\n",
       "                 0.19218785,  0.03532714],\n",
       "               [ 0.15276754,  0.14968207, -0.0778823 , ...,  0.6583154 ,\n",
       "                 0.35465336, -0.18843909],\n",
       "               [ 0.1574257 , -0.1215321 ,  0.13140586, ...,  0.17751323,\n",
       "                -0.05004001,  0.02316755]], dtype=float32)              ],\n",
       "       dtype=object)]"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "students_candidates_emb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# output essay 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_candidates = list(map(lambda ans: key_words.candidates_tokens(ans,n_gram_range=(1,2)),ess_2_model_answers))\n",
    "model_candidate_emb = list(map(lambda cand:BERT.model.encode(cand),model_candidates))\n",
    "keywords = list(map(lambda x: maximal_marginal_relevance(\n",
    "    x[0].reshape(1, -1),x[1],x[2]),zip(emb_dict['ess_2_model_answers_emb'],\n",
    "    model_candidate_emb,model_candidates)))\n",
    "students_n_grams = list(map(lambda keyword: sorted(key_words.get_n_grams(keyword)),keywords))\n",
    "docs = df.query('EssaySet == 2')[\"EssayText\"].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['improve',\n",
       " 'used experiment',\n",
       " 'student',\n",
       " 'data names',\n",
       " 'experiment plastic',\n",
       " 'better new',\n",
       " 'conclusion plastic',\n",
       " 'type plastic',\n",
       " 'student experiment',\n",
       " 'types',\n",
       " 'held stronger',\n",
       " 'plastic length',\n",
       " 'names',\n",
       " 'data',\n",
       " 'stronger different',\n",
       " 'length conclusion',\n",
       " 'experiment better',\n",
       " 'grafin',\n",
       " 'type grafin',\n",
       " 'names type',\n",
       " 'new data',\n",
       " 'better',\n",
       " 'experiment',\n",
       " 'plastic type',\n",
       " 'grafin improve',\n",
       " 'new',\n",
       " 'held',\n",
       " 'conclusion',\n",
       " 'changing type',\n",
       " 'type used',\n",
       " 'used',\n",
       " 'length',\n",
       " 'type held',\n",
       " 'different',\n",
       " 'improve student',\n",
       " 'type',\n",
       " 'changing',\n",
       " 'different types',\n",
       " 'plastic',\n",
       " 'stronger']"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key_words.candidates_tokens(str(docs[0]), n_gram_range=(1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['improve',\n",
       "  'used experiment',\n",
       "  'student',\n",
       "  'data names',\n",
       "  'experiment plastic',\n",
       "  'better new',\n",
       "  'conclusion plastic',\n",
       "  'type plastic',\n",
       "  'student experiment',\n",
       "  'types',\n",
       "  'held stronger',\n",
       "  'plastic length',\n",
       "  'names',\n",
       "  'data',\n",
       "  'stronger different',\n",
       "  'length conclusion',\n",
       "  'experiment better',\n",
       "  'grafin',\n",
       "  'type grafin',\n",
       "  'names type',\n",
       "  'new data',\n",
       "  'better',\n",
       "  'experiment',\n",
       "  'plastic type',\n",
       "  'grafin improve',\n",
       "  'new',\n",
       "  'held',\n",
       "  'conclusion',\n",
       "  'changing type',\n",
       "  'type used',\n",
       "  'used',\n",
       "  'length',\n",
       "  'type held',\n",
       "  'different',\n",
       "  'improve student',\n",
       "  'type',\n",
       "  'changing',\n",
       "  'different types',\n",
       "  'plastic',\n",
       "  'stronger']]"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_candidates([(1,2),], docs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['stretchability polymer',\n",
       "  'did',\n",
       "  'samples start',\n",
       "  'measurements length',\n",
       "  'plastic sample'],\n",
       " ['sample stretchability',\n",
       "  'polymer plastics',\n",
       "  'did',\n",
       "  'samples start',\n",
       "  'compared']]"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[ans for ind, ans in enumerate(keywords[:2])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0\n",
      "['stretchability polymer', 'did', 'samples start', 'measurements length', 'plastic sample']\n",
      "iter 1\n",
      "['sample stretchability', 'polymer plastics', 'did', 'samples start', 'compared']\n"
     ]
    }
   ],
   "source": [
    "keywords_embeddings_lis = []\n",
    "for ind, ans in enumerate(keywords[:2]):\n",
    "    print(\"iter\",ind)\n",
    "    keywords_embeddings =  np.array(list(map(BERT.model.encode, ans)))\n",
    "    print(ans)\n",
    "    keywords_embeddings_lis.append(keywords_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 5, 768)"
      ]
     },
     "execution_count": 336,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_keywords([keywords[0]]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 347,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(keywords_embeddings_lis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 768)"
      ]
     },
     "execution_count": 348,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keywords_embeddings_lis[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_candidates(n_grams, doc):\n",
    "    x = list(map(lambda gram :\n",
    "        key_words.candidates_tokens(str(doc), n_gram_range=gram)\n",
    "        , n_grams))\n",
    "    # print shapes\n",
    "    # print([len(i) for i in x])\n",
    "    return x\n",
    "\n",
    "def keywords_pipeline_output(keywords,docs,n_gram_list,batch = 50):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        keywords: list of list of keywords\n",
    "        docs: list of list of tokens\n",
    "        batch: batch size\n",
    "    Returns:\n",
    "        a dict of each\n",
    "            n_gram_dict : dict of dict of list of list of tokens\n",
    "            n_grams_list : list of list of n_grams\n",
    "            students_candidates_list : list of list of list of tokens\n",
    "            keywords_embeddings_list  : list of list of list of embeddings\n",
    "            students_candidates_emb_list : list of list of list of embeddings\n",
    "            times : list of times taken for each model answer\n",
    "    \"\"\"\n",
    "\n",
    "    def check_n_gram_dict(n_grams, doc, ind):\n",
    "        res = []\n",
    "        for n_gram in n_grams:\n",
    "            x = n_gram_dict[ind].get(n_gram)\n",
    "            if x is None:\n",
    "                # x =  key_words.get_candidates(n_gram, doc)\n",
    "                x =  get_candidates([n_gram], doc)\n",
    "                # print(\"in\",x)\n",
    "                n_gram_dict[ind][n_gram] = x\n",
    "                res.append(x)\n",
    "            else:\n",
    "                res.append(x)\n",
    "        # print(\"out\",res)\n",
    "        return res\n",
    "\n",
    "\n",
    "    def new_check_n_gram_dict(n_grams, doc, ind):\n",
    "        # res = []\n",
    "        # emb = []\n",
    "        # for n_gram in n_grams:\n",
    "        #     x = n_gram_dict[ind].get(n_gram)\n",
    "        #     if x is None:\n",
    "        #         # x =  key_words.get_candidates(n_gram, doc)\n",
    "        #         x =  get_candidates([n_gram], doc)\n",
    "        #         n_gram_dict[ind][n_gram] = x\n",
    "        #         # cand_emb_dict[ind][n_gram] = list(map(BERT.model.encode, x))\n",
    "        #         cand_emb_dict[ind][n_gram] = emb_keywords(x)\n",
    "        #         # emb.append(cand_emb_dict[ind][n_gram])\n",
    "        #         res.append(x)\n",
    "        #     else:\n",
    "        #         res.append(x)\n",
    "        # return res\n",
    "        def fn(n_gram):\n",
    "            x = n_gram_dict[ind].get(n_gram)\n",
    "            if x is None:\n",
    "                # print(\"hi\",ind)\n",
    "                x =  get_candidates([n_gram], doc)\n",
    "                n_gram_dict[ind][n_gram] = x\n",
    "                cand_emb_dict[ind][n_gram] = emb_keywords(x)\n",
    "                return x\n",
    "            else:\n",
    "                # print(\"f\",ind)\n",
    "                return x\n",
    "        return list(map(fn, n_grams))\n",
    "        # do it in parallel\n",
    "        # return list(map(lambda n_gram:\n",
    "        #     n_gram_dict[ind].get(n_gram)\n",
    "        #     if n_gram_dict[ind].get(n_gram) is None\n",
    "        #     else n_gram_dict[ind][n_gram], n_grams))\n",
    "\n",
    "\n",
    "    def check_candidates_dict(n_grams, doc, ind):\n",
    "        # res = []\n",
    "        # for n_gram in n_grams:\n",
    "        #     x = cand_emb_dict[ind].get(n_gram)\n",
    "        #     if x is None:\n",
    "        #         # x =  key_words.get_candidates(n_gram, doc)\n",
    "        #         x =  get_candidates([n_gram], doc)\n",
    "        #         cand_emb_dict[ind][n_gram] = x\n",
    "        #         res.append(x)\n",
    "        #     else:\n",
    "        #         res.append(x)\n",
    "        # return res\n",
    "        # do it in parallel\n",
    "        return list(map(lambda n_gram:\n",
    "            cand_emb_dict[ind].get(n_gram)\n",
    "            if cand_emb_dict[ind].get(n_gram) is None\n",
    "            else cand_emb_dict[ind][n_gram],\n",
    "            n_grams))\n",
    "\n",
    "\n",
    "\n",
    "    n_docs = len(docs)\n",
    "    if n_docs < batch:\n",
    "        batch = n_docs\n",
    "    \n",
    "    n_gram_dict = {ind: {n_gram: None for n_gram in set([item for sublist in n_gram_list for item in sublist])} for ind ,doc in enumerate(docs)}\n",
    "    cand_emb_dict = {ind: {n_gram: None for n_gram in set([item for sublist in n_gram_list for item in sublist])} for ind ,doc in enumerate(docs)}\n",
    "\n",
    "    n_grams_list = []\n",
    "    students_candidates_list = []\n",
    "    keywords_embeddings_list = []\n",
    "    students_candidates_emb_list = []\n",
    "    times = []\n",
    "    hh_list = []\n",
    "\n",
    "    # all model answers\n",
    "    for ind, ans in enumerate(keywords):\n",
    "        print(\"model answer\",ind)\n",
    "        t1 = time.perf_counter()\n",
    "        students_n_grams = key_words.get_n_grams(ans)\n",
    "        keywords_embeddings =  list(map(BERT.model.encode, ans))\n",
    "\n",
    "        students_candidates_list_s = []\n",
    "        students_candidates_emb_list_s = []\n",
    "        # hh_list_s = []\n",
    "\n",
    "        # do in batches\n",
    "        for i in range(0,n_docs,batch):\n",
    "            # students_candidates = [0]\n",
    "            # students_candidates = list(map(lambda doc:\n",
    "            #                 check_n_gram_dict(students_n_grams, doc[1], i+doc[0]),\n",
    "            #                 enumerate(docs[i:i+batch])))\n",
    "            # print(\"students_candidates\",students_candidates[0])\n",
    "            # students_candidates_emb = []\n",
    "            # for j,k in enumerate(students_candidates):\n",
    "            #     print(j,k)\n",
    "            #     students_candidates_emb.extend(emb_keywords(k))\n",
    "            # print(\"students_candidates\",len(students_candidates))\n",
    "            # students_candidates_emb =  []\n",
    "            students_candidates = list(map(lambda doc:\n",
    "                            new_check_n_gram_dict(students_n_grams, doc[1], i+doc[0]),\n",
    "                            enumerate(docs[i:i+batch])))\n",
    "            students_candidates_emb = list(map(lambda doc:\n",
    "                            check_candidates_dict(students_n_grams, doc[1], i+doc[0]),\n",
    "                            enumerate(docs[i:i+batch])))\n",
    "            # students_candidates_emb =  list(map(lambda st: list(map( emb_keywords, st)),students_candidates))\n",
    "            # students_candidates_emb =  list(map(lambda doc:\n",
    "            #                 check_n_gram_dict(students_n_grams, doc[1], i+doc[0]),\n",
    "            #                 enumerate(docs[i:i+batch])))\n",
    "            # hh = [x.shape for x in students_candidates_emb]\n",
    "            # students_candidates_emb =  list(map(fn, students_candidates))\n",
    "            # hh =  list(map(old_fn, students_candidates))\n",
    "            # list(map(lambda doc:\n",
    "            #                 check_candidates_dict(students_n_grams, doc[1], i+doc[0]),\n",
    "            #                 enumerate(students_candidates)))\n",
    "\n",
    "            students_candidates_list_s.extend(students_candidates)\n",
    "            # hh_list_s.extend(hh)\n",
    "            students_candidates_emb_list_s.extend(students_candidates_emb)\n",
    "\n",
    "        if n_docs % batch != 0 and n_docs > batch:\n",
    "            students_candidates = list(map(lambda doc:\n",
    "                            check_n_gram_dict(students_n_grams, doc[1], i+doc[0]),\n",
    "                            enumerate(docs[i+batch:])))\n",
    "            students_candidates_emb =  list(map(emb_keywords, students_candidates))\n",
    "\n",
    "            students_candidates_list_s.extend(students_candidates)\n",
    "            students_candidates_emb_list_s.extend(students_candidates_emb)\n",
    "\n",
    "        n_grams_list.append(students_n_grams)\n",
    "        keywords_embeddings_list.append(keywords_embeddings)\n",
    "\n",
    "        students_candidates_list.append(students_candidates_list_s)\n",
    "        students_candidates_emb_list.append(students_candidates_emb_list_s)\n",
    "        # hh_list.append(hh_list_s)\n",
    "        times.append(time.perf_counter() - t1)\n",
    "    # retun a dict of each n_grams_list, students_candidates_list, keywords_embeddings_list, students_candidates_emb_list\n",
    "    return {\n",
    "        \"n_gram_dict\": n_gram_dict,\n",
    "        \"n_grams_list\": n_grams_list,\n",
    "        # \"cand_emb_dict\": cand_emb_dict,\n",
    "        # \"hh_list\": hh_list,\n",
    "        \"students_candidates_list\": students_candidates_list,\n",
    "        \"keywords_embeddings_list\": keywords_embeddings_list,\n",
    "        \"students_candidates_emb_list\": students_candidates_emb_list,\n",
    "        \"times\": times\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_keywords(\n",
    "    keywords_emb:list[np.ndarray],\n",
    "    candidates_emb : list[np.ndarray],\n",
    "    thershold: float)\\\n",
    "        -> float:\n",
    "    \"\"\"\n",
    "    match keywords with candidates in a document\n",
    "\n",
    "    Args:\n",
    "        keywords_emb (List[np.ndarray]): list of keywords embeddings\n",
    "        candidates_emb (List[np.ndarray]): list of document's candidates embeddings\n",
    "        thershold (float): threshold\n",
    "\n",
    "    Returns:\n",
    "        float: score\n",
    "    \n",
    "    example:\n",
    "        >>> match_keywords(keywords_emb, candidates_emb, thershold=0.5)\n",
    "        >>> 0.8\n",
    "    \"\"\"\n",
    "    # candidates_emb_shapes = list(map(lambda emb: emb.shape, candidates_emb))\n",
    "    # keys_emb_shapes = list(map(lambda emb: emb.reshape(1, -1).shape, keywords_emb))\n",
    "    # check if candidates_emb shape is 1d\n",
    "    # def shape_check(emb):\n",
    "    #     if emb.size == 1:\n",
    "    #         return emb.reshape(-1, 1)\n",
    "    #     else:\n",
    "    #         return emb\n",
    "    \n",
    "    # candidates_emb = list(map(shape_check, candidates_emb))\n",
    "    # keywords_emb = list(map(shape_check, keywords_emb))\n",
    "\n",
    "    # combination = list(zip(keywords_emb,candidates_emb))\n",
    "    # print(\"checked\")\n",
    "\n",
    "    # similarities = list(map(lambda comb:\n",
    "    #                 np.array(cos_sim(comb[0],\n",
    "    #                 comb[1])).clip(-1, 1).round(6),\n",
    "    #                 combination))\n",
    "\n",
    "\n",
    "    # similarities = list(map(lambda comb:\n",
    "    #                 print(comb[0].shape,comb[1].shape),\n",
    "    #                 combination))\n",
    "\n",
    "    # similarities = list(map(lambda cand:\n",
    "    #                 print(np.array(keywords_emb).shape, cand.reshape(cand.shape[1],cand.shape[2]).shape),\n",
    "    #                 candidates_emb))\n",
    "    # .__array__().max()\n",
    "\n",
    "    # similarities = list(map(lambda cand:\n",
    "    #                 print(np.array(keywords_emb).shape, cand.shape),\n",
    "    #                 candidates_emb))\n",
    "\n",
    "    similarities = list(map(lambda cand:\n",
    "                    cos_sim(np.array(keywords_emb), cand.reshape(cand.shape[0],cand.shape[1])).__array__().max(axis=1).round(6).clip(-1, 1),\n",
    "                    candidates_emb))\n",
    "    # print shape of similarities\n",
    "    # print(np.array(similarities).shape)\n",
    "\n",
    "\n",
    "\n",
    "    # similarities = list(map(lambda cand:\n",
    "    #                 np.array(list(map(lambda key:\n",
    "    #                 cos_sim(key, cand.reshape(cand.shape[1],cand.shape[2])).clip(-1, 1).round(6),keywords_emb))),\n",
    "    #                 # print(key.shape, cand.reshape(cand.shape[1],cand.shape[2]).shape),keywords_emb))),\n",
    "    #                 candidates_emb))\n",
    "\n",
    "    # def fn_ (x: np.array):\n",
    "    #     \"\"\"\n",
    "    #     return the no. of matched keywords\n",
    "    #     \"\"\"\n",
    "    #     if not np.sum(x>= thershold):\n",
    "    #         return 0\n",
    "    #     if np.sum(x >= thershold) > 1.0:\n",
    "    #         return 1.0\n",
    "    #     return np.sum(x >= thershold)\n",
    "\n",
    "    # res = sum(map(fn_, similarities))\n",
    "    # return res/len(keywords_emb)\n",
    "    # print(type(combination[0][1]))\n",
    "\n",
    "    # similarities = list(map(lambda comb:\n",
    "    #                 list(map(lambda comb_:\n",
    "    #                 print(type(comb[0]),type(comb_)),comb[1])),\n",
    "    #                 combination))\n",
    "\n",
    "    # similarities = list(map(lambda comb:\n",
    "    #                 print(type(comb[0]),type(comb[1])),\n",
    "    #                 combination))\n",
    "    # list(map(lambda sim: print((sim.__array__().shape)) , similarities))\n",
    "    return similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def para_keywords_pipeline_output(keywords,docs,n_gram_list,batch = 50):\n",
    "\n",
    "    def check_n_gram_dict(n_grams, doc, ind):\n",
    "        res = []\n",
    "        for n_gram in n_grams:\n",
    "            x = n_gram_dict[ind].get(n_gram)\n",
    "            if x is None:\n",
    "                x =  get_candidates([n_gram], doc)\n",
    "                n_gram_dict[ind][n_gram] = x\n",
    "                res.append(x)\n",
    "            else:\n",
    "                res.append(x)\n",
    "        return res\n",
    "\n",
    "\n",
    "    def new_check_n_gram_dict(n_grams, doc, ind):\n",
    "        def fn(n_gram):\n",
    "            x = n_gram_dict[ind].get(n_gram)\n",
    "            if x is None:\n",
    "                # print(\"hi\",ind)\n",
    "                x =  get_candidates([n_gram], doc)\n",
    "                n_gram_dict[ind][n_gram] = x\n",
    "                # cand_emb_dict[ind][n_gram] = emb_keywords(x)\n",
    "                return x\n",
    "            else:\n",
    "                # print(\"f\",ind)\n",
    "                return x\n",
    "        return list(map(fn, n_grams))\n",
    "\n",
    "\n",
    "    def check_candidates_dict(n_grams, doc, ind):\n",
    "        def fn(n_gram):\n",
    "            x = cand_emb_dict[ind].get(n_gram)\n",
    "            if x is None:\n",
    "                cand_emb_dict[ind][n_gram] = emb_keywords(n_gram_dict[ind][n_gram])\n",
    "                return cand_emb_dict[ind][n_gram]\n",
    "            else:\n",
    "                return x\n",
    "        return list(map(fn,n_grams))\n",
    "\n",
    "    n_docs = len(docs)\n",
    "    if n_docs < batch:\n",
    "        batch = n_docs\n",
    "    \n",
    "    n_gram_dict = {ind: {n_gram: None for n_gram in set([item for sublist in n_gram_list for item in sublist])} for ind ,doc in enumerate(docs)}\n",
    "    cand_emb_dict = {ind: {n_gram: None for n_gram in set([item for sublist in n_gram_list for item in sublist])} for ind ,doc in enumerate(docs)}\n",
    "\n",
    "    n_grams_list = []\n",
    "    students_candidates_list = []\n",
    "    keywords_embeddings_list = []\n",
    "    students_candidates_emb_list = []\n",
    "    times = []\n",
    "    hh_list = []\n",
    "\n",
    "    # all model answers\n",
    "    for ind, ans in enumerate(keywords):\n",
    "        print(\"model answer\",ind)\n",
    "        t1 = time.perf_counter()\n",
    "        students_n_grams = key_words.get_n_grams(ans)\n",
    "        keywords_embeddings =  list(map(BERT.model.encode, ans))\n",
    "\n",
    "        students_candidates_list_s = []\n",
    "        students_candidates_emb_list_s = []\n",
    "        # hh_list_s = []\n",
    "\n",
    "        # do in batches\n",
    "        for i in range(0,n_docs,batch):\n",
    "            students_candidates = [0]\n",
    "\n",
    "            students_candidates = list(map(lambda doc:\n",
    "                            new_check_n_gram_dict(students_n_grams, doc[1], i+doc[0]),\n",
    "                            enumerate(docs[i:i+batch])))\n",
    "            students_candidates_emb = list(map(lambda doc:\n",
    "                            check_candidates_dict(students_n_grams, doc[1], i+doc[0]),\n",
    "                            enumerate(docs[i:i+batch])))\n",
    "            students_candidates_list_s.extend(students_candidates)\n",
    "            # hh_list_s.extend(hh)\n",
    "            students_candidates_emb_list_s.extend(students_candidates_emb)\n",
    "\n",
    "        if n_docs % batch != 0 and n_docs > batch:\n",
    "            students_candidates = list(map(lambda doc:\n",
    "                            check_n_gram_dict(students_n_grams, doc[1], i+doc[0]),\n",
    "                            enumerate(docs[i+batch:])))\n",
    "            # students_candidates_emb = list(map(lambda doc:\n",
    "            #                 check_candidates_dict(students_n_grams, doc[1], i+doc[0]),\n",
    "            #                 enumerate(docs[i:i+batch])))\n",
    "            students_candidates_emb =  list(map(emb_keywords, students_candidates))\n",
    "\n",
    "            students_candidates_list_s.extend(students_candidates)\n",
    "            students_candidates_emb_list_s.extend(students_candidates_emb)\n",
    "\n",
    "        n_grams_list.append(students_n_grams)\n",
    "        keywords_embeddings_list.append(keywords_embeddings)\n",
    "\n",
    "        students_candidates_list.append(students_candidates_list_s)\n",
    "        students_candidates_emb_list.append(students_candidates_emb_list_s)\n",
    "        # hh_list.append(hh_list_s)\n",
    "        times.append(time.perf_counter() - t1)\n",
    "    # retun a dict of each n_grams_list, students_candidates_list, keywords_embeddings_list, students_candidates_emb_list\n",
    "    return {\n",
    "        \"n_gram_dict\": n_gram_dict,\n",
    "        \"n_grams_list\": n_grams_list,\n",
    "        \"students_candidates_list\": students_candidates_list,\n",
    "        \"keywords_embeddings_list\": keywords_embeddings_list,\n",
    "        \"students_candidates_emb_list\": students_candidates_emb_list,\n",
    "        \"times\": times\n",
    "    }\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def para_keywords_pipeline_output_dict(keywords,docs,n_gram_list,batch = 50):\n",
    "\n",
    "    def check_n_gram_dict(n_grams, doc, ind):\n",
    "        def fn(n_gram):\n",
    "            x = n_gram_dict[ind].get(n_gram)\n",
    "            if x is None:\n",
    "                x =  get_candidates([n_gram], doc)\n",
    "                n_gram_dict[ind][n_gram] = x\n",
    "                return x\n",
    "            else:\n",
    "                return x\n",
    "        return list(map(fn, n_grams))\n",
    "\n",
    "    n_docs = len(docs)\n",
    "    if n_docs < batch:\n",
    "        batch = n_docs\n",
    "    \n",
    "    n_gram_dict = {ind: {n_gram: None for n_gram in set([item for sublist in n_gram_list for item in sublist])} for ind ,doc in enumerate(docs)}\n",
    "\n",
    "    n_grams_list = []\n",
    "    students_candidates_list = []\n",
    "    keywords_embeddings_list = []\n",
    "    students_candidates_emb_list = []\n",
    "    times = []\n",
    "\n",
    "    # all model answers\n",
    "    for ind, ans in enumerate(keywords):\n",
    "        print(\"model answer\",ind)\n",
    "        t1 = time.perf_counter()\n",
    "        students_n_grams = key_words.get_n_grams(ans)\n",
    "        keywords_embeddings =  list(map(BERT.model.encode, ans))\n",
    "\n",
    "        students_candidates_list_s = []\n",
    "        students_candidates_emb_list_s = []\n",
    "        # do in batches\n",
    "        for i in range(0,n_docs,batch):\n",
    "            students_candidates = list(map(lambda doc:\n",
    "                            check_n_gram_dict(students_n_grams, doc[1], i+doc[0]),\n",
    "                            enumerate(docs[i:i+batch])))\n",
    "            students_candidates_emb =  list(map( lambda st: list(map( emb_keywords ,st)), students_candidates))\n",
    "            # students_candidates_emb =  list(map(emb_d, students_candidates))\n",
    "            students_candidates_list_s.extend(students_candidates)\n",
    "            students_candidates_emb_list_s.extend(students_candidates_emb)\n",
    "\n",
    "        if n_docs % batch != 0 and n_docs > batch:\n",
    "            students_candidates = list(map(lambda doc:\n",
    "                            check_n_gram_dict(students_n_grams, doc[1], i+doc[0]),\n",
    "                            enumerate(docs[i+batch:])))\n",
    "            students_candidates_emb =  list(map( lambda st: list(map( emb_keywords ,st)), students_candidates))\n",
    "\n",
    "            students_candidates_list_s.extend(students_candidates)\n",
    "            students_candidates_emb_list_s.extend(students_candidates_emb)\n",
    "\n",
    "        n_grams_list.append(students_n_grams)\n",
    "        keywords_embeddings_list.append(keywords_embeddings)\n",
    "        students_candidates_list.append(students_candidates_list_s)\n",
    "        students_candidates_emb_list.append(students_candidates_emb_list_s)\n",
    "        times.append(time.perf_counter() - t1)\n",
    "    return {\n",
    "        \"n_gram_dict\": n_gram_dict,\n",
    "        \"n_grams_list\": n_grams_list,\n",
    "        \"students_candidates_list\": students_candidates_list,\n",
    "        \"keywords_embeddings_list\": keywords_embeddings_list,\n",
    "        \"students_candidates_emb_list\": students_candidates_emb_list,\n",
    "        \"times\": times\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def emb_(keywords):\n",
    "    # x = list(map(lambda st: list(map(lambda k:\n",
    "    #  list(map(lambda ans: len(ans),k)) ,st)) ,st_cands))\n",
    "    #! x = list(map(lambda k:list(map(lambda x: BERT.model.encode(str(x)) ,k)) ,keywords))\n",
    "    x = list(map(lambda k:list(map(lambda x: get_words_emb(str(x)) ,k)) ,keywords))\n",
    "    # x = np.array(list(map(lambda k:\n",
    "    #  np.array(list(map(lambda x: BERT.model.encode(str(x)), k))),st_cands)))\n",
    "    # if x.ndim == 3:\n",
    "    #     x = x.reshape(max(x.shape[1],x.shape[0]),x.shape[2])\n",
    "    # print(\"emb\",x.shape)\n",
    "    return x\n",
    "    # return list(map(lambda k: np.array(list(map(lambda x: BERT.model.encode(str(x)), k))),keywords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normal_keywords_pipeline_output(keywords,docs,n_gram_list,batch = 50):\n",
    "    # if not isinstance(docs, list):\n",
    "    #     docs = [docs]\n",
    "\n",
    "    n_docs = len(docs)\n",
    "    if n_docs < batch:\n",
    "        batch = n_docs\n",
    "    keywords_embeddings_list = []\n",
    "    students_candidates_emb_list = []\n",
    "\n",
    "    students_n_grams = key_words.get_n_grams(keywords)\n",
    "    keywords_embeddings =  list(map(BERT.model.encode, keywords))\n",
    "    #! keywords_embeddings =  list(map(BERT.model.encode, [keywords]))\n",
    "    # print(\"keywords_embeddings\",len(keywords_embeddings))\n",
    "    # print(\"keywords_embeddings\",keywords_embeddings[0].shape)\n",
    "\n",
    "    students_candidates_emb_list_s = []\n",
    "    # do in batches\n",
    "    print(\"students n grams\",students_n_grams)\n",
    "    for i in range(0,n_docs,batch):\n",
    "        students_candidates = list(map(lambda doc: get_candidates(students_n_grams, doc),docs[i:i+batch]))\n",
    "        # print  lens of students_candidates\n",
    "        print(\"students_candidates\",[[len(y) for y in st ]for st in students_candidates])\n",
    "        # students_candidates = list(map(lambda doc:\n",
    "        #                 new_check_n_gram_dict(students_n_grams, doc[1], i+doc[0]),\n",
    "        #                 enumerate(docs[i:i+batch])))\n",
    "\n",
    "        # students_candidates_emb = emb_keywords(n_gram_dict[ind][n_gram])\n",
    "        # list(map(lambda n_grams:list(map(lambda doc:\n",
    "        #     key_words.get_candidates(n_grams, doc),docs[i:i+batch])),students_n_grams))\n",
    "\n",
    "        # students_candidates_emb =  list(map(BERT.model.encode, students_candidates))\n",
    "        # students_candidates_emb =  list(map(lambda st: list(map( lambda x: BERT.model.encode(str(x)), st)),students_candidates))\n",
    "        # students_candidates_emb = list(map(lambda st: list(map( lambda x: len(x), st)),students_candidates))\n",
    "        # print(list(map(lambda st: list(map( lambda x: len(x), st)),students_candidates)))\n",
    "        # print(list(map(lambda st: emb_keywords(st).shape,students_candidates)))\n",
    "        students_candidates_emb =  list(map( emb_, students_candidates))\n",
    "        # print(emb_keywords(students_candidates).shape)\n",
    "        # students_candidates_emb = list(map(lambda doc:\n",
    "        #                 check_candidates_dict(students_n_grams, doc[1], i+doc[0]),\n",
    "        #                 enumerate(docs[i:i+batch])))\n",
    "        students_candidates_emb_list_s.extend(students_candidates_emb)\n",
    "\n",
    "    if n_docs % batch != 0 and n_docs > batch:\n",
    "        students_candidates = list(map(lambda doc:\n",
    "            get_candidates(students_n_grams, doc),docs[i:i+batch]))\n",
    "        # students_candidates_emb =  list(map( lambda st: list(map(emb_keywords, st)),students_candidates))\n",
    "        students_candidates_emb =  list(map(lambda st: list(map( lambda x: BERT.model.encode(str(x)), st)),students_candidates))\n",
    "        students_candidates_emb_list_s.extend(students_candidates_emb)\n",
    "\n",
    "    keywords_embeddings_list.append(keywords_embeddings)\n",
    "    students_candidates_emb_list.append(students_candidates_emb_list_s)\n",
    "    return {\n",
    "        \"keywords_embeddings_list\": keywords_embeddings_list,\n",
    "        \"students_candidates_emb_list\": students_candidates_emb_list,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def emb_keywords(keywords):\n",
    "    # x = np.array(list(map(lambda k: np.array(list(map(lambda x: BERT.model.encode(str(x)), k))),keywords)))\n",
    "    x = np.array(list(map(lambda k: np.array(list(map(lambda x: get_words_emb(str(x)), k))),keywords)))\n",
    "    # print(\"emb be\",x.shape)\n",
    "\n",
    "    if x.ndim == 3:\n",
    "        x = x.reshape(max(x.shape[1],x.shape[0]),x.shape[2])\n",
    "    # print(\"emb\",x.shape)\n",
    "    return x\n",
    "    # return list(map(lambda k: np.array(list(map(lambda x: BERT.model.encode(str(x)), k))),keywords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_emb_dict = {}\n",
    "\n",
    "# def emb_keywords(keywords):\n",
    "#     # return np.array(list(map(lambda k: np.array(list(map(lambda x: BERT.model.encode(str(x)), k))),keywords)))\n",
    "#     return np.array(list(map(lambda k: np.array(list(map(lambda x: get_words_emb(str(x)), k))),keywords)))\n",
    "#     # return np.array(list(map(BERT.model.encode, keywords)))\n",
    "\n",
    "def get_words_emb(word):\n",
    "    if word in words_emb_dict:\n",
    "        return words_emb_dict[word]\n",
    "    else:\n",
    "        words_emb_dict[word] = BERT.model.encode(word)\n",
    "        return words_emb_dict[word]\n",
    "\n",
    "def emb_d(keywords):\n",
    "    return np.array(list(map(lambda k: np.array(list(map(lambda x: get_words_emb(str(x)) ,k))) ,keywords)))\n",
    "    # return list(map(lambda k:list(map(lambda x: get_words_emb(str(x)) ,k)) ,keywords))\n",
    "\n",
    "def normal_keywords_pipeline_output_with_dict(keywords,docs,n_gram_list,batch = 10):\n",
    "    # if not isinstance(docs, list):\n",
    "    #     docs = [docs]\n",
    "    words_emb_dict = {}\n",
    "\n",
    "    n_docs = len(docs)\n",
    "    if n_docs < batch:\n",
    "        batch = n_docs\n",
    "    keywords_embeddings_list = []\n",
    "    students_candidates_emb_list = []\n",
    "\n",
    "    students_n_grams = key_words.get_n_grams(keywords)\n",
    "    keywords_embeddings =  list(map(BERT.model.encode, keywords))\n",
    "    #! keywords_embeddings =  list(map(BERT.model.encode, [keywords]))\n",
    "    students_candidates_emb_list_s = []\n",
    "    # do in batches\n",
    "    # print(\"students n grams\",students_n_grams)\n",
    "    for i in range(0,n_docs,batch):\n",
    "        students_candidates = list(map(lambda doc: get_candidates(students_n_grams, doc),docs[i:i+batch]))\n",
    "        # print(\"students_candidates\",[[len(y) for y in st ]for st in students_candidates])\n",
    "        students_candidates_emb =  list(map( emb_d, students_candidates))\n",
    "        # students_candidates_emb =  list(map( emb_d, students_candidates))\n",
    "        students_candidates_emb_list_s.extend(students_candidates_emb)\n",
    "\n",
    "    if n_docs % batch != 0 and n_docs > batch:\n",
    "        students_candidates = list(map(lambda doc:\n",
    "            get_candidates(students_n_grams, doc),docs[i:i+batch]))\n",
    "        # students_candidates_emb =  list(map( lambda st: list(map(emb_keywords, st)),students_candidates))\n",
    "        # students_candidates_emb =  list(map(lambda st: list(map( lambda x: BERT.model.encode(str(x)), st)),students_candidates))\n",
    "        students_candidates_emb =  list(map( emb_d, students_candidates))\n",
    "        students_candidates_emb_list_s.extend(students_candidates_emb)\n",
    "\n",
    "    keywords_embeddings_list.append(keywords_embeddings)\n",
    "    students_candidates_emb_list.append(students_candidates_emb_list_s)\n",
    "    return {\n",
    "        \"keywords_embeddings_list\": keywords_embeddings_list,\n",
    "        \"students_candidates_emb_list\": students_candidates_emb_list,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'weights second way'"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(list(words_emb_dict.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grading(keywords_embeddings_list,students_candidates_emb_list,thershold=0.5):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        keywords_embeddings_list: list of list of list of embeddings\n",
    "        students_candidates_emb_list: list of list of list of embeddings\n",
    "        thershold: thershold for the similarity\n",
    "    Returns:\n",
    "        a list of list of list of grades\n",
    "    \"\"\"\n",
    "    grades = []\n",
    "    for i in range(len(keywords_embeddings_list)):\n",
    "        # for j in range(len(keywords_embeddings_list[i])):\n",
    "        #     grades.append(key_words.match_keywords(keywords_embeddings_list[i][j],\n",
    "        #                                     students_candidates_emb_list[i][j],\n",
    "        #                                     thershold))\n",
    "        \n",
    "        # map(lambda j: grades.append(key_words.match_keywords(keywords_embeddings_list[i][j],)))\n",
    "        grades.append(np.array(list(map(lambda st_cand:\n",
    "                match_keywords(keywords_embeddings_list[i], st_cand,\n",
    "                thershold=thershold),\n",
    "                students_candidates_emb_list[i]\n",
    "                ))))\n",
    "    grades = np.array(list(map(lambda sim: (sim.__array__().max(axis=1) >thershold).sum(axis=1)/float(sim.shape[-1]) , grades)))\n",
    "    return grades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random array of shape (22, 1, 768) (22, 1, 768) (1, 40, 768)\n",
    "random_array = np.random.rand(22, 1, 768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.77372219, 0.75673716, 0.78498689, 0.77073208, 0.76771135])"
      ]
     },
     "execution_count": 394,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos_sim(np.random.rand(5,768), np.random.rand(40, 768)).__array__().max(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 5)\n",
      "(5,)\n",
      "(5,)\n",
      "(2, 5)\n",
      "(5,)\n",
      "(5,)\n",
      "(2, 5)\n",
      "(5,)\n",
      "(5,)\n",
      "(2, 5)\n",
      "(5,)\n",
      "(5,)\n",
      "(2, 5)\n",
      "(5,)\n",
      "(5,)\n",
      "(2, 5)\n",
      "(5,)\n",
      "(5,)\n",
      "(2, 5)\n",
      "(5,)\n",
      "(5,)\n",
      "(2, 5)\n",
      "(5,)\n",
      "(5,)\n",
      "(2, 5)\n",
      "(5,)\n",
      "(5,)\n",
      "(2, 5)\n",
      "(5,)\n",
      "(5,)\n",
      "(2, 5)\n",
      "(5,)\n",
      "(5,)\n",
      "(2, 5)\n",
      "(5,)\n",
      "(5,)\n",
      "(2, 5)\n",
      "(5,)\n",
      "(5,)\n",
      "(2, 5)\n",
      "(5,)\n",
      "(5,)\n",
      "(2, 5)\n",
      "(5,)\n",
      "(5,)\n",
      "(2, 5)\n",
      "(5,)\n",
      "(5,)\n",
      "(2, 5)\n",
      "(5,)\n",
      "(5,)\n",
      "(2, 5)\n",
      "(5,)\n",
      "(5,)\n",
      "(2, 5)\n",
      "(5,)\n",
      "(5,)\n",
      "(2, 5)\n",
      "(5,)\n",
      "(5,)\n",
      "(2, 5)\n",
      "(5,)\n",
      "(5,)\n",
      "(2, 5)\n",
      "(5,)\n",
      "(5,)\n",
      "(2, 5)\n",
      "(5,)\n",
      "(5,)\n",
      "(2, 5)\n",
      "(5,)\n",
      "(5,)\n",
      "(2, 5)\n",
      "(5,)\n",
      "(5,)\n",
      "(2, 5)\n",
      "(5,)\n",
      "(5,)\n",
      "(2, 5)\n",
      "(5,)\n",
      "(5,)\n",
      "(2, 5)\n",
      "(5,)\n",
      "(5,)\n",
      "(2, 5)\n",
      "(5,)\n",
      "(5,)\n",
      "(2, 5)\n",
      "(5,)\n",
      "(5,)\n",
      "(2, 5)\n",
      "(5,)\n",
      "(5,)\n",
      "(2, 5)\n",
      "(5,)\n",
      "(5,)\n",
      "(2, 5)\n",
      "(5,)\n",
      "(5,)\n",
      "(2, 5)\n",
      "(5,)\n",
      "(5,)\n",
      "(2, 5)\n",
      "(5,)\n",
      "(5,)\n",
      "(2, 5)\n",
      "(5,)\n",
      "(5,)\n",
      "(2, 5)\n",
      "(5,)\n",
      "(5,)\n",
      "(2, 5)\n",
      "(5,)\n",
      "(5,)\n",
      "(2, 5)\n",
      "(5,)\n",
      "(5,)\n",
      "(2, 5)\n",
      "(5,)\n",
      "(5,)\n"
     ]
    }
   ],
   "source": [
    "g = grading(keywords_embeddings_lis,d['students_candidates_emb_list'],0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 20)"
      ]
     },
     "execution_count": 433,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_candidates = list(map(lambda ans: key_words.candidates_tokens(ans,n_gram_range=(1,2)),ess_2_model_answers))\n",
    "model_candidate_emb = list(map(lambda cand:BERT.model.encode(cand),model_candidates))\n",
    "keywords = list(map(lambda x: maximal_marginal_relevance(\n",
    "    x[0].reshape(1, -1),x[1],x[2]),zip(emb_dict['ess_2_model_answers_emb'],\n",
    "    model_candidate_emb,model_candidates)))\n",
    "students_n_grams = list(map(lambda keyword: sorted(key_words.get_n_grams(keyword)),keywords))\n",
    "docs = df.query('EssaySet == 2')[\"EssayText\"].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model answer 0\n",
      "emb be (1, 40, 768)\n",
      "emb (40, 768)\n",
      "emb be (1, 41, 768)\n",
      "emb (41, 768)\n",
      "emb be (1, 45, 768)\n",
      "emb (45, 768)\n",
      "emb be (1, 45, 768)\n",
      "emb (45, 768)\n",
      "emb be (1, 27, 768)\n",
      "emb (27, 768)\n",
      "emb be (1, 25, 768)\n",
      "emb (25, 768)\n",
      "emb be (1, 12, 768)\n",
      "emb (12, 768)\n",
      "emb be (1, 11, 768)\n",
      "emb (11, 768)\n",
      "model answer 1\n",
      "emb be (1, 40, 768)\n",
      "emb (40, 768)\n",
      "emb be (1, 41, 768)\n",
      "emb (41, 768)\n",
      "emb be (1, 45, 768)\n",
      "emb (45, 768)\n",
      "emb be (1, 45, 768)\n",
      "emb (45, 768)\n",
      "emb be (1, 27, 768)\n",
      "emb (27, 768)\n",
      "emb be (1, 25, 768)\n",
      "emb (25, 768)\n",
      "emb be (1, 12, 768)\n",
      "emb (12, 768)\n",
      "emb be (1, 11, 768)\n",
      "emb (11, 768)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.41940510000131326"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1 = time.perf_counter()\n",
    "dd = para_keywords_pipeline_output_dict(keywords[0:2],docs[:4],students_n_grams,batch=10)\n",
    "time.perf_counter() - t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 1.        , 1.        , 0.57142857],\n",
       "       [1.        , 1.        , 0.85714286, 0.42857143]])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grading(dd['keywords_embeddings_list'],dd['students_candidates_emb_list'],0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 759,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model answer 0\n",
      "[40]\n",
      "emb (40, 768)\n",
      "[44]\n",
      "emb (44, 768)\n",
      "[43]\n",
      "emb (43, 768)\n",
      "[45]\n",
      "emb (45, 768)\n",
      "[22]\n",
      "emb (22, 768)\n",
      "[21]\n",
      "emb (21, 768)\n",
      "[43]\n",
      "emb (43, 768)\n",
      "[44]\n",
      "emb (44, 768)\n"
     ]
    }
   ],
   "source": [
    "d = keywords_pipeline_output([keywords[0]],docs[:4],students_n_grams,batch=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 678,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model answer 0\n",
      "model answer 0\n",
      "model answer 0\n",
      "model answer 0\n",
      "model answer 0\n",
      "model answer 0\n",
      "model answer 0\n",
      "model answer 0\n",
      "10 s ± 233 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit keywords_pipeline_output([keywords[0]],docs[:5],students_n_grams,batch=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 679,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7.98857470000803]"
      ]
     },
     "execution_count": 679,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d['times']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 680,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model answer 0\n",
      "model answer 0\n",
      "model answer 0\n",
      "model answer 0\n",
      "model answer 0\n",
      "model answer 0\n",
      "model answer 0\n",
      "model answer 0\n",
      "9.75 s ± 47.9 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit para_keywords_pipeline_output([keywords[0]],docs[:5],students_n_grams,batch=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "students n grams [(1, 2), (2, 3)]\n",
      "students_candidates [[40, 44], [43, 45], [22, 21]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\oeb\\AppData\\Local\\Temp/ipykernel_15280/2889523460.py:4: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  x = np.array(list(map(lambda k:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5.166980099998909"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1 = time.perf_counter()\n",
    "dn = normal_keywords_pipeline_output(keywords[0],docs[:3],students_n_grams,batch=10)\n",
    "time.perf_counter() - t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.6, 0.8, 0.6]])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grading(dn['keywords_embeddings_list'],dn['students_candidates_emb_list'],0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\oeb\\AppData\\Local\\Temp/ipykernel_15916/2509446158.py:11: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return np.array(list(map(lambda k: np.array(list(map(lambda x: get_words_emb(str(x)) ,k))) ,keywords)))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.14998909999849275"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1 = time.perf_counter()\n",
    "dn = normal_keywords_pipeline_output_with_dict(keywords[0],docs[:3],students_n_grams,batch=10)\n",
    "time.perf_counter() - t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# {\"asndgv\":[1,2,3,3]}*2\n",
    "embdd = BERT.model.encode(\"asndgv\")\n",
    "dic = {}\n",
    "dic = {\"asndgv\"+str(int(k)) : embdd for k in list(np.arange(1,1000*100))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.000083923339844"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.getsizeof(dic) / 1024 / 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model answer 0\n",
      "model answer 1\n",
      "model answer 2\n",
      "model answer 3\n"
     ]
    }
   ],
   "source": [
    "d1 = para_keywords_pipeline_output(keywords[:4],docs[:4],students_n_grams,batch=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.12975770000048215,\n",
       " 0.12392139999974461,\n",
       " 0.12380529999973078,\n",
       " 0.12464680000084627]"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d1['times']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'keywords_embeddings_list'"
      ]
     },
     "execution_count": 540,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"keywords_embeddings_list\"\n",
    "# n models\n",
    "# [0] n of keywords ARRAY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'students_candidates_emb_list'"
      ]
     },
     "execution_count": 539,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"students_candidates_emb_list\"\n",
    "# n models\n",
    "# [0] n of students\n",
    "# [0][0] each student's candidates (40, 768) , (41, 768) ARRAY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arr 0 (2, 768)\n",
      "arr 1 (2, 768)\n",
      "arr 2 (2, 768)\n",
      "arr 3 (2, 768)\n"
     ]
    }
   ],
   "source": [
    "for i, v in enumerate(dd['students_candidates_emb_list'][0]):\n",
    "    try:\n",
    "        print(\"arr\",i, v.shape)\n",
    "    except:\n",
    "        print(i, len(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 30720 into shape (1,40)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_15280/2174989434.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mgrading\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'keywords_embeddings_list'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0md1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'students_candidates_emb_list'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_15280/4173432912.py\u001b[0m in \u001b[0;36mgrading\u001b[1;34m(keywords_embeddings_list, students_candidates_emb_list, thershold)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[1;31m# map(lambda j: grades.append(key_words.match_keywords(keywords_embeddings_list[i][j],)))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m         grades.append(np.array(list(map(lambda st_cand:\n\u001b[0m\u001b[0;32m     19\u001b[0m                 match_keywords(keywords_embeddings_list[i], st_cand,\n\u001b[0;32m     20\u001b[0m                 thershold=thershold),\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_15280/4173432912.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(st_cand)\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[1;31m# map(lambda j: grades.append(key_words.match_keywords(keywords_embeddings_list[i][j],)))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         grades.append(np.array(list(map(lambda st_cand:\n\u001b[1;32m---> 19\u001b[1;33m                 match_keywords(keywords_embeddings_list[i], st_cand,\n\u001b[0m\u001b[0;32m     20\u001b[0m                 thershold=thershold),\n\u001b[0;32m     21\u001b[0m                 \u001b[0mstudents_candidates_emb_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_15280/1947212871.py\u001b[0m in \u001b[0;36mmatch_keywords\u001b[1;34m(keywords_emb, candidates_emb, thershold)\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[1;31m#                 candidates_emb))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 55\u001b[1;33m     similarities = list(map(lambda cand:\n\u001b[0m\u001b[0;32m     56\u001b[0m                     \u001b[0mcos_sim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkeywords_emb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcand\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcand\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcand\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__array__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mround\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m                     candidates_emb))\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_15280/1947212871.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(cand)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m     similarities = list(map(lambda cand:\n\u001b[1;32m---> 56\u001b[1;33m                     \u001b[0mcos_sim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkeywords_emb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcand\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcand\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcand\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__array__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mround\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     57\u001b[0m                     candidates_emb))\n\u001b[0;32m     58\u001b[0m     \u001b[1;31m# print shape of similarities\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: cannot reshape array of size 30720 into shape (1,40)"
     ]
    }
   ],
   "source": [
    "grading(d1['keywords_embeddings_list'],d1['students_candidates_emb_list'],0.5).max(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn(keywords):\n",
    "    return list(map(lambda k: np.array(list(map(lambda x: np.array(list(map(lambda y:  BERT.model.encode(str(y)),x))), k))),keywords))\n",
    "    # return np.array(list(map(BERT.model.encode, keywords)))\n",
    "def fn_old(keywords):\n",
    "    return list(map(lambda k: np.array(list(map(lambda x: list(map(lambda y:  BERT.model.encode(str(y)),x)), k))),keywords))\n",
    "    # return[list(map(lambda x: list(map(lambda y: str(y)+\" 1\",x)), k)) for k in keywords]\n",
    "    # return np.array(list(map(BERT.model.encode, keywords)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[23.139581300005375, 22.66242659999989, 22.939434699997946, 23.50101580000046]"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d['times'] # it will be huge difference for bigger data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(2,\n",
       "  3): [['stretched minutes',\n",
       "   'type way student',\n",
       "   '12mm trial conclusion',\n",
       "   'minutes stretching 22mm',\n",
       "   'stretched minutes stretching',\n",
       "   'stretched type',\n",
       "   'type stretched 10mm',\n",
       "   'weight added trials',\n",
       "   'experiment tell type',\n",
       "   'stretching 22mm',\n",
       "   '10mm trial 12mm',\n",
       "   '23mm trial',\n",
       "   'student improve',\n",
       "   'tell weight added',\n",
       "   'type stretched type',\n",
       "   'polymer type',\n",
       "   'stretched 10mm trial',\n",
       "   'way student improve',\n",
       "   '12mm trial',\n",
       "   'added trials',\n",
       "   '23mm trial plastic',\n",
       "   'conclusion strongest',\n",
       "   'experiment tell',\n",
       "   'trial conclusion',\n",
       "   '10mm trial',\n",
       "   'type way',\n",
       "   'trial 12mm trial',\n",
       "   'improve experiment',\n",
       "   'type stretched',\n",
       "   'stretched 10mm',\n",
       "   'trial plastic type',\n",
       "   'way student',\n",
       "   'trial 23mm trial',\n",
       "   'polymer type way',\n",
       "   'conclusion strongest polymer',\n",
       "   'plastic type',\n",
       "   'strongest polymer type',\n",
       "   'student improve experiment',\n",
       "   'stretching 22mm trial',\n",
       "   'tell type plastics',\n",
       "   'minutes stretching',\n",
       "   'type plastics tell',\n",
       "   'trial 12mm',\n",
       "   'weight added',\n",
       "   'plastic type stretched',\n",
       "   '22mm trial',\n",
       "   '22mm trial 23mm',\n",
       "   'plastics tell',\n",
       "   'stretched type stretched',\n",
       "   'tell weight',\n",
       "   'trial 23mm',\n",
       "   'trial conclusion strongest',\n",
       "   'trial plastic',\n",
       "   'type plastics',\n",
       "   'improve experiment tell',\n",
       "   'tell type',\n",
       "   'strongest polymer',\n",
       "   'plastics tell weight',\n",
       "   'type stretched minutes']],\n",
       " (1,\n",
       "  2): [['stretched minutes',\n",
       "   'improve',\n",
       "   'tell',\n",
       "   'trial',\n",
       "   'stretched type',\n",
       "   'student',\n",
       "   'stretching 22mm',\n",
       "   '23mm trial',\n",
       "   'student improve',\n",
       "   'polymer type',\n",
       "   '10mm',\n",
       "   '12mm trial',\n",
       "   'added trials',\n",
       "   'conclusion strongest',\n",
       "   'experiment tell',\n",
       "   'plastics',\n",
       "   '10mm trial',\n",
       "   'trial conclusion',\n",
       "   'type way',\n",
       "   'improve experiment',\n",
       "   'type stretched',\n",
       "   'stretched 10mm',\n",
       "   'way student',\n",
       "   '12mm',\n",
       "   'strongest',\n",
       "   'experiment',\n",
       "   'plastic type',\n",
       "   'minutes stretching',\n",
       "   'conclusion',\n",
       "   'added',\n",
       "   'trial 12mm',\n",
       "   'way',\n",
       "   'polymer',\n",
       "   'weight added',\n",
       "   'weight',\n",
       "   'stretching',\n",
       "   '22mm trial',\n",
       "   'stretched',\n",
       "   'plastics tell',\n",
       "   'tell weight',\n",
       "   'trial 23mm',\n",
       "   'minutes',\n",
       "   'trial plastic',\n",
       "   'type',\n",
       "   'type plastics',\n",
       "   'plastic',\n",
       "   'trials',\n",
       "   'tell type',\n",
       "   'strongest polymer',\n",
       "   '23mm',\n",
       "   '22mm']]}"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_gram_dict[49]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **perfect now the full essays**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 768) (40, 768)\n",
      "(5, 768) (45, 768)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\oeb\\AppData\\Local\\Temp/ipykernel_15544/4174833780.py:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  kwrds = np.array(list(map(lambda x: x,st)))\n"
     ]
    }
   ],
   "source": [
    "kwrds=[]\n",
    "for k,st in zip(keywords_embeddings,students_candidates_emb[0]):\n",
    "    print(k.shape,st[0].shape)\n",
    "    # kwrds = key_words.match_keywords(k,st,thershold=0.5)\n",
    "    # kwrds = np.array(list(map(lambda x: key_words.match_keywords(k,x,thershold=0.5),st)))\n",
    "    kwrds = np.array(list(map(lambda x: x,st)))\n",
    "# lambda st_cand:\n",
    "#                     key_words.match_keywords(k, st_cand,\n",
    "#                     thershold=.5)\n",
    "    # print(len(st))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 45, 768)"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_candidates = key_words.candidates_tokens(docs[0],n_gram_range=n_gram_range)\n",
    "model_candidate_emb = self.model.encode(model_candidates)\n",
    "\n",
    "\n",
    "keywords_ = key_words.maximal_marginal_relevance(\n",
    "        model_answer_emb, model_candidate_emb,\n",
    "        model_candidates,\n",
    "        top_n =top_n, diversity=diversity)\n",
    "\n",
    "# keywords__emb = model.encode(keywords_)\n",
    "students_n_grams = key_words.get_n_grams(keywords_)\n",
    "\n",
    "students_candidates = list(map(lambda doc:\n",
    "            key_words.get_candidates(students_n_grams, doc),\n",
    "            docs[1:]))\n",
    "\n",
    "students_candidates_emb = list(map(self.__emb_keywords, students_candidates))\n",
    "keywords_embeddings =  list(map(self.__emb_keywords, keywords_))\n",
    "\n",
    "keywords_grades = np.array(list(map(lambda st_cand:\n",
    "            key_words.match_keywords(keywords_embeddings, st_cand,\n",
    "            thershold=threshold),\n",
    "            students_candidates_emb,\n",
    "            )))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'size'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_15544/1213562158.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m keywords_grades = np.array(list(map(\n\u001b[0m\u001b[0;32m      2\u001b[0m     lambda k_emb : np.array(list(map(lambda st_cand:\n\u001b[0;32m      3\u001b[0m             key_words.match_keywords(k_emb, st_cand,\n\u001b[0;32m      4\u001b[0m             thershold=.5),\n\u001b[0;32m      5\u001b[0m             \u001b[0mstudents_candidates_emb\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_15544/1213562158.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(k_emb)\u001b[0m\n\u001b[0;32m      1\u001b[0m keywords_grades = np.array(list(map(\n\u001b[1;32m----> 2\u001b[1;33m     lambda k_emb : np.array(list(map(lambda st_cand:\n\u001b[0m\u001b[0;32m      3\u001b[0m             key_words.match_keywords(k_emb, st_cand,\n\u001b[0;32m      4\u001b[0m             thershold=.5),\n\u001b[0;32m      5\u001b[0m             \u001b[0mstudents_candidates_emb\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_15544/1213562158.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(st_cand)\u001b[0m\n\u001b[0;32m      1\u001b[0m keywords_grades = np.array(list(map(\n\u001b[0;32m      2\u001b[0m     lambda k_emb : np.array(list(map(lambda st_cand:\n\u001b[1;32m----> 3\u001b[1;33m             key_words.match_keywords(k_emb, st_cand,\n\u001b[0m\u001b[0;32m      4\u001b[0m             thershold=.5),\n\u001b[0;32m      5\u001b[0m             \u001b[0mstudents_candidates_emb\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\projects\\NLP-APIS\\key_words.py\u001b[0m in \u001b[0;36mmatch_keywords\u001b[1;34m(keywords_emb, candidates_emb, thershold)\u001b[0m\n\u001b[0;32m    178\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0memb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    179\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 180\u001b[1;33m     \u001b[0mcandidates_emb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape_check\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcandidates_emb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    181\u001b[0m     \u001b[0mkeywords_emb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape_check\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeywords_emb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    182\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\projects\\NLP-APIS\\key_words.py\u001b[0m in \u001b[0;36mshape_check\u001b[1;34m(emb)\u001b[0m\n\u001b[0;32m    173\u001b[0m     \u001b[1;31m# check if candidates_emb shape is 1d\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    174\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mshape_check\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0memb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 175\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[0memb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    176\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0memb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    177\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'size'"
     ]
    }
   ],
   "source": [
    "keywords_grades = np.array(list(map(\n",
    "    lambda k_emb : np.array(list(map(lambda st_cand:\n",
    "            key_words.match_keywords(k_emb, st_cand,\n",
    "            thershold=.5),\n",
    "            students_candidates_emb,\n",
    "            ))),keywords_embeddings)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# moment of truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ngram_top_n_diversity(docs):\n",
    "    lens = []\n",
    "    tokens = []\n",
    "    for doc in docs:\n",
    "        tokens_ = [w.translate(str.maketrans('', '', string.punctuation)) for w in [doc]]\n",
    "        tokens_ = [w for w in set(doc.split()) if w not in stop_words]\n",
    "        tokens.append(tokens_)\n",
    "        lens.append(len(tokens))\n",
    "\n",
    "    model_candidates = tokens\n",
    "    model_candidate_emb = np.array(list(map(lambda cand:BERT.model.encode(cand),model_candidates)))\n",
    "    print(model_candidate_emb.shape)\n",
    "    # list(map(lambda st:cos_sim(model_answer_emb,st),model_candidate_emb))\n",
    "    # cos_sim(model_answer_emb, model_candidate_emb)\n",
    "\n",
    "    lens = np.median(np.array(lens))\n",
    "    return lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23.5\n",
      "19.5\n",
      "33.0\n"
     ]
    }
   ],
   "source": [
    "for i,v in ess_dict.items():\n",
    "    l = get_ngram_top_n_diversity(v)\n",
    "    print(l)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "ess_dict = {\n",
    "    \"ess_1_model_answers\": ess_1_model_answers,\n",
    "    \"ess_2_model_answers\": ess_2_model_answers,\n",
    "    \"ess_3_model_answers\": ess_3_model_answers,\n",
    "    \"ess_4_model_answers\": ess_4_model_answers,\n",
    "    \"ess_5_model_answers\": ess_5_model_answers,\n",
    "    \"ess_6_model_answers\": ess_6_model_answers,\n",
    "    \"ess_7_model_answers\": ess_7_model_answers,\n",
    "    \"ess_8_model_answers\": ess_8_model_answers,\n",
    "    \"ess_9_model_answers\": ess_9_model_answers,\n",
    "    \"ess_10_model_answers\": ess_10_model_answers,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19.0\n",
      "19.0\n",
      "30.0\n",
      "34.0\n",
      "37.0\n",
      "29.5\n",
      "20.5\n",
      "32.0\n",
      "35.0\n",
      "25.5\n"
     ]
    }
   ],
   "source": [
    "for i,v in ess_dict.items():\n",
    "    l = get_ngram_top_n_diversity(v)\n",
    "    if l < 10:\n",
    "        # short \n",
    "        # \n",
    "        tokens = [w.translate(str.maketrans('', '', string.punctuation)) for w in [doc]]\n",
    "        tokens = [w for w in set(doc.split()) if w not in stop_words]\n",
    "        model_candidates = tokens\n",
    "        model_candidate_emb = BERT.model.encode(model_candidates)\n",
    "        cos_sim(model_answer_emb, model_candidate_emb)\n",
    "    print(l)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_keys_div(ind,essay,top_n=5,diversities=[0.8]):\n",
    "    for div in diversities:\n",
    "        keywords = list(map(lambda x: maximal_marginal_relevance(x[0].reshape(1, -1),x[1],x[2],top_n=top_n,diversity=div),\n",
    "            zip(emb_dict[f'ess_{essay}_model_answers_emb'],\n",
    "            model_candidate_emb,model_candidates)))\n",
    "        print(div,keywords[ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# out = {}\n",
    "top_n_l = [7]*10\n",
    "diversity_l = [0.6,0.6, 0.7,0.7,0.7,0.7,0.6,0.7,0.7,0.7]\n",
    "ngram_range_l = [(2,2),(2,2),(2,3),(2,3),(2,3),(2,3),(2,2),(2,3),(2,3),(2,3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "essayset 1\n",
      "model answer 0\n",
      "model answer 1\n",
      "model answer 2\n",
      "model answer 3\n",
      "model answer 4\n",
      "model answer 5\n",
      "model answer 6\n",
      "model answer 7\n",
      "model answer 8\n",
      "model answer 9\n",
      "model answer 10\n",
      "model answer 11\n",
      "model answer 12\n",
      "model answer 13\n",
      "model answer 14\n",
      "model answer 15\n",
      "model answer 16\n",
      "model answer 17\n",
      "model answer 18\n",
      "model answer 19\n",
      "model answer 20\n",
      "model answer 21\n",
      "essayset 2\n",
      "model answer 0\n",
      "model answer 1\n",
      "model answer 2\n",
      "model answer 3\n",
      "model answer 4\n",
      "model answer 5\n",
      "model answer 6\n",
      "model answer 7\n",
      "model answer 8\n",
      "model answer 9\n",
      "model answer 10\n",
      "model answer 11\n",
      "model answer 12\n",
      "model answer 13\n",
      "model answer 14\n",
      "model answer 15\n",
      "model answer 16\n",
      "model answer 17\n",
      "model answer 18\n",
      "model answer 19\n",
      "model answer 20\n",
      "model answer 21\n",
      "essayset 3\n",
      "model answer 0\n",
      "model answer 1\n",
      "model answer 2\n",
      "model answer 3\n",
      "model answer 4\n",
      "model answer 5\n",
      "model answer 6\n",
      "model answer 7\n",
      "model answer 8\n",
      "model answer 9\n",
      "model answer 10\n",
      "model answer 11\n",
      "model answer 12\n",
      "model answer 13\n",
      "model answer 14\n",
      "model answer 15\n",
      "model answer 16\n",
      "model answer 17\n",
      "model answer 18\n",
      "model answer 19\n",
      "model answer 20\n",
      "model answer 21\n",
      "essayset 4\n",
      "model answer 0\n",
      "model answer 1\n",
      "model answer 2\n",
      "model answer 3\n",
      "model answer 4\n",
      "model answer 5\n",
      "model answer 6\n",
      "model answer 7\n",
      "model answer 8\n",
      "model answer 9\n",
      "model answer 10\n",
      "model answer 11\n",
      "model answer 12\n",
      "model answer 13\n",
      "model answer 14\n",
      "model answer 15\n",
      "model answer 16\n",
      "model answer 17\n",
      "model answer 18\n",
      "model answer 19\n",
      "model answer 20\n",
      "model answer 21\n",
      "essayset 5\n",
      "model answer 0\n",
      "model answer 1\n",
      "model answer 2\n",
      "model answer 3\n",
      "model answer 4\n",
      "model answer 5\n",
      "model answer 6\n",
      "model answer 7\n",
      "model answer 8\n",
      "model answer 9\n",
      "model answer 10\n",
      "model answer 11\n",
      "model answer 12\n",
      "model answer 13\n",
      "model answer 14\n",
      "model answer 15\n",
      "model answer 16\n",
      "model answer 17\n",
      "model answer 18\n",
      "model answer 19\n",
      "model answer 20\n",
      "model answer 21\n",
      "essayset 6\n",
      "model answer 0\n",
      "model answer 1\n",
      "model answer 2\n",
      "model answer 3\n",
      "model answer 4\n",
      "model answer 5\n",
      "model answer 6\n",
      "model answer 7\n",
      "model answer 8\n",
      "model answer 9\n",
      "model answer 10\n",
      "model answer 11\n",
      "model answer 12\n",
      "model answer 13\n",
      "model answer 14\n",
      "model answer 15\n",
      "model answer 16\n",
      "model answer 17\n",
      "model answer 18\n",
      "model answer 19\n",
      "model answer 20\n",
      "model answer 21\n",
      "essayset 7\n",
      "model answer 0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_13800/3865244871.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0mdocs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mquery\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'EssaySet == {essay+1}'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"EssayText\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m     \u001b[0mdd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpara_keywords_pipeline_output_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkeywords\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdocs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mstudents_n_grams\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[0mout\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34mf'ess_{essay+1}_keywords'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_13800/2122905503.py\u001b[0m in \u001b[0;36mpara_keywords_pipeline_output_dict\u001b[1;34m(keywords, docs, n_gram_list, batch)\u001b[0m\n\u001b[0;32m     38\u001b[0m                             \u001b[0mcheck_n_gram_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstudents_n_grams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdoc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m                             enumerate(docs[i:i+batch])))\n\u001b[1;32m---> 40\u001b[1;33m             \u001b[0mstudents_candidates_emb\u001b[0m \u001b[1;33m=\u001b[0m  \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mst\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0memb_keywords\u001b[0m \u001b[1;33m,\u001b[0m\u001b[0mst\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstudents_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m             \u001b[1;31m# students_candidates_emb =  list(map(emb_d, students_candidates))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m             \u001b[0mstudents_candidates_list_s\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstudents_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_13800/2122905503.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(st)\u001b[0m\n\u001b[0;32m     38\u001b[0m                             \u001b[0mcheck_n_gram_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstudents_n_grams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdoc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m                             enumerate(docs[i:i+batch])))\n\u001b[1;32m---> 40\u001b[1;33m             \u001b[0mstudents_candidates_emb\u001b[0m \u001b[1;33m=\u001b[0m  \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mst\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0memb_keywords\u001b[0m \u001b[1;33m,\u001b[0m\u001b[0mst\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstudents_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m             \u001b[1;31m# students_candidates_emb =  list(map(emb_d, students_candidates))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m             \u001b[0mstudents_candidates_list_s\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstudents_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_13800/3553079213.py\u001b[0m in \u001b[0;36memb_keywords\u001b[1;34m(keywords)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0memb_keywords\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkeywords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[1;31m# x = np.array(list(map(lambda k: np.array(list(map(lambda x: BERT.model.encode(str(x)), k))),keywords)))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mget_words_emb\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mkeywords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[1;31m# print(\"emb be\",x.shape)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_13800/3553079213.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(k)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0memb_keywords\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkeywords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[1;31m# x = np.array(list(map(lambda k: np.array(list(map(lambda x: BERT.model.encode(str(x)), k))),keywords)))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mget_words_emb\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mkeywords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[1;31m# print(\"emb be\",x.shape)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_13800/3553079213.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0memb_keywords\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkeywords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[1;31m# x = np.array(list(map(lambda k: np.array(list(map(lambda x: BERT.model.encode(str(x)), k))),keywords)))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mget_words_emb\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mkeywords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[1;31m# print(\"emb be\",x.shape)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_13800/3560175523.py\u001b[0m in \u001b[0;36mget_words_emb\u001b[1;34m(word)\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwords_emb_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m         \u001b[0mwords_emb_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBERT\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwords_emb_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\oeb\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sentence_transformers\\SentenceTransformer.py\u001b[0m in \u001b[0;36mencode\u001b[1;34m(self, sentences, batch_size, show_progress_bar, output_value, convert_to_numpy, convert_to_tensor, device, normalize_embeddings)\u001b[0m\n\u001b[0;32m    155\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    156\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 157\u001b[1;33m                 \u001b[0mout_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    158\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    159\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0moutput_value\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'token_embeddings'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\oeb\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    139\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 141\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    142\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\oeb\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\oeb\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sentence_transformers\\models\\Transformer.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, features)\u001b[0m\n\u001b[0;32m     49\u001b[0m             \u001b[0mtrans_features\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'token_type_ids'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'token_type_ids'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m         \u001b[0moutput_states\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mtrans_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m         \u001b[0moutput_tokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutput_states\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\oeb\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\oeb\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    848\u001b[0m             \u001b[0mpast_key_values_length\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpast_key_values_length\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    849\u001b[0m         )\n\u001b[1;32m--> 850\u001b[1;33m         encoder_outputs = self.encoder(\n\u001b[0m\u001b[0;32m    851\u001b[0m             \u001b[0membedding_output\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    852\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mextended_attention_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\oeb\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\oeb\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    522\u001b[0m                 )\n\u001b[0;32m    523\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 524\u001b[1;33m                 layer_outputs = layer_module(\n\u001b[0m\u001b[0;32m    525\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    526\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\oeb\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\oeb\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    408\u001b[0m         \u001b[1;31m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    409\u001b[0m         \u001b[0mself_attn_past_key_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpast_key_value\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mpast_key_value\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 410\u001b[1;33m         self_attention_outputs = self.attention(\n\u001b[0m\u001b[0;32m    411\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    412\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\oeb\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\oeb\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    335\u001b[0m         \u001b[0moutput_attentions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    336\u001b[0m     ):\n\u001b[1;32m--> 337\u001b[1;33m         self_outputs = self.self(\n\u001b[0m\u001b[0;32m    338\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    339\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\oeb\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\oeb\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    275\u001b[0m         \u001b[0mcontext_layer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcontext_layer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    276\u001b[0m         \u001b[0mnew_context_layer_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcontext_layer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mall_head_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 277\u001b[1;33m         \u001b[0mcontext_layer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcontext_layer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mnew_context_layer_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    278\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    279\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mcontext_layer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattention_probs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0moutput_attentions\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mcontext_layer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for essay in range(0,10):\n",
    "    # get top_n, diversity, ngram_range\n",
    "    print(\"essayset\",essay+1)\n",
    "    \n",
    "    model_candidates = list(map(lambda ans: key_words.candidates_tokens(ans,n_gram_range=ngram_range_l[essay+1]),ess_dict[f'ess_{essay+1}_model_answers']))\n",
    "    model_candidate_emb = list(map(lambda cand:BERT.model.encode(cand),model_candidates))\n",
    "    keywords = list(map(lambda x: maximal_marginal_relevance(\n",
    "        x[0].reshape(1, -1),x[1],x[2],top_n=top_n_l[essay+1],diversity=diversity_l[essay+1]),\n",
    "        zip(emb_dict[f'ess_{essay+1}_model_answers_emb'],\n",
    "        model_candidate_emb,model_candidates)))\n",
    "    students_n_grams = list(map(lambda keyword: sorted(key_words.get_n_grams(keyword)),keywords))\n",
    "\n",
    "    docs = df.query(f'EssaySet == {essay+1}')[\"EssayText\"].values.tolist()\n",
    "\n",
    "    dd = para_keywords_pipeline_output_dict(keywords,docs,students_n_grams,batch=10)\n",
    "    \n",
    "    save_obj(dd,f'data/results/keywords_res_essay_{essay+1}')\n",
    "    # out[f'ess_{essay+1}_keywords'] = dd\n",
    "    # save_obj(out,f'data/results/keywords_output_dict{essay+1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_obj(words_emb_dict,f'data/results/words_emb_dict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.000091552734375"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.getsizeof(words_emb_dict) / 1024 / 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data/results/words_emb_dict_j']"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# joblib\n",
    "import joblib\n",
    "joblib.dump(words_emb_dict,f'data/results/words_emb_dict_j')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "741.5214424000001\n",
      "1851.9714044000002\n",
      "3172.2670228999996\n",
      "4569.112715900001\n",
      "4641.669221300001\n",
      "1953.6475882999985\n"
     ]
    }
   ],
   "source": [
    "for i,v in out.items():\n",
    "    print(v['times'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(2, 3)]\n",
      "[(2, 3), (3, 4)]\n",
      "[(2, 3), (3, 4)]\n",
      "[(2, 3), (3, 4)]\n",
      "[(2, 3), (3, 4)]\n",
      "[(2, 3)]\n"
     ]
    }
   ],
   "source": [
    "for i,v in out.items():\n",
    "    print(v['n_grams_list'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ess_1_keywords',\n",
       " 'ess_2_keywords',\n",
       " 'ess_3_keywords',\n",
       " 'ess_4_keywords',\n",
       " 'ess_5_keywords',\n",
       " 'ess_6_keywords']"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[i for i in out.keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "for essay in range(0,6):\n",
    "# for i,v in out.items():\n",
    "    save_obj(out[f'ess_{essay+1}_keywords'],f'data/results/keywords_res_essay_{essay+1}')\n",
    "    # print(out[f'ess_{essay+1}_keywords'].keys())\n",
    "    # print(f'data/results/keywords_res_essay_{i+1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "essayset 7\n",
      "essayset 8\n",
      "essayset 9\n",
      "essayset 10\n"
     ]
    }
   ],
   "source": [
    "for essay in range(6,10):\n",
    "    # get top_n, diversity, ngram_range\n",
    "    print(\"essayset\",essay+1)\n",
    "    \n",
    "    model_candidates = list(map(lambda ans: key_words.candidates_tokens(ans,n_gram_range=ngram_range_l[essay+1]),ess_dict[f'ess_{essay+1}_model_answers']))\n",
    "    model_candidate_emb = list(map(lambda cand:BERT.model.encode(cand),model_candidates))\n",
    "    keywords = list(map(lambda x: maximal_marginal_relevance(\n",
    "        x[0].reshape(1, -1),x[1],x[2],top_n=top_n_l[essay+1],diversity=diversity_l[essay+1]),\n",
    "        zip(emb_dict[f'ess_{essay+1}_model_answers_emb'],\n",
    "        model_candidate_emb,model_candidates)))\n",
    "    students_n_grams = list(map(lambda keyword: sorted(key_words.get_n_grams(keyword)),keywords))\n",
    "\n",
    "    docs = df.query(f'EssaySet == {essay+1}')[\"EssayText\"].values.tolist()\n",
    "\n",
    "    dd = para_keywords_pipeline_output_dict(keywords,docs,students_n_grams,batch=10)\n",
    "    save_obj(dd,f'data/results/keywords_res_essay_{essay+1}')\n",
    "\n",
    "    del dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer(\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(doc1,doc2):\n",
    "    return float(cos_sim(model.encode(doc1),model.encode(doc2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = df.query(f'EssaySet == {1+1}')[\"EssayText\"].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55.90439499999775\n"
     ]
    }
   ],
   "source": [
    "t1 = time.perf_counter()\n",
    "# 749mb\n",
    "x = model.encode(docs) # 30mb memory\n",
    "print(time.perf_counter() - t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "162.5099635000006"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1 = time.perf_counter()\n",
    "# 750mb\n",
    "x = BERT.model.encode(docs) # 100mb memory\n",
    "time.perf_counter() - t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.48043105006217957\n",
      "0.8797786831855774\n",
      "0.3992762267589569\n",
      "0.5011984705924988\n",
      "0.6617798805236816\n",
      "0.8037792444229126\n"
     ]
    }
   ],
   "source": [
    "print(predict('I am a student','I am hazem')) # 0.48043105006217957\n",
    "print(predict('lava is the molten rock expelled by a volcano during an eruption',\n",
    "    \"الحمم البركانية هي الصخور المنصهرة التي طردها بركان أثناء ثوران بركان\")) # 0.8797786831855774\n",
    "print(predict('increases','decreases')) #0.3992762267589569\n",
    "print(predict('تقل','decrease')) # 0.5011984705924988\n",
    "print(predict('تنقص','decrease')) # 0.6617798805236816\n",
    "print(predict('تخفيض','decrease')) # 0.8037792444229126\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.8798]])"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos_sim(model.encode([\"lava is the molten rock expelled by a volcano during an eruption\"]),\n",
    "model.encode([\"الحمم البركانية هي الصخور المنصهرة التي طردها بركان أثناء ثوران بركان\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.7766]])"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos_sim(BERT.model.encode([\"increases\"]),BERT.model.encode([\"decreases\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kriging-style spatial regression / inverse distance interpolation\n",
    "import numpy as np\n",
    "import random\n",
    "random.seed(100)\n",
    "# --- Highlights of this \"fuzzy regression\" code:\n",
    "# Model-free; produces big output file to compute prediction intervals; Bivariate case,\n",
    "# featuring nearest-neighbor approach (the weights); Exact predictions for training\n",
    "# set, yet robust (no overfitting); Increasing M is \"lazy way\" to boost performance,\n",
    "# but it slows speed\n",
    "# Math-free (no matrix algrebra, square root or calculus); Statistics-free (no\n",
    "# statistical science involved at all); Requires no technical knowledge beyond high\n",
    "# school, but far from trivial!\n",
    "# Acts as low-pass, amplitude reduction, or signal compression filter; Also acts as\n",
    "# noise filtering, signal enhancement. Amplitude restoration step not included, but\n",
    "# easy to do.\n",
    "# By Vincent Granville, www.MLTechniques.com\n",
    "# --- Hyperparameters\n",
    "# n (number of obs, called points) set after reading input file [n=1000 here]\n",
    "P=0.8 # proportion of data allocated to training the remaining is for validation\n",
    "M=5000 # max number of splines used per point; M=5000 offers modest gain over M=800\n",
    "r=2 # number of points defining a spline; also works with r=1 or larger r\n",
    "smoother=1.5 # smoothing param used in weighted predictions; try 0.5 for more smoothing(0 = max smoothing)\n",
    "thresh1=25.0 # max distance allowed to nearby spline; increase to eliminate points with no predictions;\n",
    "# decrease to narrow (improve) confidence intervals\n",
    "thresh2=1.5 # max outlier level allowed for predicted values ; if < 1, predicted can’t be more \n",
    "# extreme than observed; if too low, may increase number of points with no prediction; if too \n",
    "# large, may produce a few strong outlier predictions;\n",
    "thresh3=0.001 # control numerical stability (keep 0.001)\n",
    "# --- Output var (defined later)\n",
    "# missing : number of points not assigned a prediction\n",
    "#\n",
    "# count : actual number of splines used for a specific point\n",
    "# error : code telling why a point is not assigned a prediction\n",
    "# weight : weight assigned to a spline, for a given point\n",
    "# zpred : predicted value for a point zz = (xx, yy)\n",
    "# zpredw : weighted predicted value\n",
    "# Input var (defined later)\n",
    "#\n",
    "# xx, yy, zz: coordinates of a point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Reading input file\n",
    "x=[]\n",
    "y=[]\n",
    "z=[]\n",
    "file=open('fuzzy2b.txt',\"r\")\n",
    "lines=file.readlines()\n",
    "for aux in lines:\n",
    "    x.append(aux.split('\\t')[0])\n",
    "    y.append(aux.split('\\t')[1])\n",
    "    z.append(aux.split('\\t')[2])\n",
    "file.close()\n",
    "x = list(map(float, x))\n",
    "y = list(map(float, y))\n",
    "z = list(map(float, z))\n",
    "zmin=np.min(z)\n",
    "zmax=np.max(z)\n",
    "zavg=np.mean(z)\n",
    "zdev=max(abs(zmin-zavg),abs(zmax-zavg))\n",
    "n=len(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Core function: spline-based interpolator\n",
    "def F(xx,yy,r):\n",
    "    zz=0\n",
    "    distmin=1\n",
    "    error=0\n",
    "    idx=[]\n",
    "    A=[]\n",
    "    B=[]\n",
    "    for i in range(0,r):\n",
    "        idx.insert(i,int(n*P*random.random()))\n",
    "    prod=1.0;\n",
    "    for i in range(0,r):\n",
    "        for j in range(i+1,r):\n",
    "            prod*=(x[idx[i]]-x[idx[j]])*(y[idx[i]]-y[idx[j]])\n",
    "    if abs(prod)>thresh3:\n",
    "        for i in range(0,r):\n",
    "            A.insert(i,1.0)\n",
    "            B.insert(i,1.0)\n",
    "            for j in range(0,r):\n",
    "                if j != i:\n",
    "                    A[i]*=(xx-x[idx[j]])/(x[idx[i]]-x[idx[j]])\n",
    "                    B[i]*=(yy-y[idx[j]])/(y[idx[i]]-y[idx[j]])\n",
    "            zz+=z[idx[i]]*(A[i]+B[i])/2\n",
    "            distmin*=max(abs(xx-x[idx[i]]),abs(yy-y[idx[i]]))\n",
    "        distmin=pow(distmin,1/r)\n",
    "    else:\n",
    "        error=1;\n",
    "    return [zz,distmin,error]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ignored points\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "#----------------------------------------------------\n",
    "# Main step: predictions for points in validation set\n",
    "# For training set points, change range(int(P*n),n) to range(0,int(P*n))\n",
    "\n",
    "file_small=open(\"fuzzy_small.txt\",\"w\")\n",
    "file_big=open(\"fuzzy_big.txt\",\"w\")\n",
    "\n",
    "for j in range(int(P*n),n): # loop over all validation points \n",
    "\n",
    "  xx=x[j]\n",
    "  yy=y[j]\n",
    "  zobs=z[j]\n",
    "  count=0\n",
    "  missing=0\n",
    "  sweight=0.0\n",
    "  zpredw=0.0\n",
    "  zpred=0.0\n",
    "\n",
    "  for k in range(0,M): # inner loop over all splines\n",
    "\n",
    "    list=F(xx,yy,r)\n",
    "    zz=list[0]\n",
    "    distmin=list[1]\n",
    "    error=list[2]\n",
    "    weight=math.exp(-smoother*distmin)  \n",
    "    zzdevratio=abs(zz-zavg)/zdev\n",
    "\n",
    "    if distmin<thresh1 and zzdevratio<thresh2 and error==0: \n",
    "      count+=1\n",
    "      sweight+=weight\n",
    "      zpredw+=zz*weight\n",
    "      zpred+=zz\n",
    "      row=[j,xx,yy,zobs,zz,distmin,weight,zzdevratio]\n",
    "      for field in row:\n",
    "        file_big.write(str(field)+\"\\t\")\n",
    "      file_big.write(\"\\n\")\n",
    "\n",
    "  if count>0:\n",
    "    zpredw=zpredw/sweight\n",
    "    zpred=zpred/count\n",
    "  else:\n",
    "    missing+=1\n",
    "    zpredw=\"\"\n",
    "    zpred=\"\"\n",
    "\n",
    "  row=[j,count,xx,yy,zobs,zpred,zpredw] \n",
    "  for field in row:\n",
    "    file_small.write(str(field)+\"\\t\")\n",
    "  file_small.write(\"\\n\")\n",
    "\n",
    "file_big.close()\n",
    "file_small.close()\n",
    "print(missing,\"ignored points\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0f876b07db73824ba94c3da26a300833b9286c0dd0d4e31723ae4574ddd9b9bb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
